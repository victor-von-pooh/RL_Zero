{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"private_outputs":true,"authorship_tag":"ABX9TyN5is3hpEzgI12wEhTU1Ag5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第4章 〜動的計画法〜\n","\n","ここでは、ベルマン方程式によって得られる連立方程式を解くための「<font color=\"red\">**動的計画法**</font>」について扱う。"],"metadata":{"id":"iQWIWGB8e0r0"}},{"cell_type":"markdown","source":["## 動的計画法と方策評価\n","\n","強化学習の問題の多くは2つのタスクに取り組むことになる。  \n","ある方策 $\\pi$ が与えられたときに、その方策の価値関数 $v_\\pi(s)$ や $q_\\pi(s, a)$ を求めることを<font color=\"red\">**方策評価**</font>(Policy Evaluation)といい、方策を制御して最適方策へと調整することを<font color=\"red\">**方策制御**</font>(Policy Control)という。  \n","強化学習の最終的なゴールは方策制御だが、そのための最初のステップとして方策評価に取り組むことは多くある。  \n","ここでは、動的計画法(Dynamic Programming)で方策評価を行う。  \n","以降、動的計画法を DP と略記する。"],"metadata":{"id":"1QmNCTg1fjZ2"}},{"cell_type":"markdown","source":["### DP の概要\n","\n","以下の価値関数は無限を含む期待値の計算をしているため、通常は計算できない。\n","\n","$$v_{\\pi}(s) = \\mathbb{E}_\\pi[R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots| S_t = s]$$\n","\n","これを解決するのがベルマン方程式である。\n","\n","$$v_{\\pi}(s) = \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}$$\n","\n","これは、「現在の状態 $s$ の価値関数 $v_{\\pi}(s)$」と「次の状態 $s'$ の価値関数 $v_{\\pi}(s')$」との関係性を表している。  \n","これを「更新式」として見ると\n","\n","$$V_{k+1}(s) = \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V_k(s') \\right\\}$$\n","\n","のようになる。  \n","$V$ は推測値であり、真の価値関数 $v$ とは異なる。  \n","ここで行われているのは、「推定値 $V_k(s')$」を使って「別の推定値 $V_{k+1}(s)$」を改善することであり、このプロセスを<font color=\"red\">**ブートストラッピング**</font>(Bootstrapping)という。"],"metadata":{"id":"Bx0EjM8ohBsW"}},{"cell_type":"markdown","source":["### 反復方策評価(*コーディング*)\n","\n","初期値 $V_0(s)$ から $V_1(s)$ へ更新し、さらに $V_2(s)$ へ更新というように繰り返し行うことで $v_{\\pi}(s)$ へと近づいていくアルゴリズムを<font color=\"red\">**反復方策評価**</font>(Iterative Policy Evaluation)という。  "],"metadata":{"id":"-JaqgN65dznt"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"-x4eOq8aioM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# グリッドワールドを作成する関数を定義\n","# 0: 進める道, 1: 障害物, 2: スタート, 3: ゴール, 4: 加点ポイント, 5: 減点ポイント\n","def create_grid_world(\n","        rows: int, cols: int,\n","        start: set, goal: set,\n","        obstacles: list, items: dict, enemies: dict\n","    ):\n","    grid = np.zeros((rows, cols))\n","    points = np.zeros((rows, cols), dtype=int)\n","\n","    for obs in obstacles:\n","        grid[obs] = 1\n","\n","    grid[start] = 2\n","    if goal != ():\n","        grid[goal] = 3\n","\n","    for item, point in items.items():\n","        grid[item] = 4\n","        points[item] = point\n","\n","    for enemy, point in enemies.items():\n","        grid[enemy] = 5\n","        points[enemy] = point\n","\n","    return grid, points\n","\n","\n","# グリッドワールドを描画する関数を定義\n","def draw_grid_world(grid, points, figsize_x, figsize_y):\n","    rows, cols = grid.shape\n","    fig = plt.figure(figsize=(figsize_x, figsize_y), tight_layout=True)\n","    ax = fig.subplots()\n","\n","    ax.matshow(grid, vmin=-0.5, vmax=5)\n","\n","    ax.set_xticks(np.arange(-.5, cols, 1), minor=True)\n","    ax.set_yticks(np.arange(-.5, rows, 1), minor=True)\n","    ax.grid(which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=2)\n","\n","    for (i, j), val in np.ndenumerate(grid):\n","        if val == 1:\n","            ax.text(\n","                j, i, \"X\", ha=\"center\", va=\"center\",\n","                color=\"yellow\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 2:\n","            ax.text(\n","                j, i, \"S\", ha=\"center\", va=\"center\",\n","                color=\"red\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 3:\n","            ax.text(\n","                j, i, \"G\", ha=\"center\", va=\"center\",\n","                color=\"blue\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 4:\n","            ax.text(\n","                j, i, f\"+{points[i, j]}\", ha=\"center\", va=\"center\",\n","                color=\"white\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 5:\n","            ax.text(\n","                j, i, f\"{points[i, j]}\", ha=\"center\", va=\"center\",\n","                color=\"black\", fontsize=16, fontweight=\"bold\"\n","            )\n","\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","    plt.show()"],"metadata":{"id":"jedT0al-iqUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# グリッドワールドのパラメータを設定\n","rows, cols = 1, 4\n","start = (0, 1)\n","goal = ()\n","obstacles = []\n","items = {(0, 2): +1}\n","enemies = {(0, 0): -1, (0, 3): -1}\n","\n","# グリッドワールドを作成して描画\n","grid, points = create_grid_world(\n","    rows, cols, start, goal, obstacles, items, enemies\n",")\n","draw_grid_world(grid, points, cols, rows)"],"metadata":{"id":"0biFu2n6it2T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2マスのグリッドワールド問題において、エージェントはランダムな方策 $\\pi$ (確率0.5で左へ移動、確率0.5で右へ移動)に従って行動すると仮定する。  \n","なお、状態遷移は決定論的に決まるとする。  \n","このとき、価値関数の更新式は次のように簡略化できる。\n","\n","$$\n","V_{k+1}(s) = \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V_k(s') \\right\\} \\\\\n","\\downarrow \\space \\downarrow \\space \\downarrow \\\\\n","V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V_k(s') \\right\\} \\\\\n","\\downarrow \\space \\downarrow \\space \\downarrow \\\\\n","V_{k+1}(s) = \\sum_a \\pi(a|s) \\left\\{ r(s, a, s') + \\gamma V_k(s') \\right\\}\n","$$\n","\n","最後の式は、状態遷移は決定論的であることから $s' = f(s, a)$ として $\\sum_{s'} p(s'|s, a)$ の部分を省略している。\n","\n","方策 $\\pi$ における価値関数を反復方策評価アルゴリズムを使って求めてみる。  \n","まずは初期値として $V_0(s) = 0$ とする。  \n","すなわち、\n","\n","\\begin{eqnarray*}\n","V_0((0, 1)) &=& 0 \\\\\n","V_0((0, 2)) &=& 0\n","\\end{eqnarray*}\n","\n","となる。  \n","状態(0, 1)にいるとき、割引率 $\\gamma$ を0.9とすると更新式は\n","\n","$$\n","0.5 \\left \\{ -1 + 0.9 V_0((0, 1)) \\right \\} \\\\\n","0.5 \\left \\{ 1 + 0.9 V_0((0, 2)) \\right \\}\n","$$\n","\n","となるので、 $V_1((0, 1))$ は\n","\n","\\begin{eqnarray*}\n","V_1((0, 1)) &=& 0.5 \\left \\{ -1 + 0.9 V_0((0, 1)) \\right \\} + 0.5 \\left \\{ 1 + 0.9 V_0((0, 2)) \\right \\} \\\\\n","&=& 0.5 \\left( -1 + 0.9 \\cdot 0 \\right) + 0.5 \\left( 1 + 0.9 \\cdot 0 \\right) \\\\\n","&=& 0\n","\\end{eqnarray*}\n","\n","となる。  \n","同様にして $V_1((0, 2))$ は\n","\n","\\begin{eqnarray*}\n","V_1((0, 2)) &=& 0.5 \\left \\{ 0 + 0.9 V_0((0, 1)) \\right \\} + 0.5 \\left \\{ -1 + 0.9 V_0((0, 2)) \\right \\} \\\\\n","&=& 0.5 \\left( 0 + 0.9 \\cdot 0 \\right) + 0.5 \\left( -1 + 0.9 \\cdot 0 \\right) \\\\\n","&=& -0.5\n","\\end{eqnarray*}\n","\n","となる。  \n","これをプログラムで繰り返していく。"],"metadata":{"id":"JlQV30QXikoh"}},{"cell_type":"code","source":["# V の初期化\n","V = {\"s_1\": 0.0, \"s_2\": 0.0}\n","new_V = V.copy()\n","\n","# V を new_V を使って100回更新\n","for _ in range(100):\n","    new_V[\"s_1\"] = 0.5 * (-1 + 0.9 * V[\"s_1\"]) +\\\n","                   0.5 * (1 + 0.9 * V[\"s_2\"])\n","    new_V[\"s_2\"] = 0.5 * (0 + 0.9 * V[\"s_1\"]) +\\\n","                   0.5 * (-1 + 0.9 * V[\"s_2\"])\n","\n","    V = new_V.copy()\n","    print(V)"],"metadata":{"id":"Tq9isMj5lzVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["真の価値関数の値は\n","\n","```\n","{'s_1': -2.25, 's_2': -2.75}\n","```\n","\n","であるので、上の実験結果はこれとほぼ一致していることがわかる。  \n","今度は、閾値を設定して更新する回数を自動で決める。"],"metadata":{"id":"VdSh8cfsEQ6_"}},{"cell_type":"code","source":["# V の初期化\n","V = {\"s_1\": 0.0, \"s_2\": 0.0}\n","new_V = V.copy()\n","\n","# カウントしながら閾値を超えるまで更新する\n","cnt = 0\n","while True:\n","    new_V[\"s_1\"] = 0.5 * (-1 + 0.9 * V[\"s_1\"]) + 0.5 * (1 + 0.9 * V[\"s_2\"])\n","    new_V[\"s_2\"] = 0.5 * (0 + 0.9 * V[\"s_1\"]) + 0.5 * (-1 + 0.9 * V[\"s_2\"])\n","\n","    delta = abs(new_V[\"s_1\"] - V[\"s_1\"])\n","    delta = max(delta, abs(new_V[\"s_2\"] - V[\"s_2\"]))\n","\n","    V = new_V.copy()\n","\n","    cnt += 1\n","    if delta < 0.0001:\n","        print(V)\n","        print(cnt)\n","        break"],"metadata":{"id":"rBAA3twnFCqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 反復方策評価の別の方法(*コーディング*)\n","\n","先ほどの方法とは別に、辞書の各要素を上書きする方法を扱う。  \n","これを「上書き方式」と呼ぶことにする。"],"metadata":{"id":"vZUDdtVWMyc1"}},{"cell_type":"code","source":["# V の初期化\n","V = {\"s_1\": 0.0, \"s_2\": 0.0}\n","\n","# カウントしながら「上書き方式」で閾値を超えるまで更新する\n","cnt = 0\n","while True:\n","    t = 0.5 * (-1 + 0.9 * V[\"s_1\"]) + 0.5 * (1 + 0.9 * V[\"s_2\"])\n","    delta = abs(t - V[\"s_1\"])\n","    V[\"s_1\"] = t\n","\n","    t = 0.5 * (0 + 0.9 * V[\"s_1\"]) + 0.5 * (-1 + 0.9 * V[\"s_2\"])\n","    delta = max(delta, abs(t - V[\"s_2\"]))\n","    V[\"s_2\"] = t\n","\n","    cnt += 1\n","    if delta < 0.0001:\n","        print(V)\n","        print(cnt)\n","        break"],"metadata":{"id":"rezKi17VO0Oj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["今回は `V` という辞書のみを使い、各要素が即座に上書きされるようにした。  \n","前回が76回だったのに対し、今回は60回と少なくなっていることが確認できる。  \n","以降、反復方策評価アルゴリズムは「上書き方式」で実装する。"],"metadata":{"id":"wDXwLR0hQytZ"}},{"cell_type":"markdown","source":["## より大きな問題へ\n","\n","ここでは、「2マスのグリッドワールド」から「3×4のグリッドワールド」に問題の大きさをグレードアップする。  \n","問題の設定は次のとおりである。\n","\n","- エージェントは上下左右の4方向に移動できる\n","- \"X\" で表されたマス目は壁を表し、壁には入れない\n","- グリッドワールドは壁で囲われており、それ以上先に進むことはできない\n","- 壁にぶつかった場合の報酬は0とする\n","- 環境の状態遷移は一意に定まる\n","- エピソードタスクとして、報酬 $+1$ を取ったら終了とする"],"metadata":{"id":"cziBx4YPRvVL"}},{"cell_type":"markdown","source":["### GridWorld クラスの実装(*コーディング*)\n","\n","ここでは `GridWorld` クラスを実装する。"],"metadata":{"id":"BjkzK9A0lE0O"}},{"cell_type":"code","source":["# グリッドワールドのパラメータを設定\n","rows, cols = 5, 6\n","start = (3, 1)\n","goal = ()\n","obstacles = [\n","    (0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5),\n","    (1, 0), (1, 5), (2, 0), (2, 2), (2, 5), (3, 0),\n","    (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5)\n","]\n","items = {(1, 4): +1}\n","enemies = {(2, 4): -1}\n","\n","# グリッドワールドを作成して描画\n","grid, points = create_grid_world(\n","    rows, cols, start, goal, obstacles, items, enemies\n",")\n","draw_grid_world(grid, points, cols, rows)"],"metadata":{"id":"UIF2OFUMlD1e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上図を探索する `GridWorld` クラスを作成する。"],"metadata":{"id":"PrklNMP6nfKQ"}},{"cell_type":"code","source":["# GridWorld クラスの実装\n","class GridWorld:\n","    def __init__(self):\n","        self.action_space = [0, 1, 2, 3]\n","        self.action_meaning = {\n","            0: \"UP\",\n","            1: \"DOWN\",\n","            2: \"LEFT\",\n","            3: \"RIGHT\"\n","        }\n","        self.reward_map = np.array(\n","            [\n","                [0, 0, 0, 1.0],\n","                [0, None, 0, -1.0],\n","                [0, 0, 0, 0]\n","            ]\n","        )\n","        self.goal_state = (0, 3)\n","        self.wall_state = (1, 1)\n","        self.start_state = (2, 0)\n","        self.agent_state = self.start_state\n","\n","    @property\n","    def height(self):\n","        return len(self.reward_map)\n","\n","    @property\n","    def width(self):\n","        return len(self.reward_map[0])\n","\n","    @property\n","    def shape(self):\n","        return self.reward_map.shape\n","\n","    def actions(self):\n","        return self.action_space\n","\n","    def states(self):\n","        for h in range(self.height):\n","            for w in range(self.width):\n","                yield (h, w)\n","\n","    def next_state(self, state, action):\n","        action_move_map = [\n","            (-1, 0), (1, 0), (0, -1), (0, 1)\n","        ]\n","        move = action_move_map[action]\n","        next_state = (state[0] + move[0], state[1] + move[1])\n","        ny, nx = next_state\n","\n","        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:\n","            next_state = state\n","        elif next_state == self.wall_state:\n","            next_state = state\n","\n","        return next_state\n","\n","    def reward(self, state, action, next_state):\n","        return self.reward_map[next_state]"],"metadata":{"id":"FHv0J0KNn2NL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ここでは `@property` デコレータを使って `GridWorld` クラスに便利なインスタンス変数をいくつか実装している。  \n","これは、メソッド名の前行に配置することで対象のメソッドをインスタンス変数として使用することができる。"],"metadata":{"id":"lDgqBij69RtZ"}},{"cell_type":"code","source":["# GridWorld クラスのインスタンスを生成\n","env = GridWorld()\n","\n","# デコレータを使ったメソッドをインスタンス変数として表示できることの確認\n","print(env.height)\n","print(env.width)\n","print(env.shape)"],"metadata":{"id":"65eX4FQT-1fm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["また、 `GridWorld` クラスの `actions()` と `states()` というメソッドを使うことで、すべての行動、すべての状態に対して順にアクセスできる。"],"metadata":{"id":"Z4cvBjuC_hNP"}},{"cell_type":"code","source":["# actions メソッドからすべての行動を出力\n","for action in env.actions():\n","    print(action)\n","\n","# 分ける\n","print(\"=================\")\n","\n","# states メソッドからすべての状態を出力\n","for state in env.states():\n","    print(state)"],"metadata":{"id":"2_S_wOyP_grY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["続いて、環境の状態遷移を表すメソッド `next_state()` と報酬関数のメソッド `reward()` を確認する。\n","\n","`next_state()` メソッドの前半では、壁やグリッドワールドの枠は一旦無視して移動先の計算をする。  \n","後半で枠外や壁に移動していないか判定し、移動が出来なければ現在の状態のままとする。\n","\n","`reward()` メソッドは、報酬関数の数式の $r(s, a, s')$ の引数に対応させるためにこのような引数を使っている。"],"metadata":{"id":"sWG9dO4pNYpY"}},{"cell_type":"markdown","source":["### defaultdict を使う(*コーディング*)\n","\n","辞書型の変数の場合、 `key` に相当するものが存在しない場合にエラーが発生する。  \n","`collections` ライブラリの `defaultdict` を使うと辞書内に存在しない `key` にアクセスしても自動で生成してくれる。"],"metadata":{"id":"ZlGz3tOfPT6p"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from collections import defaultdict"],"metadata":{"id":"lzHtfuvSQKkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GridWorld のインスタンスを生成し、辞書 V を defaultdict で生成\n","env = GridWorld()\n","V = defaultdict(lambda: 0)\n","\n","# 辞書の中にない state を key に取って実行する\n","state = (1, 2)\n","print(V[state])"],"metadata":{"id":"CriJDd_lQRTA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["では、ランダムな方策を `defaultdict` で実装する。  \n","各行動 `[0, 1, 2, 3]` が均一にランダムに行われる場合、それぞれの行動が取られる確率は $\\frac{1}{4}$ である。  \n","これを踏まえて、ランダムな方策は次のように実装できる。"],"metadata":{"id":"aUcBA5MQQou5"}},{"cell_type":"code","source":["# 行動の確率を defaultdict の辞書に落とし込む\n","pi = defaultdict(\n","    lambda: {\n","        0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","    }\n",")\n","\n","# 辞書の中にない state を key に取って実行する\n","state = (0, 1)\n","print(pi[state])"],"metadata":{"id":"aA7KEQdZRMAH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 反復方策評価の実装(*コーディング*)\n","\n","ここでは、反復方策評価アルゴリズムを実装する。  \n","まずは、1ステップの更新だけを行うメソッドを実装する。"],"metadata":{"id":"Xo9IS00wX4AK"}},{"cell_type":"code","source":["# 1ステップの更新を行う関数を定義\n","def eval_onestep(\n","        pi: dict, V: dict, env: GridWorld, gamma: float = 0.9\n","    ) -> dict:\n","    for state in env.states():\n","        if state == env.goal_state:\n","            V[state] = 0\n","            continue\n","\n","        action_probs = pi[state]\n","        new_V = 0\n","\n","        for action, action_prob in action_probs.items():\n","            next_state = env.next_state(state, action)\n","            r = env.reward(state, action, next_state)\n","\n","            new_V += action_prob * (r + gamma * V[next_state])\n","\n","        V[state] = new_V\n","\n","    return V"],"metadata":{"id":"98KXKKesYU1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これで価値関数の1ステップの更新が終わるので、この更新を繰り返すメソッドを実装する。"],"metadata":{"id":"fusVoQiiamb_"}},{"cell_type":"code","source":["# 方策評価を行う関数を定義\n","def policy_eval(\n","        pi: dict, V: dict, env: GridWorld, gamma: float, threshold: float=0.001\n","    ) -> dict:\n","    while True:\n","        old_V = V.copy()\n","        V = eval_onestep(pi, V, env, gamma)\n","\n","        delta = 0\n","        for state in V.keys():\n","            t = abs(V[state] - old_V[state])\n","            if delta < t:\n","                delta = t\n","\n","        if delta < threshold:\n","            break\n","\n","    return V"],"metadata":{"id":"LGzNhpYOa0k-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これと `GridWorld` クラスを使って方策評価を行ってみる。"],"metadata":{"id":"AogVVVAecSLA"}},{"cell_type":"code","source":["# パラメータの初期化\n","env = GridWorld()\n","gamma = 0.9\n","pi = defaultdict(\n","    lambda: {\n","        0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","    }\n",")\n","V = defaultdict(lambda: 0)\n","\n","# 方策評価を行う\n","V = policy_eval(pi, V, env, gamma)\n","print(V)"],"metadata":{"id":"fgr_UBfccZW6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["それぞれの位置でのランダムな方策における価値関数が分かった。  \n","例えば(2, 0)の位置では-0.0989となっており、これはスタート地点からランダムに動くと得られる収益の期待値は-0.0989になるということである。  \n","全体的に見ると、(0, 0), (0, 1), (0, 2), (0, 3)と一番上の行以外が負の値を取っており、(1, 3)にある負の報酬の影響を強く受けているということがわかる。\n","\n","DP を使うことで効率的に方策の評価が行える。  \n","しかし、これはまだ方策の評価だけしか行っていないので、次は最適方策を見つける方法について考える。"],"metadata":{"id":"mYXPbe0begCw"}},{"cell_type":"markdown","source":["## 方策反復法\n","\n","我々の目標は最適方策を得ることであり、そのための1つの方法がベルマン方程式を満たす連立方程式を解くことであった。  \n","しかし、その方法には計算量的に問題があり、状態のサイズを $S$ 、行動のサイズを $A$ としたときに、解を求めるために $A^S$ のオーダーの計算量が必要になる。\n","\n","前節では、 DP を使って方策を評価した。  \n","それは「反復方策評価」と呼ばれるアルゴリズムであり、方策を「評価」できた。  \n","次は、方策の「改善」をする。"],"metadata":{"id":"pNGHd6NTmsvL"}},{"cell_type":"markdown","source":["### 方策の改善\n","\n","次の記号を用いて方策の改善をする方法について説明する。\n","\n","- 最適方策: $\\mu_*(s)$\n","- 最適方策における状態価値関数: $v_*(s)$\n","- 最適方策における行動価値関数: $q_*(s, a)$\n","\n","まずは復習として、最適方策 $\\mu_*$ は次の式で表された。\n","\n","\\begin{eqnarray*}\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu_*(s) &=& \\argmax_a q_*(s, a) \\\\\n","&=& \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}\n","\\end{eqnarray*}\n","\n","これは、最適方策 $\\mu_*$ に対しての式であるが、ここでは「何らかの決定論的方策 $\\mu$」に対して上記の式を適用することを考える。\n","\n","\\begin{eqnarray*}\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu'(s) &=& \\argmax_a q_\\mu(s, a) \\\\\n","&=& \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_\\mu(s') \\right\\}\n","\\end{eqnarray*}\n","\n","ここでは、\n","\n","- 現状の方策: $\\mu(s)$\n","- 方策 $\\mu(s)$ における状態価値関数: $v_\\mu(s)$\n","- 新たな方策: $\\mu'(s)$\n","\n","で表記する。  \n","またこの式で方策を更新することを「greedy 化」と呼ぶことにする。  \n","この greedy 化された方策 $\\mu'(s)$ にある性質の1つとして、もしすべての状態 $s$ において $\\mu(s)$ と $\\mu'(s)$ が同じであれば、方策 $\\mu(s)$ は既に最適方策であると言える。  \n","なぜなら、もし\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits} \\\\\n","\\mu'(s) = \\argmax_a q_\\mu(s, a)\n","$$\n","\n","によって方策が変更されなければ、次の式を満たすことになるからである。\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits} \\\\\n","\\mu(s) = \\argmax_a q_\\mu(s, a)\n","$$\n","\n","この式は最適方策が満たす式そのものである。  \n","そのため、 greedy 化を行ってもすべての状態 $s$ において $\\mu'(s)$ が更新されないのであれば、 $\\mu(s)$ は既に最適方策だということになる。  \n","方策 $\\mu'$ と方策 $\\mu$ が異なる場合、すべての状態 $s$ において $v_{\\mu'}(s) \\geq v_\\mu(s)$ が成り立つ。  \n","以上をまとめると、\n","\n","- 方策は常に改善される\n","- もし方策の改善がなければそれが最適方策である\n","\n","ということである。"],"metadata":{"id":"TQB5QKhhntvv"}},{"cell_type":"markdown","source":["### 評価と改善を繰り返す\n","\n","ここでは状態価値関数に基づく式\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu'(s) = \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_\\mu(s') \\right\\}\n","$$\n","\n","を想定して話を進める。  \n","我々は前節で状態価値関数を評価するアルゴリズムを実装した。  \n","この方法の処理の流れは次のようになる。\n","\n","- まずは $\\pi_0$ という方策からスタートする(確率的方策も考えられるので $\\mu_0(s)$ ではなく $\\pi_0(s|a)$ と表記する)\n","- 反復方策評価アルゴリズムにより、方策 $\\pi_0$ における価値関数を評価して $V_0$ を得る\n","- 価値関数 $V_0$ を使って greedy 化を行い、決定論的な方策 $\\mu_1$ を得る\n","\n","このフローを繰り返すことで、 greedy 化によって方策が変更されない地点に到達する。  \n","この評価と改善を繰り返すアルゴリズムを<font color=\"red\">**方策反復法**</font>(Policy Iteration)という。"],"metadata":{"id":"4Z6p0d7Gs9qX"}},{"cell_type":"markdown","source":["## 方策反復法の実装\n","\n","前と同じ「3×4のグリッドワールド」問題を扱う。  \n","目標は、方策反復法を使って最適方策を得ることである。"],"metadata":{"id":"a-u-mWSAvMua"}},{"cell_type":"markdown","source":["### 方策の改善(*コーディング*)\n","\n","方策を改善するには、現状の価値関数に対して greedy な方策を得る。\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu'(s) = \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_\\mu(s') \\right\\}\n","$$\n","\n","また、今回の問題では状態は一意に遷移する。  \n","そのため、方策の greedy 化は\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu'(s) = \\argmax_a \\left\\{ r(s, a, s') + \\gamma v_\\mu(s') \\right\\}\n","$$\n","\n","と簡略化できる。"],"metadata":{"id":"xyklv-5tvjSR"}},{"cell_type":"code","source":["# 辞書を受け取り、辞書の値が最大となる key を返す関数を定義\n","def argmax(d: dict) -> int:\n","    max_value = max(d.values())\n","    max_key = 0\n","\n","    for key, value in d.items():\n","        if value == max_value:\n","            max_key = key\n","\n","    return max_key"],"metadata":{"id":"h5f2g0fN55Vk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 行動価値の辞書を設定\n","action_values = {0: 0.1, 1: -0.3, 2: 9.9, 3: -1.3}\n","\n","# argmax 関数で行動を獲得\n","max_action = argmax(action_values)\n","print(max_action)"],"metadata":{"id":"sUmxuh_K6klq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["この `argmax` 関数を使って、価値関数を greedy 化する関数を実装する。"],"metadata":{"id":"Iv3fn6jT7K2z"}},{"cell_type":"code","source":["# 価値関数を greedy 化する関数を定義\n","def greedy_policy(\n","        V: dict, env: GridWorld, gamma: float\n","    ) -> dict:\n","    pi = {}\n","\n","    for state in env.states():\n","        action_values = {}\n","\n","        for action in env.actions():\n","            next_state = env.next_state(state, action)\n","            r = env.reward(state, action, next_state)\n","            value = r + gamma * V[next_state]\n","            action_values[action] = value\n","\n","        max_action = argmax(action_values)\n","        action_probs = {0: 0, 1: 0, 2: 0, 3: 0}\n","        action_probs[max_action] = 1.0\n","        pi[state] = action_probs\n","\n","    return pi"],"metadata":{"id":"fqfg29ol7TNK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 評価と改善を繰り返す(*コーディング*)\n","\n","評価と改善を繰り返す「方策反復法」の実装準備が整った。  \n","ここでは、方策反復法を `policy_iter()` という関数で実装する。  \n","引数は以下の通りである。\n","\n","- `env`: 環境\n","- `gammma`: 割引率\n","- `threshold`: 方策評価を行うときの更新を止めるための閾値"],"metadata":{"id":"0CY-dnuu83fY"}},{"cell_type":"code","source":["# 方策反復法の関数を定義\n","def policy_iter(\n","        env: GridWorld, gamma: float, threshold: float=0.001\n","    ) -> dict:\n","    pi = defaultdict(\n","        lambda: {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","    )\n","    V = defaultdict(lambda: 0)\n","\n","    while True:\n","        V = policy_eval(pi, V, env, gamma, threshold)\n","        new_pi = greedy_policy(V, env, gamma)\n","\n","        if new_pi == pi:\n","            break\n","\n","        pi = new_pi\n","\n","    return pi"],"metadata":{"id":"NM4W9gTF9swG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# パラメータの設定\n","env = GridWorld()\n","gamma = 0.9\n","\n","# 方策反復法で解く\n","pi = policy_iter(env, gamma)\n","print(pi)"],"metadata":{"id":"pHiJbzvy-5NP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これにて方策反復法を使って最適方策に辿り着くことができた。"],"metadata":{"id":"ch129PDy_v_U"}},{"cell_type":"markdown","source":["## 価値反復法\n","\n","方策反復法は、「評価」と「改善」を交互に繰り返す。  \n","「評価」のフェーズでは、方策 $\\mu$ を評価して $V_\\mu$ を得る。  \n","一方で、「改善」のフェーズでは、 $V$ を greedy 化する。\n","\n","実は、「評価」と「改善」という2つの作業を交互に繰り返す枠組みでは、「評価」を完全に行う前に「改善」フェーズに行ったり、「改善」を完全に行う前に「評価」フェーズへ行ったりする<font color=\"red\">**一般化方策反復**</font>(Generalized Policy Iteration)という概念がある。  \n","方策反復法では、「評価」と「改善」をそれぞれ最大限に行って、交互にフェーズを切り替えたが、これを最小限に抑えるのが<font color=\"red\">**価値反復法**</font>(Value Iteration)のアイデアになる。"],"metadata":{"id":"A5NIyvURPw2o"}},{"cell_type":"markdown","source":["### 価値反復法の導出\n","\n","方策反復法では、すべての状態における価値関数を何度も更新し、それが収束してから greedy 化のフェーズに移った。  \n","これと対極にある更新方法は、1つの状態だけを一度更新したらすぐに greedy 化のフェーズに移るというものである。  \n","このアイデアを数式を使って整理する。  \n","改善のフェーズで行う greedy 化は次のように表せた。\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu(s) = \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}\n","$$\n","\n","ここでは、現状の価値関数を $V(s)$ で表すことにする。  \n","続いて評価のフェーズだが、更新前の価値関数を $V(s)$ 、更新後の価値関数を $V'(s)$ とすれば DP による更新式は次のように表される。\n","\n","$$V'(s) = \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}$$\n","\n","方策 $\\pi(a|s)$ が確率的な方策として表記されている。  \n","ただし、「改善」のフェーズを一度経由することで方策は greedy な方策として表すことができる。  \n","そのため、決定論的な方策 $\\mu(s)$ として扱うことができ、次のように簡略化できる。\n","\n","$$V'(s) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}$$\n","\n","これが「評価」フェーズにおける価値関数の更新式である。\n","\n","2つの式\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu(s) = \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\} \\\\\n","V'(s) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}\n","$$\n","\n","では\n","\n","$$\\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}$$\n","\n","の部分で同じ計算を行っていることがわかるので、1つにまとめると\n","\n","$$V'(s) = \\max_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}$$\n","\n","となる。  \n","ここで注目すべき点は、方策 $\\mu$ が登場しないことである。  \n","方策を使わずに価値関数を更新しているので、最適価値関数を得るアルゴリズムは「価値反復法」と呼ばれる。  \n","また、ベルマン最適方程式\n","\n","$$v_*(s) = \\max_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}$$\n","\n","と比べると、これはベルマン最適方程式を「更新式」にしたものであることがわかる。  \n","さらに、この更新式は次のようにも表せる。\n","\n","$$V_{k+1}(s) = \\max_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V_k(s') \\right\\}$$\n","\n","価値反復法は無限回更新すると最適価値関数が得られるが、現実的にはどこかで止める必要がある。  \n","あらかじめ閾値を決めておき、すべての状態の更新量がその閾値を下回った場合に更新を止めるようにする。  \n","なお、 $V_*(s)$ が得られれば、最適方策 $\\mu_*(s)$ は次のように得られる。\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu_*(s) = \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V_*(s') \\right\\}\n","$$"],"metadata":{"id":"gZgpXSk-R9Te"}},{"cell_type":"markdown","source":["### 価値反復法の実装(*コーディング*)\n","\n","価値反復法の実装を行う。  \n","ここでも「3×4のグリッドワールド」問題を扱う。  \n","この問題では状態遷移は決定論的なので、価値関数の更新式は次のように簡略化できる。\n","\n","$$V'(s) = \\max_a \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}$$\n","\n","まずはこの式によって一度だけ更新する関数を実装する。"],"metadata":{"id":"riXSca1WcDRy"}},{"cell_type":"code","source":["# 一度だけ更新する関数の定義\n","def value_iter_onestep(\n","        V: dict, env: GridWorld, gamma: float\n","    ) -> dict:\n","    for state in env.states():\n","        if state == env.goal_state:\n","            V[state] = 0\n","            continue\n","\n","        action_values = []\n","        for action in env.actions():\n","            next_state = env.next_state(state, action)\n","            r = env.reward(state, action, next_state)\n","            value = r + gamma * V[next_state]\n","            action_values.append(value)\n","\n","        V[state] = max(action_values)\n","\n","    return V"],"metadata":{"id":"oE1i27CLfZxm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["この `value_iter_onestep` 関数を更新が収束するまで繰り返し呼ぶ関数 `value_iter` を実装する。"],"metadata":{"id":"fxi9KMa5gwCL"}},{"cell_type":"code","source":["# 更新を繰り返す関数を定義\n","def value_iter(\n","        V: dict, env: GridWorld, gamma: float, threshold: float=0.001\n","    ) -> dict:\n","    while True:\n","        old_V = V.copy()\n","        V = value_iter_onestep(V, env, gamma)\n","\n","        delta = 0\n","        for state in V.keys():\n","            t = abs(V[state] - old_V[state])\n","            if delta < t:\n","                delta = t\n","\n","        if delta < threshold:\n","            break\n","\n","    return V"],"metadata":{"id":"uUiGJqF6g_gr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["それではこれを使って見てみる。"],"metadata":{"id":"uLEac7UXn0Gu"}},{"cell_type":"code","source":["# パラメータの設定\n","V = defaultdict(lambda: 0)\n","env = GridWorld()\n","gamma = 0.9\n","\n","# 価値反復法で更新を繰り返す\n","V = value_iter(V, env, gamma)\n","\n","# 価値関数を greedy 化する\n","pi = greedy_policy(V, env, gamma)\n","print(pi)"],"metadata":{"id":"qROTzt83nzLj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["価値関数は最初は全てが0の要素を持つ辞書からスタートする。  \n","そこから更新を繰り返して価値関数の値が収束する。  \n","これで価値反復法を使って効率的に最適方策を得ることができた。"],"metadata":{"id":"ywG5XZDso8jo"}}]}