{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyMISoRqTtGWYEeRqek4RACb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第9章 〜方策勾配法〜\n","\n","これまで学んだ Q 学習や SARSA やモンテカルロ法などは、大別すれば<font color=\"red\">**価値ベースの手法**</font>(Value-based Method)に分類される。  \n","これらの価値ベースの手法では価値関数をモデル化し、価値関数を学習する。  \n","そして価値関数を経由して方策を得る。  \n","これらは<font color=\"red\">**一般化方策反復**</font>というアイデアに基づいて最適方策を見つけることが多く行われる。\n","\n","一方で、<font color=\"red\">**方策ベースの手法**</font>(Policy-based Method)は価値関数を経由せずに方策を直接表す。  \n","方策をニューラルネットワークなどでモデル化し、勾配を使って方策を最適化する手法は<font color=\"red\">**方策勾配法**</font>(Policy Gradient Method)と呼ばれる。  \n","ここでは、最も単純な方策勾配法から徐々に改善する流れで、様々な手法を扱う。"],"metadata":{"id":"877C2vDki6mE"}},{"cell_type":"markdown","source":["## 最も単純な方策勾配法\n","\n","方策勾配法は、勾配を使った方策を更新する手法の総称である。  \n","ここでは方策勾配法の中で最も単純なものを導出する。"],"metadata":{"id":"HiB_XSfAlY2E"}},{"cell_type":"markdown","source":["### 方策勾配法の導出\n","\n","確率的な方策は $\\pi(a|s)$ と表される。  \n","$\\pi(a|s)$ は状態 $s$ において $a$ という行動を取る確率を表す。  \n","方策をニューラルネットワークでモデル化するにあたって、全ての重みのパラメータを $\\theta$ で集約して表すことにする。  \n","そして、ニューラルネットワークによる方策を $\\pi_{\\theta}(a|s)$ と表すことにする。  \n","まず、次のような「状態、行動、報酬」からなる時系列データが得られたエピソードタスクを考える。\n","\n","$$\\tau = (S_0, A_0, R_0, S_1, A_1, \\cdots, S_{T+1})$$\n","\n","$\\tau$ は<font color=\"red\">**軌道**</font>(Trajectory)とも呼ばれる。  \n","このとき、収益は割引率 $\\gamma$ を使って\n","\n","$$G(\\tau) = R_0 + \\gamma R_1 + \\gamma^2 R_2 + \\cdots + \\gamma^T R_T$$\n","\n","と設定できる。  \n","ここでは、収益が $\\tau$ から計算できることを明示するために $G(\\tau)$ と表記している。  \n","このとき、目的関数 $J(\\theta)$ は\n","\n","$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[G(\\tau)]$$\n","\n","と表される。  \n","収益 $G(\\tau)$ は確率的に変動するので、その期待値が目的関数となる。  \n","目的関数が決まれば、次はその勾配を求める。  \n","ここではパラメータ $\\theta$ に関する勾配を $\\nabla_\\theta$ で表すことにする。\n","\n","\\begin{eqnarray*}\n","\\nabla_\\theta J(\\theta) &=& \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[G(\\tau)] \\\\\n","&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right]\n","\\end{eqnarray*}\n","\n","そして、ニューラルネットワークのパラメータの更新は、\n","\n","$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$\n","\n","と勾配方向に学習率 $\\alpha$ だけ更新する。"],"metadata":{"id":"BmgaNWRDllTA"}},{"cell_type":"markdown","source":["### 方策勾配法のアルゴリズム\n","\n","上記で求めた期待値 $\\nabla_\\theta J(\\theta)$ はモンテカルロ法によって求められる。  \n","方策 $\\pi_{\\theta}$ のエージェントに実際に行動させ、軌道 $\\tau$ を $n$ 個得たとする。  \n","その場合、各 $\\tau$ において $\\sum_{t=0}^T G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t)$ を計算して、その平均を求めることで $\\nabla_\\theta J(\\theta)$ を近似できる。\n","\n","$$\n","\\text{sampling: } \\tau^{(i)} \\sim \\pi_{\\theta} \\quad (i = 1, 2, \\cdots, n) \\\\\n","x^{(i)} = \\sum_{t=0}^T G(\\tau^{(i)}) \\nabla_\\theta \\log \\pi_{\\theta} \\left( A_t^{(i)} | S_t^{(i)} \\right) \\\\\n","\\nabla_\\theta J(\\theta) \\simeq \\frac{x^{(1)} + x^{(2)} + \\cdots + x^{(n)}}{n}\n","$$\n","\n","ここでは $i$ 番目のエピソードで得られた軌道を $\\tau^{(i)}$ 、 $i$ 番目のエピソードの時刻 $t$ における行動を $A_t^{(i)}$ 、状態を $S_t^{(i)}$ で表すことにする。  \n","$n = 1$ の場合を考えると、\n","\n","$$\n","\\text{sampling: } \\tau \\sim \\pi_{\\theta} \\\\\n","\\nabla_\\theta J(\\theta) \\simeq \\sum_{t=0}^T G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t)\n","$$\n","\n","である。  \n","この式は、 $\\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t)$ を全ての時刻 $(t = 0 \\sim T)$ で求め、各勾配に「重み」として収益 $G(\\tau)$ をかけてそれらの和を求める。  \n","すなわち、\n","\n","$$G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_0|S_0) + G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_1|S_1) + \\cdots + G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_T|S_T)$$\n","\n","である。  \n","この計算の「意味」を考える。  \n","まずは $\\log$ の微分により、\n","\n","$$\\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) = \\frac{\\nabla_\\theta \\pi_{\\theta}(A_t|S_t)}{\\pi_{\\theta}(A_t|S_t)}$$\n","\n","が成り立つ。  \n","$\\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t)$ は、 $\\nabla_\\theta \\pi_{\\theta}(A_t|S_t)$ という勾配を $\\frac{1}{\\pi_{\\theta}(A_t|S_t)}$ 倍したものであり、これより $\\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t)$ と $\\nabla_\\theta \\pi_{\\theta}(A_t|S_t)$ は同じ方向を指すことがわかる。  \n","$\\nabla_\\theta \\pi_{\\theta}(A_t|S_t)$ は、状態 $S_t$ で行動 $A_t$ を取る確率が最も増える方向を指す。  \n","同じく $\\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t)$ も状態 $S_t$ で行動 $A_t$ を取る確率が最も増える方向を指す。  \n","そこに「重み」 $G(\\tau)$ がかけられる。  \n","これは、上手くいったら上手くいった分だけそれまでに取った行動が強められ、反対に、上手くいかなかった場合、その間に取った行動はその分だけ弱められることを「意味」する。"],"metadata":{"id":"W9eL_YlZt4mh"}},{"cell_type":"markdown","source":["### 方策勾配法の実装(*コーディング*)\n","\n","続いて、最も単純な方策勾配法の実装に移る。"],"metadata":{"id":"xugXSemLNnhk"}},{"cell_type":"code","source":["!pip install -U japanize-matplotlib"],"metadata":{"id":"OzNE_smnWa2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7P75u1qhsXe"},"outputs":[],"source":["# ライブラリのインポート\n","import gym\n","import japanize_matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.distributions import Categorical\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"code","source":["# ニューラルネットワークの方策を表す Policy クラスの実装\n","class Policy(nn.Module):\n","    def __init__(self, action_size):\n","        super().__init__()\n","        self.l1 = nn.Linear(4, 128)\n","        self.l2 = nn.Linear(128, action_size)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.l1(x)\n","        x = self.relu(x)\n","        x = self.l2(x)\n","        x = self.softmax(x)\n","        return x\n","\n","\n","# Agent クラスの実装\n","class Agent:\n","    def __init__(self):\n","        self.gamma = 0.98\n","        self.lr = 0.0002\n","        self.action_size = 2\n","\n","        self.memory = []\n","        self.pi = Policy(self.action_size)\n","        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)\n","\n","    def get_action(self, state):\n","        state = torch.tensor(state[np.newaxis, :])\n","        probs = self.pi(state)\n","        probs = probs[0]\n","        m = Categorical(probs)\n","        action = m.sample().item()\n","        return action, probs[action]\n","\n","    def add(self, reward, prob):\n","        data = (reward, prob)\n","        self.memory.append(data)\n","\n","    def update(self):\n","        G, loss = 0, 0\n","        for reward, prob in reversed(self.memory):\n","            G = reward + self.gamma * G\n","\n","        for reward, prob in self.memory:\n","            loss += - torch.log(prob) * G\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        self.memory = []"],"metadata":{"id":"32NSvItfOzdz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 報酬をプロットする関数を定義\n","def reward_show(history: list) -> None:\n","    x = [i for i in range(1, len(history) + 1)]\n","\n","    plt.figure(figsize=(8, 6), tight_layout=True)\n","    plt.title(\"報酬の遷移\", size=15, color=\"red\")\n","    plt.grid()\n","    plt.plot(x, history)"],"metadata":{"id":"itNIXYJcWRTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = gym.make(\"CartPole-v1\")\n","agent = Agent()\n","\n","# 3000回のエピソードで実行\n","episodes = 3000\n","reward_history = []\n","for episode in range(episodes):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        action, prob = agent.get_action(state)\n","        next_state, reward, done, info = env.step(action)\n","\n","        agent.add(reward, prob)\n","        state = next_state\n","        total_reward += reward\n","\n","    agent.update()\n","\n","    reward_history.append(total_reward)\n","    if episode % 100 == 0:\n","        print(f\"episode :{episode}, total reward : {total_reward}\")\n","\n","# 報酬の遷移画像を描画\n","reward_show(reward_history)"],"metadata":{"id":"7JhXGW_LVa3p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["大きな変動はあるが、エピソードが進むにつれて徐々に良い結果が得られている。"],"metadata":{"id":"Od2yg15CaH0x"}},{"cell_type":"markdown","source":["## REINFORCE\n","\n","前節の方策勾配法では、報酬の総和がエピソードを重ねるごとに向上する一方で、後半の方でも低い値を取ってしまっている。  \n","REINFORCE はこれを改善した手法であり、ここでは数式ベースでアルゴリズムの導出を行う。  \n","そして、前のコードを一部修正する形で REINFORCE を実装する。"],"metadata":{"id":"c1nBvYJdaSAs"}},{"cell_type":"markdown","source":["### REINFORCE アルゴリズム\n","\n","最も単純な勾配方策法は次の式に基づいて実装される。\n","\n","$$\n","\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T G(\\tau) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right]\n","$$\n","\n","エージェントの行動の良し悪しは、その行動の<font color=\"red\">**後**</font>に得られた報酬の総和によって評価される。  \n","逆に、ある行動を起こす<font color=\"red\">**前**</font>に得られた報酬は、その行動の良し悪しとは関係がない。  \n","行動 $A_t$ に対する重みは $G(\\tau)$ であり、これには時刻 $t$ よりも前の報酬も含まれる。  \n","これを改善するには、重み $G(\\tau)$ を次のように変更することが考えられる。\n","\n","$$\n","\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T G_t \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right] \\\\\n","G_t = R_t + \\gamma R_{t+1} + \\cdots + \\gamma^{T-1}R_T\n","$$\n","\n","これにより、行動 $A_t$ の選ばれる確率は、時刻 $t$ より前の報酬を含まない重み $G_t$ によって強められる。"],"metadata":{"id":"Yk_ufvdvfm8_"}},{"cell_type":"markdown","source":["### REINFORCE の実装(*コーディング*)\n","\n","REINFORCE は分散が小さいので、データのサンプル数が少なくても精度良く近似できる。  \n","前節の `Agent` クラスの `update` メソッドのみ変更を加える。"],"metadata":{"id":"52c1QUoDjTMp"}},{"cell_type":"code","source":["# REINFORCE 用の Agent クラスの実装\n","class Agent:\n","    def __init__(self):\n","        self.gamma = 0.98\n","        self.lr = 0.0002\n","        self.action_size = 2\n","\n","        self.memory = []\n","        self.pi = Policy(self.action_size)\n","        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)\n","\n","    def get_action(self, state):\n","        state = torch.tensor(state[np.newaxis, :])\n","        probs = self.pi(state)\n","        probs = probs[0]\n","        m = Categorical(probs)\n","        action = m.sample().item()\n","        return action, probs[action]\n","\n","    def add(self, reward, prob):\n","        data = (reward, prob)\n","        self.memory.append(data)\n","\n","    def update(self):\n","        G, loss = 0, 0\n","        for reward, prob in reversed(self.memory):\n","            G = reward + self.gamma * G\n","            loss += - torch.log(prob) * G\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        self.memory = []"],"metadata":{"id":"_9bD5ixKjtkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = gym.make(\"CartPole-v1\")\n","agent = Agent()\n","\n","# 3000回のエピソードで実行\n","episodes = 3000\n","reward_history = []\n","for episode in range(episodes):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        action, prob = agent.get_action(state)\n","        next_state, reward, done, info = env.step(action)\n","\n","        agent.add(reward, prob)\n","        state = next_state\n","        total_reward += reward\n","\n","    agent.update()\n","\n","    reward_history.append(total_reward)\n","    if episode % 100 == 0:\n","        print(f\"episode :{episode}, total reward : {total_reward}\")\n","\n","# 報酬の遷移画像を描画\n","reward_show(reward_history)"],"metadata":{"id":"tTMiMXMjj9xu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["前節の勾配方策法と比べて安定した学習が行われており、上限の500に近づいている。"],"metadata":{"id":"kQbl3Db_lsXp"}},{"cell_type":"markdown","source":["## ベースライン\n","\n","REINFORCE を改善するために、<font color=\"red\">**ベースライン**</font>(Baseline)という技術を導入する。"],"metadata":{"id":"0FeiO8_cl-T9"}},{"cell_type":"markdown","source":["### ベースラインのアイデア\n","\n","A 、 B 、 C の3人がテストを受けて、それぞれ90点、40点、50点を取ったとする。\n","\n","|  |  |\n","| :--: | :--: |\n","| A | 90 |\n","| B | 40 |\n","| C | 50 |\n","\n","これに対して分散を求めると、466.6667と大きな値を取る。  \n","この分散を減らす方法を考える。\n","\n","|  | 1回目のテスト | 2回目のテスト | ... | 10回目のテスト |\n","| :--: | :--: | :--: | :--: | :--: |\n","| A | 92 | 80 | ... | 74 |\n","| B | 32 | 51 | ... | 56 |\n","| C | 45 | 53 | ... | 49 |\n","\n","このように、これまでのテスト結果があれば過去のテストを平均することで、次のテスト結果は過去の平均点からの「差分」として予測できる。  \n","上の表の結果をそれぞれ平均すると、 A は82点、 B は46点、 C は49点になったとする。  \n","これを「予測値」として使い、対象とするテスト結果との差分について考える。\n","\n","|  | 今回のテスト | 平均 | 差分 |\n","| :--: | :--: | :--: | :--: |\n","| A | 90 | 82 | 8 |\n","| B | 40 | 46 | -6 |\n","| C | 50 | 49 | 1 |\n","\n","差分について分散を求めると、32.6667となり、最初と比べると大きく値を減らすことができた。  \n","この例で示すように、ある結果に対して予測値を引くことで分散を減らすことができる。  \n","予測値の精度が高ければ高いほど、分散は小さくなる。  \n","これがベースラインのアイデアである。"],"metadata":{"id":"tUsiKAmD249N"}},{"cell_type":"markdown","source":["### ベースライン付きの方策勾配法\n","\n","REINFORCE にベースラインを適用すると、次のようになる。\n","\n","\\begin{eqnarray*}\n","\\nabla_\\theta J(\\theta) &=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T G_t \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right] \\\\\n","&=& \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T (G_t - b(S_t)) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right]\n","\\end{eqnarray*}\n","\n","$G_t$ の代わりに $G_t - b(S_t)$ を使うことができる。  \n","ここで、 $b(S_t)$ は入力を $S_t$ とする任意の関数であり、これがベースラインである。  \n","実践的によく使われるのは価値関数であり、 $b(S_t) = V_{\\pi_{\\theta}}(S_t)$ となる。  \n","ベースラインを使って分散を小さくすることができれば、サンプル効率の良い学習が行える。  \n","なお、ベースラインとして価値関数を使う場合、真の価値関数 $v_{\\pi_{\\theta}}(S_t)$ を知ることはできず、この場合、価値関数についても学習をする必要がある。"],"metadata":{"id":"_ldetFs-ATwI"}},{"cell_type":"markdown","source":["## Actor-Critic\n","\n","強化学習のアルゴリズムは、大別すると価値ベースの手法と方策ベースの手法に分けられるが、その両方を使用する手法も考えられる。  \n","Actor-Critic もその一つであり、ベースライン付きの REINFORCE をさらに推し進めた手法である。"],"metadata":{"id":"gtOCiKHIGrfX"}},{"cell_type":"markdown","source":["### Actor-Critic の導出\n","\n","ベースライン付きの REINFORCE では、目的関数の勾配は次の式で表された。\n","\n","$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T (G_t - b(S_t)) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right]$$\n","\n","ここで、 $G_t$ は収益、 $b(S_t)$ はベースラインを表している。  \n","ベースラインには任意の関数を用いることができるので、ここでは、ニューラルネットワークでモデル化した価値関数を使う。  \n","それにあたり、次の記号を新たに使用する。\n","\n","- $w$ : 価値関数を表すニューラルネットワークの全ての重みパラメータ\n","- $V_w(S_t)$ : 価値関数をモデル化したニューラルネットワーク\n","\n","この場合の目的関数の勾配は次の式で表される。\n","\n","$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T (G_t - V_w(S_t)) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right]$$\n","\n","この式には問題があり、それは収益 $G_t$ はゴールに達しないと値が定まらないことである。  \n","つまり、ゴールに達するまで方策や価値関数の更新ができないということである。  \n","これを解決するために TD 法が導入された。  \n","上記の式を TD 法へ切り替えると次のような式が得られる。\n","\n","$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T (R_t + \\gamma V_w(S_{t+1}) - V_w(S_t)) \\nabla_\\theta \\log \\pi_{\\theta}(A_t|S_t) \\right]$$\n","\n","この式に基づく手法が Actor-Critic である。"],"metadata":{"id":"TbJ122uECZqb"}},{"cell_type":"markdown","source":["### Actor-Critic の実装(*コーディング*)\n","\n","Actor-Critic の実装に移る。"],"metadata":{"id":"i2fbgPaZHNdj"}},{"cell_type":"code","source":["# 価値関数を表す ValueNet クラスの実装\n","class ValueNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.l1 = nn.Linear(4, 128)\n","        self.l2 = nn.Linear(128, 1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.l1(x)\n","        x = self.relu(x)\n","        x = self.l2(x)\n","        return x\n","\n","\n","# A 用の Agent クラスの実装\n","class Agent:\n","    def __init__(self):\n","        self.gamma = 0.98\n","        self.lr_pi = 0.0002\n","        self.lr_v = 0.0005\n","        self.action_size = 2\n","\n","        self.pi = Policy(self.action_size)\n","        self.v = ValueNet()\n","\n","        self.optimizer_pi = optim.Adam(self.pi.parameters(), lr=self.lr_pi)\n","        self.optimizer_v = optim.Adam(self.v.parameters(), lr=self.lr_v)\n","\n","    def get_action(self, state):\n","        state = torch.tensor(state[np.newaxis, :])\n","        probs = self.pi(state)\n","        probs = probs[0]\n","        m = Categorical(probs)\n","        action = m.sample().item()\n","        return action, probs[action]\n","\n","    def update(self, state, action_prob, reward, next_state, done):\n","        state = torch.tensor(state[np.newaxis, :])\n","        next_state = torch.tensor(next_state[np.newaxis, :])\n","\n","        target = reward + self.gamma * self.v(next_state) * (1 - done)\n","        target.detach()\n","        v = self.v(state)\n","        loss_fn = nn.MSELoss()\n","        loss_v = loss_fn(v, target)\n","\n","        delta = target - v\n","        loss_pi = -torch.log(action_prob) * delta.item()\n","\n","        self.optimizer_v.zero_grad()\n","        self.optimizer_pi.zero_grad()\n","        loss_v.backward()\n","        loss_pi.backward()\n","        self.optimizer_v.step()\n","        self.optimizer_pi.step()"],"metadata":{"id":"MWm2HpJcHVCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = gym.make(\"CartPole-v1\")\n","agent = Agent()\n","\n","# 3000回のエピソードで実行\n","episodes = 3000\n","reward_history = []\n","for episode in range(2000):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        action, prob = agent.get_action(state)\n","        next_state, reward, done, info = env.step(action)\n","\n","        agent.update(state, prob, reward, next_state, done)\n","\n","        state = next_state\n","        total_reward += reward\n","\n","    reward_history.append(total_reward)\n","    if episode % 100 == 0:\n","        print(f\"episode :{episode}, total reward : {total_reward}\")\n","\n","# 報酬の遷移画像を描画\n","reward_show(reward_history)"],"metadata":{"id":"wIOu2CQUzqaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["グラフを見ると、順調に学習が行われていることがわかる。"],"metadata":{"id":"ZT2HnBVY344G"}},{"cell_type":"markdown","source":["## 方策ベースの手法の利点\n","\n","最後に、方策ベースの利点について説明する。\n","\n","1. ***方策を直接モデル化するので効率的***  \n","    価値ベースの手法は、価値関数を推定して、それを基に方策を決める。  \n","    一方、方策ベースの手法は方策を「直接」推定する。  \n","    問題によっては価値関数は複雑な形状をしながらも、最適方策は単純な場合に考えられる。  \n","    そのような場合、方策ベースの手法の方がより速く学習できることが期待できる。\n","\n","\n","2. ***連続的な行動空間でも使える***  \n","    これまでのカートポールの場合は、右か左かの2つの行動のどちらかを選ぶ。  \n","    そのような離散行動空間では、行動はいくつかの候補の中から1つを選択する。  \n","    一方で、 OpenAI Gym の「Pendulum」のような連続的な行動空間も考えられ、価値ベースの手法では適用が難しくなる。  \n","    対策として「量子化」するなどで離散化する必要があるが、多くの場合、良い方法は試行錯誤して探す必要がある。  \n","    しかし、方策ベースでは、たとえばニューラルネットワークの出力が正規分布を想定した場合、ニューラルネットワークは正規分布の平均と分散を出力することが考えられる。  \n","    その平均と分散を基にサンプリングすることで連続値が得られる。\n","\n","\n","3. ***行動の選択確率がスムーズに変化する***  \n","    価値ベースの手法では、エージェントの行動は ε-greedy 法によって選ばれることが多い。  \n","    その場合、基本的には Q 関数の一番大きな行動が選ばれる。  \n","    このとき、 Q 関数の更新により最大値となる行動が変わると、行動の取り方が急に変わることになる。  \n","    一方、方策ベースの手法は、ソフトマックス関数によって各行動の確率が決まる。  \n","    そのため、方策のパラメータを更新していく過程で各行動の確率はスムーズに変わり、方策勾配法の学習は安定しやすくなる。"],"metadata":{"id":"GGiAU3y_3-qO"}}]}