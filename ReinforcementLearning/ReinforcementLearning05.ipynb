{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNSnSqJqi3APw/91wlTIOHv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第5章 〜モンテカルロ法〜\n","\n","環境のモデルが既知の場合は DP で最適価値関数と最適方策を得ることができた。  \n","しかし、環境のモデルが未知の場合があるため、すべての問題で扱えるわけではない。  \n","ここでは、データのサンプリングを繰り返し行って、その結果から推定する「<font color=\"red\">**モンテカルロ法**</font>」について扱う。"],"metadata":{"id":"Vrv_D1j1qCGx"}},{"cell_type":"markdown","source":["## モンテカルロ法の基礎\n","\n","強化学習では、モンテカルロ法(Monte Carlo Method)を使うことで、経験から価値関数を推定することができる。  \n","ここに出てくる「経験」とは、具体的には環境とエージェントがやり取りを行って得られた「状態、行動、報酬」の一連のデータのことを指す。\n","\n","さて、今まで扱ってきたグリッドワールドの問題などとは異なり、現実にある多くの問題では環境のモデルを知ることはできない。  \n","例えば、「商品の在庫管理」を行う問題を考えたときに、「商品がどれだけ売れるか」ということが環境の状態遷移確率に相当する。  \n","しかし、商品の売れ行きは様々な要因が複雑に絡み合っているため、それを完全に知ることは現実的には不可能である。  \n","もし、環境の状態遷移確率を理論的に知ることができたとしても、そのために必要な計算は多くの場合大変である。"],"metadata":{"id":"rIIoKZ4q6xz-"}},{"cell_type":"markdown","source":["### サイコロの目の和(*コーディング*)\n","\n","ここでは、2つのサイコロのを振る問題を考える。  \n","サイコロの各目の出る確率は均一に $\\frac{1}{6}$ とする。  \n","すると、確率分布表は次のようになる。\n","\n","| サイコロの<br>目の和 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |\n","| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n","| 確率 | $\\frac{1}{36}$ | $\\frac{2}{36}$ | $\\frac{3}{36}$ | $\\frac{4}{36}$ | $\\frac{5}{36}$ | $\\frac{6}{36}$ | $\\frac{5}{36}$ | $\\frac{4}{36}$ | $\\frac{3}{36}$ | $\\frac{2}{36}$ | $\\frac{1}{36}$ |\n","\n","これの期待値を計算してみる。"],"metadata":{"id":"5G-AsqXH8YgR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4l78y6xMp8cU"},"outputs":[],"source":["# サイコロの確率分布\n","ps = {\n","    2: 1/36, 3: 2/36, 4: 3/36, 5: 4/36, 6: 5/36, 7: 6/36,\n","    8: 5/36, 9: 4/36, 10: 3/36, 11: 2/36, 12: 1/36\n","}\n","\n","# 期待値の計算\n","V = 0\n","for x, p in ps.items():\n","    V += x * p\n","print(V)"]},{"cell_type":"markdown","source":["このように、確率分布が分かれば期待値の計算を行うことができる。"],"metadata":{"id":"hmlrd4dm-moP"}},{"cell_type":"markdown","source":["### 分布モデルとサンプルモデル(*コーディング*)\n","\n","サイコロを振るという試行を「確率分布」としてモデル化した。  \n","このように確率分布として表されたモデルは<font color=\"red\">**分布モデル**</font>(Distribution Model)と呼ぶことができる。  \n","一方で、サンプリングさえできれば良いという<font color=\"red\">**サンプルモデル**</font>(Sample Model)というものがある。  \n","サイコロの例でいうと、実際にサイコロを振り、その目の和を観測するという方法である。  \n","分布モデルが確率分布を明示的に保持できることが条件であるのに対し、サンプルモデルはサンプリングできることが条件である。  \n","それではサンプルモデルを実装し、2つのサイコロの目の和を観測する。"],"metadata":{"id":"OZ0bx-SJ-shy"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import numpy as np"],"metadata":{"id":"S4B3BXAjAG8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# サイコロの目の和を出す関数を定義\n","def sample(dices: int=2) -> int:\n","    x = 0\n","    for _ in range(dices):\n","        x += np.random.choice([1, 2, 3, 4, 5, 6])\n","    return x"],"metadata":{"id":"Ch_niwFbAJf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 実験をする\n","sample_a = sample()\n","print(f\"sample a: {sample_a}\")\n","sample_b = sample()\n","print(f\"sample b: {sample_b}\")\n","sample_c = sample()\n","print(f\"sample c: {sample_c}\")"],"metadata":{"id":"cIiBEQ4aAsoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["このように2つのサイコロを「サンプルモデル」として実装することができた。  \n","これを使って期待値を計算してみる。"],"metadata":{"id":"blnoaUCyBFa9"}},{"cell_type":"markdown","source":["### モンテカルロ法の実装(*コーディング*)\n","\n","モンテカルロ法によって期待値を求めるコードを書いてみる。"],"metadata":{"id":"_LtCM_RIBTXI"}},{"cell_type":"code","source":["# サンプリング回数\n","trial = 1000\n","\n","# サンプルを繰り返し観測する\n","samples = []\n","for _ in range(trial):\n","    s = sample()\n","    samples.append(s)\n","\n","# 期待値を算出する\n","V = sum(samples) / len(samples)\n","print(V)"],"metadata":{"id":"fpJeQHO-BhAn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7が期待値として正解なので、おおよそ正しいことがわかる。  \n","モンテカルロ法は、サンプル数を増やすことで信頼性が高まる。  \n","すなわち、分散が小さくなるということだ。  \n","続いて、サンプルデータを得るたびに平均値を求めたい場合を考える。"],"metadata":{"id":"q7vMUvxQB9h5"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import matplotlib.pyplot as plt\n","from numpy.typing import ArrayLike"],"metadata":{"id":"SQf1FOq2CyqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# サンプリングした結果をプロットする関数を定義\n","def plot_sampling(x: ArrayLike, y: ArrayLike) -> None:\n","    plt.figure(figsize=(18, 12))\n","\n","    plt.xlabel(\"samples\")\n","    plt.xlim(min(x) - 1, max(x) + 1)\n","    plt.grid()\n","\n","    plt.plot(x, y)\n","    plt.show()"],"metadata":{"id":"Y65aT68KDk40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# サンプリング回数\n","trial = 1000\n","\n","# サンプルを繰り返し観測し、毎回期待値を算出する\n","samples = []\n","means = []\n","for _ in range(trial):\n","    s = sample()\n","    samples.append(s)\n","    V = sum(samples) / len(samples)\n","    means.append(V)\n","\n","# 繰り返しごとの期待値の推移をプロット\n","x_axis = [i for i in range(1, 1001)]\n","plot_sampling(x_axis, means)"],"metadata":{"id":"4BWKjqFZCYVH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これをみると、だんだん期待値が7に収束していることがわかる。  \n","次は、強化学習の問題に対してモンテカルロ法を適用する。"],"metadata":{"id":"MymkPEi0Gwao"}},{"cell_type":"markdown","source":["## モンテカルロ法による方策評価\n","\n","強化学習の問題にモンテカルロ法を適用するには、エージェントが実際に行動して得た経験から価値関数を推定するということを考える。  \n","ここでは、方策 $\\pi$ が与えられたときに、その方策の価値関数をモンテカルロ法を使って計算する。"],"metadata":{"id":"BjgVRUoxG7Wt"}},{"cell_type":"markdown","source":["### 価値関数をモンテカルロ法で求める\n","\n","価値関数は次の式で表された。\n","\n","$$v_{\\pi}(s) = \\mathbb{E}_\\pi[G|s]$$\n","\n","ここでは、状態 $s$ からスタートして得られる収益を $G$ で表す。  \n","価値関数 $c_\\pi(s)$ は方策 $\\pi$ に従って行動したときに得られる収益の期待値として定義される。  \n","ここでは、エピソードタスクを想定し、ある時刻にはゴールに辿り着くことを考える。  \n","モンテカルロ法を使って計算するには、方策 $\\pi$ に従ってエージェントに行動させ、得られた実際の収益をサンプルデータとする。\n","\n","$$V_\\pi(s) = \\frac{G^{(1)} + G^{(2)} + \\cdots + G^{(n)}}{n}$$\n","\n","$i$ 回目のエピソードで得られた収益を $G^{(i)}$ と表記している。  \n","モンテカルロ法で計算するには、 $n$ 回のエピソードを行い、そこで得られたサンプルデータの平均を求める。"],"metadata":{"id":"o5s5MsbHIB3W"}},{"cell_type":"markdown","source":["### すべての状態の価値関数を求める\n","\n","ここでは、モンテカルロ法を使ってすべての状態において価値関数を求めてみる。  \n","単純に考えれば、開始する状態を変更して、1つの状態だけの価値関数を求めることを繰り返せば達成できるが、それでは計算効率上の問題がある。  \n","それは、ある1つの状態からスタートして得られたサンプルデータは、その価値関数を計算するためだけに使われており、他の状態の価値関数の計算には貢献していないということだ。  \n","例えば、3つの状態 $A, B, C$ があり、 $A \\rightarrow B \\rightarrow C$ の順に状態を経てゴールに辿り着くとする。  \n","その間に得られる報酬を $R_A, R_B, R_C$ とし、このタスクでの割引率を $\\gamma$ とすると、状態 $A$ からスタートして得られる収益は次のように表せる。\n","\n","$$G_A = R_A + \\gamma R_B + \\gamma^2 R_C$$\n","\n","続いて状態 $B$ から下の遷移を考えると、これは状態 $B$ からスタートした場合の収益サンプルデータと見なせるので、この収益は次のように表せる。\n","\n","$$G_B = R_B + \\gamma R_C$$\n","\n","同様に考えると、状態 $C$ からスタートして得られる収益は\n","\n","$$G_C = R_C$$\n","\n","と考えられる。  \n","このように、1つの施行だけから3つの状態に対する収益が得られる。"],"metadata":{"id":"NBcmkKnswTAp"}},{"cell_type":"markdown","source":["### モンテカルロ法の効率の良い実装\n","\n","先ほどの例から、次の3つの収益を計算する必要がある。\n","\n","\\begin{eqnarray*}\n","G_A &=& R_A + \\gamma R_B + \\gamma^2 R_C \\\\\n","G_B &=& R_B + \\gamma R_C \\\\\n","G_C &=& R_C\n","\\end{eqnarray*}\n","\n","ここで特に難しい計算はないが、 $G_C \\rightarrow G_B \\rightarrow G_A$ の順番で次のように式変形することができる。\n","\n","\\begin{eqnarray*}\n","G_C &=& R_C \\\\\n","G_B &=& R_B + \\gamma G_C \\\\\n","G_A &=& R_A + \\gamma G_B\n","\\end{eqnarray*}\n","\n","このように後ろから順に計算することで効率化できる。"],"metadata":{"id":"MvoendsDzXsE"}},{"cell_type":"markdown","source":["## モンテカルロ法の実装\n","\n","前章で解いた「3×4のグリッドワールド」の問題をモンテカルロ法で解いてみる。  \n","`GridWorld` クラスに新しく `step` というメソッドを加える。"],"metadata":{"id":"BdmetP550ycJ"}},{"cell_type":"markdown","source":["### step メソッド(*コーディング*)\n","\n","今回は、環境のモデルを使わずに方策評価を行うので、エージェントに実際に行動を行わせるための `step` メソッドが必要になる。"],"metadata":{"id":"EYgAXdr_VGVM"}},{"cell_type":"code","source":["# step メソッドと reset メソッドを追加した GridWorld クラスの実装\n","class GridWorld:\n","    def __init__(self):\n","        self.action_space = [0, 1, 2, 3]\n","        self.action_meaning = {\n","            0: \"UP\",\n","            1: \"DOWN\",\n","            2: \"LEFT\",\n","            3: \"RIGHT\"\n","        }\n","        self.reward_map = np.array(\n","            [\n","                [0, 0, 0, 1.0],\n","                [0, None, 0, -1.0],\n","                [0, 0, 0, 0]\n","            ]\n","        )\n","        self.goal_state = (0, 3)\n","        self.wall_state = (1, 1)\n","        self.start_state = (2, 0)\n","        self.agent_state = self.start_state\n","\n","    @property\n","    def height(self):\n","        return len(self.reward_map)\n","\n","    @property\n","    def width(self):\n","        return len(self.reward_map[0])\n","\n","    @property\n","    def shape(self):\n","        return self.reward_map.shape\n","\n","    def actions(self):\n","        return self.action_space\n","\n","    def states(self):\n","        for h in range(self.height):\n","            for w in range(self.width):\n","                yield (h, w)\n","\n","    def next_state(self, state, action):\n","        action_move_map = [\n","            (-1, 0), (1, 0), (0, -1), (0, 1)\n","        ]\n","        move = action_move_map[action]\n","        next_state = (state[0] + move[0], state[1] + move[1])\n","        ny, nx = next_state\n","\n","        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:\n","            next_state = state\n","        elif next_state == self.wall_state:\n","            next_state = state\n","\n","        return next_state\n","\n","    def reward(self, state, action, next_state):\n","        return self.reward_map[next_state]\n","\n","    def step(self, action):\n","        state = self.agent_state\n","        next_state = self.next_state(state, action)\n","        reward = self.reward(state, action, next_state)\n","        done = (next_state == self.goal_state)\n","\n","        self.agent_state = next_state\n","\n","        return next_state, reward, done\n","\n","    def reset(self):\n","        self.agent_state = self.start_state\n","        return self.agent_state"],"metadata":{"id":"ExS9mKDUVYjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# パラメータの設定\n","env = GridWorld()\n","action = 0\n","\n","# step メソッドを使う\n","next_state, reward, done = env.step(action)\n","print(f\"next_state:\\t{next_state}\")\n","print(f\"reward:\\t{reward}\")\n","print(f\"done:\\t{done}\")"],"metadata":{"id":"QTGVf8pmWd9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["この `step` メソッドを使ってエージェントに行動を行わせ、サンプルデータを得る。  \n","また、 `reset` メソッドを使うことで、最初の状態にリセットする。"],"metadata":{"id":"IKW-Nvr1Xkx-"}},{"cell_type":"code","source":["# GridWorld クラスのインスタンスを生成\n","env = GridWorld()\n","\n","# 状態をリセット\n","state = env.reset()"],"metadata":{"id":"CrAM9SGtYDVP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### エージェントクラスの実装(*コーディング*)\n","\n","続いてモンテカルロ法を使って方策評価を行うエージェントを実装する。  \n","このエージェントは、ランダムな方策に従って行動するものとする。  \n","エージェントを `RandomAgent` クラスとして実装する。"],"metadata":{"id":"X4u6KjHlYVAu"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from collections import defaultdict"],"metadata":{"id":"WSYuYoN7grfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RandomAgent クラスの実装\n","class RandomAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.V = defaultdict(lambda: 0)\n","        self.cnts = defaultdict(lambda: 0)\n","        self.memory = []\n","\n","    def get_actions(self, state):\n","        action_probs = self.pi[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def add(self, state, action, reward):\n","        data = (state, action, reward)\n","        self.memory.append(data)\n","\n","    def reset(self):\n","        self.memory.clear()\n","\n","    def eval(self):\n","        G = 0\n","        for data in reversed(self.memory):\n","            state, action, reward = data\n","            G = self.gamma * G + reward\n","            self.cnts[state] += 1\n","            self.V[state] += (G - self.V[state]) / self.cnts[state]"],"metadata":{"id":"NA8Iw04LYrXz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### モンテカルロ法を動かす(*コーディング*)\n","\n","準備が整ったので、エージェントの `RandomAgent` クラスと環境の `GridWorld` クラスを連携させて動かしてみる。  \n","ここでは、1000回のエピソードを実行している。  \n","エピソードの冒頭で環境とエージェントのリセットを行い、 while ループの中でエージェントに行動させる。  \n","その過程で得られた「状態、行動、報酬」のサンプルデータを記録する。  \n","ゴールに辿り着いたら得られたサンプルデータをもとに、モンテカルロ法で価値関数を更新する。  \n","最後に while ループを抜けて次のエピソードを始める。"],"metadata":{"id":"NBur5mawiku5"}},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = GridWorld()\n","agent = RandomAgent()\n","\n","# 1000回のエピソードで実行\n","episodes = 1000\n","for episode in range(episodes):\n","    state = env.reset()\n","    agent.reset()\n","\n","    while True:\n","        action = agent.get_actions(state)\n","        next_state, reward, done = env.step(action)\n","\n","        agent.add(state, action, reward)\n","        if done:\n","            agent.eval()\n","            break\n","\n","        state = next_state\n","\n","print(agent.V)"],"metadata":{"id":"ftyaFnaii-A_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["モンテカルロ法を使った場合も DP のときと似たような結果が得られた。"],"metadata":{"id":"BJMg8jJ5kbjU"}},{"cell_type":"markdown","source":["## モンテカルロ法による方策制御\n","\n","方策評価をしたら、次は最適方策を見つける「方策制御」をモンテカルロ法で行うことを考える。  \n","前章の方策反復法で鍵となるアイデアは学んでいるので、特段新しいことは多くない。"],"metadata":{"id":"eJ117M8f9OJM"}},{"cell_type":"markdown","source":["### 評価と改善\n","\n","最適方策は「評価」と「改善」を交互に繰り返すことによって得られる。  \n","「評価」のフェーズでは、方策を評価して価値関数を得る。  \n","「改善」のフェーズでは、価値関数を greedy 化することで方策を改善する。  \n","この2つのフェーズを交互に繰り返すことで最適方策へと近づいていく。\n","\n","\n","前章でモンテカルロ法を使って方策の評価を行った。  \n","例えば、 $\\pi$ という方策があれば $V_\\pi(s)$ を得ることができた。  \n","改善のフェーズでは次のような greedy 化を行う。\n","\n","\\begin{eqnarray*}\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu(s) &=& \\argmax_a Q(s, a) \\\\\n","&=& \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V(s') \\right\\}\n","\\end{eqnarray*}\n","\n","この式の問題点として、一般的な強化学習の問題では環境のモデルを知ることが出来ないということが挙げられる。  \n","式の内部に $p(s'|s, a)$ と $r(s, a, s')$ があるため計算が出来ない。  \n","そこで、式の1行目に戻って Q 関数 $Q(s, a)$ のみを使って方策の改善を行う必要がある。  \n","これは、単に $Q(s, a)$ が最大となる $a$ を取り出すだけなので環境のモデルを必要としない。  \n","Q 関数を対象に改善をする場合、モンテカルロ法の更新式を $V(s)$ から $Q(s, a)$ に切り替える。\n","\n","---\n","\n","**【状態価値関数に関しての評価】**\n","\n","\\begin{eqnarray*}\n","\\text{一般的な方式} &:& V_n(s) = \\frac{G^{(1)} + G^{(2)} + \\cdots + G^{(n)}}{n} \\\\\n","\\text{インクリメンタルな方式} &:& V_n(s) = V_{n-1}(s) + \\frac{1}{n} \\left\\{ G^{(n)} - V_{n-1}(s) \\right\\}\n","\\end{eqnarray*}\n","\n","---\n","\n","**【Q 関数に関しての評価】**\n","\n","\\begin{eqnarray*}\n","\\text{一般的な方式} &:& Q_n(s, a) = \\frac{G^{(1)} + G^{(2)} + \\cdots + G^{(n)}}{n} \\\\\n","\\text{インクリメンタルな方式} &:& Q_n(s, a) = Q_{n-1}(s, a) + \\frac{1}{n} \\left\\{ G^{(n)} - Q_{n-1}(s, a) \\right\\}\n","\\end{eqnarray*}\n","\n","---\n","\n","上式の通り、状態価値関数であっても Q 関数であってもモンテカルロ法で行う計算自体は変わらない。"],"metadata":{"id":"rnH3VUKV9-7g"}},{"cell_type":"markdown","source":["### モンテカルロ法を使った方策制御の実装(*コーディング*)\n","\n","ここでは、モンテカルロ法を使って方策制御を行うエージェントの `McAgent` クラスを実装する。"],"metadata":{"id":"OnolQxgZGKAR"}},{"cell_type":"code","source":["# 行動の確率分布を返す関数を定義\n","def greedy_probs(\n","        Q: dict, state: tuple[int, int], action_size: int=4\n","    ) -> dict:\n","    qs = [Q[(state, action)] for action in range(action_size)]\n","    max_action = np.argmax(qs)\n","\n","    action_probs = {action: 0.0 for action in range(action_size)}\n","    action_probs[max_action] = 1\n","\n","    return action_probs"],"metadata":{"id":"9EhQj7tmJupB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# McAgent クラスの実装\n","class McAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.Q = defaultdict(lambda: 0)\n","        self.cnts = defaultdict(lambda: 0)\n","        self.memory = []\n","\n","    def get_action(self, state):\n","        action_probs = self.pi[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def add(self, state, action, reward):\n","        data = (state, action, reward)\n","        self.memory.append(data)\n","\n","    def reset(self):\n","        self.memory.clear()\n","\n","    def update(self):\n","        G = 0\n","        for data in reversed(self.memory):\n","            state, action, reward = data\n","            G = self.gamma * G + reward\n","            key = (state, action)\n","            self.cnts[key] += 1\n","            self.Q[key] += (G - self.Q[key]) / self.cnts[key]\n","            self.pi[state] = greedy_probs(self.Q, state)"],"metadata":{"id":"x_ufPL7iGi0B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`greedy_probs()` 関数が返すのは、 greedy な行動を取る確率分布、つまり、状態 `state` において Q 関数の値が最大の行動だけを取る確率分布である。  \n","この関数は、今後他のクラスからも利用するため `McAgent` クラスの外に定義している。\n","\n","実はこの実装ではあまり上手くいかない。  \n","改善点は以下の2点である。\n","\n","- `greedy_probs()` 関数の最後を、完全な greedy ではなく ε-greedy にする\n","- `McAgent` クラスの `update()` メソッドの Q の更新は「固定値 $\\alpha$ 方式」で行う"],"metadata":{"id":"yI6MHLTHLan9"}},{"cell_type":"markdown","source":["### ε-greedy 法(*コーディング*)\n","\n","greedy な行動を行った場合、エージェントの辿るルートはただ1つに固定されてしまう。  \n","これでは全ての状態と行動の組み合わせに対して収益のサンプルデータを集めることが出来ない。  \n","そこで ε-greedy 法を使って、エージェントの行動にランダム性を少しだけ加えることで、基本的には Q 関数が最大の行動を選び、低い確率でランダムな行動を選ぶようにする。"],"metadata":{"id":"R95XV34YiEsk"}},{"cell_type":"code","source":["# ε-greedy 法に直した greedy_probs 関数を定義\n","def greedy_probs(\n","        Q: dict, state: tuple[int, int], eps: float=0, action_size: int=4\n","    ) -> dict:\n","    qs = [Q[(state, action)] for action in range(action_size)]\n","    max_action = np.argmax(qs)\n","\n","    base_prob = eps / action_size\n","    action_probs = {action: base_prob for action in range(action_size)}\n","    action_probs[max_action] += (1 - eps)\n","\n","    return action_probs"],"metadata":{"id":"9IW3p2SUjAGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 固定値 $\\alpha$ 方式へ(*コーディング*)\n","\n","`McAgent` クラスの `update()` メソッドを修正する。"],"metadata":{"id":"7PmR2fk-kPbs"}},{"cell_type":"code","source":["# update() メソッドが修正された McAgent クラスの実装\n","class McAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.eps = 0.1\n","        self.alpha = 0.1\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.Q = defaultdict(lambda: 0)\n","        self.cnts = defaultdict(lambda: 0)\n","        self.memory = []\n","\n","    def get_action(self, state):\n","        action_probs = self.pi[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def add(self, state, action, reward):\n","        data = (state, action, reward)\n","        self.memory.append(data)\n","\n","    def reset(self):\n","        self.memory.clear()\n","\n","    def update(self):\n","        G = 0\n","        for data in reversed(self.memory):\n","            state, action, reward = data\n","            G = self.gamma * G + reward\n","            key = (state, action)\n","            self.Q[key] += (G - self.Q[key]) * self.alpha\n","            self.pi[state] = greedy_probs(self.Q, state, self.eps)"],"metadata":{"id":"kLz3WZr3kh7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["固定値 $\\alpha$ で更新すると、各データに対する重みが指数関数的に増加する(指数移動平均)。  \n","これによって新しいデータほど重みが大きくなる。"],"metadata":{"id":"kcUJyYG5lZqy"}},{"cell_type":"markdown","source":["### 修正後のモンテカルロ法を使った方策制御の実装(*コーディング*)\n","\n","以下の2点の改善を行ったモンテカルロ法の方策制御を実装した。\n","\n","- `greedy_probs()` 関数の最後を、完全な greedy ではなく ε-greedy にする\n","- `McAgent` クラスの `update()` メソッドの Q の更新は「固定値 $\\alpha$ 方式」で行う\n","\n","この `McAgent` クラスを使って実験をしてみる。"],"metadata":{"id":"cvbAPc5Jl3m4"}},{"cell_type":"code","source":["# インスタンスの生成\n","env = GridWorld()\n","agent = McAgent()\n","\n","\n","episodes = 1000\n","for episode in range(episodes):\n","    state = env.reset()\n","    agent.reset()\n","\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done = env.step(action)\n","\n","        agent.add(state, action, reward)\n","        if done:\n","            agent.update()\n","            break\n","\n","        state = next_state\n","\n","print(agent.Q)"],"metadata":{"id":"tCWTJfhSmc9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 方策オフ型と重点サンプリング\n","\n","モンテカルロ法に ε-greedy 法を組み合わせることで最適に近い方策を得ることが出来た。  \n","しかし、それは完全に最適方策ではない。  \n","ここでは、モンテカルロ法を使って完全に最適な方法を学習する方法について考える。  \n","その上で、方策オン型とオフ型について説明する。"],"metadata":{"id":"Nj2omysTsXW0"}},{"cell_type":"markdown","source":["### 方策オン型とオフ型\n","\n","自分とは別の場所で得られた経験から自分の方策を改善するというアプローチを、強化学習では<font color=\"red\">**方策オフ型**</font>(off-policy)という。  \n","一方、自分で得た経験から自分の方策を改善する場合は<font color=\"red\">**方策オン型**</font>(on-policy)という。  \n","エージェントの方策は、役割的に見ると、2つの方策があると見做せる。  \n","1つは評価と改善の対象となる方策であり、<font color=\"red\">**ターゲット方策**</font>(Target Policy)と呼ばれる。  \n","もう1つは、エージェントが実際に行動を起こす際に使う方策であり、<font color=\"red\">**挙動方策**</font>(Behaviour Policy)と呼ばれる。  \n","我々はこれまで、「ターゲット方策」と「挙動方策」を区別せずに使ってきた。  \n","このようにターゲット方策と挙動方策を同じものと見做せる場合、それは方策オン型である。  \n","一方で、これら2つを分けて考えなければいけない場合は、方策オフ型である。  \n","この方策オフ型であれば、挙動方策には「探索」を行わせ、ターゲット方策には「活用」だけを行わせることができる。  \n","ただし、挙動方策から得られたサンプルデータを使ってターゲット方策に関連する期待値を求めるには計算に工夫が必要になる。  \n","ここで登場するのが<font color=\"red\">**重点サンプリング**</font>(Importance Sampling)というテクニックである。"],"metadata":{"id":"-ly8SeNus_Wg"}},{"cell_type":"markdown","source":["### 重点サンプリング(*コーディング*)\n","\n","重点サンプリングは、ある確率分布の期待値を別の確率分布からサンプリングしたデータを使って計算する手法である。  \n","ここでは、重点サンプリングを説明するにあたって、単純な例として $\\mathbb{E}_\\pi[x]$ という期待値の計算について考える。  \n","$x$ は確率変数であり、 $x$ の確率は $\\pi(s)$ で表すことにする。\n","\n","$$\\mathbb{E}_\\pi[x] = \\sum x\\pi(x)$$\n","\n","この期待値をモンテカルロ法で近似するには、 $x$ を確率分布 $\\pi$ からサンプリングしてその平均を取る。\n","\n","$$\n","\\text{sampling: } x^{(i)} \\sim \\pi \\quad (i = 1, 2, \\cdots, n) \\\\\n","\\mathbb{E}_\\pi[x] \\simeq \\frac{x^{(1)} + x^{(2)} + \\cdots + x^{(n)}}{n}\n","$$\n","\n","ここで、 $x$ が別の確率分布 $b$ からサンプリングされた場合を考える。  \n","期待値 $\\mathbb{E}_\\pi[x]$ を次のように式変形する。\n","\n","\\begin{eqnarray*}\n","\\mathbb{E}_\\pi[x] &=& \\sum x\\pi(x) \\\\\n","&=& \\sum x \\frac{b(x)}{b(x)} \\pi(x) \\\\\n","&=& \\sum x \\frac{\\pi(x)}{b(x)} b(x) \\\\\n","&=& \\mathbb{E}_b \\left \\lbrack x \\frac{\\pi(x)}{b(x)} \\right \\rbrack\n","\\end{eqnarray*}\n","\n","このようにすることで、確率分布 $b$ に関する期待値として表せるようになる。  \n","また、各 $x$ には $\\frac{\\pi(x)}{b(x)}$ が乗算されており、 $\\rho(x) = \\frac{\\pi(x)}{b(x)}$ とすれば各 $x$ には「重み」として $\\rho(x)$ がかけられていると見做せる。  \n","以上より、\n","\n","$$\n","\\text{sampling: } x^{(i)} \\sim b \\quad (i = 1, 2, \\cdots, n) \\\\\n","\\mathbb{E}_\\pi[x] \\simeq \\frac{\\rho(x^{(1)})x^{(1)} + \\rho(x^{(2)})x^{(2)} + \\cdots + \\rho(x^{(n)})x^{(n)}}{n}\n","$$\n","\n","これで $\\pi$ とは異なる確率分布 $b$ からサンプリングしたデータを使って $\\mathbb{E}_\\pi[x]$ を計算することができる。  \n","重点サンプリングを実装してみる。  \n","初めに確率分布 $\\pi$ の期待値を通常のモンテカルロ法で求める。"],"metadata":{"id":"WJzWgM5CwABx"}},{"cell_type":"code","source":["# x と確率分布 pi の設定\n","x = np.array([1, 2, 3])\n","pi = np.array([0.1, 0.1, 0.8])\n","\n","# 期待値\n","e = np.sum(x * pi)\n","print(f\"E_pi[x]: {e}\")\n","\n","# モンテカルロ法\n","n = 100\n","samples = []\n","for _ in range(n):\n","    s = np.random.choice(x, p=pi)\n","    samples.append(s)\n","\n","# 平均と分散の算出\n","mean = np.mean(samples)\n","var = np.var(samples)\n","print(f\"MC: {round(mean, 3)}\\t(var: {round(var, 3)})\")"],"metadata":{"id":"4GpntAouJSmS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["次に、重点サンプリングを使って期待値を求める。"],"metadata":{"id":"z37GMLx7MEn_"}},{"cell_type":"code","source":["# 確率分布 b の設定\n","b = np.array([1/3, 1/3, 1/3])\n","\n","# 重点サンプリング\n","n = 100\n","samples = []\n","for _ in range(n):\n","    idx = np.arange(len(b))\n","    i = np.random.choice(idx, p=b)\n","    s = x[i]\n","    rho = pi[i] / b[i]\n","    samples.append(rho * s)\n","\n","# 平均と分散の算出\n","mean = np.mean(samples)\n","var = np.var(samples)\n","print(f\"MC: {round(mean, 3)}\\t(var: {round(var, 3)})\")"],"metadata":{"id":"ajWd6DiEMER-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["平均は真値の2.7から少しだけ離れるものの、その近辺の値を取ることがわかる。  \n","一方で、分散についてはモンテカルロ法のときよりも大きいことがわかる。"],"metadata":{"id":"rkjMgRyiOc-H"}},{"cell_type":"markdown","source":["### 分散を小さくする(*コーディング*)\n","\n","分散が小さければ小さいほど少ないサンプル数で精度良く近似できる。  \n","逆に、分散が大きくなればなるほど、精度良く近似するにはサンプル数を増やす必要がある。  \n","分散を小さくする方法の1つが、2つの確率分布を近づけることである。"],"metadata":{"id":"V01N-q8pOzX8"}},{"cell_type":"code","source":["# 変更後の確率分布 b の設定\n","b = np.array([0.2, 0.2, 0.6])\n","\n","# 重点サンプリング\n","n = 100\n","samples = []\n","for _ in range(n):\n","    idx = np.arange(len(b))\n","    i = np.random.choice(idx, p=b)\n","    s = x[i]\n","    rho = pi[i] / b[i]\n","    samples.append(rho * s)\n","\n","# 平均と分散の算出\n","mean = np.mean(samples)\n","var = np.var(samples)\n","print(f\"MC: {round(mean, 3)}\\t(var: {round(var, 3)})\")"],"metadata":{"id":"HaMo8im-QGeR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["以上で分散が小さくなったことが確認できた。  \n","強化学習における主眼は、一方の方策に「探索」を、もう一方には「活用」を行わせることである。  \n","その条件を満たした上で、できるだけ2つの確率分布を近づけることで分散を小さくする。"],"metadata":{"id":"QaJfZdM7QcJr"}}]}