{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"private_outputs":true,"collapsed_sections":["UqSNbVDr1rOT","kAkkGEDL2B1W","3jqGspedTKXt","oPiJSmKvnY78","oScZDF-d8YGi","OSRgRfUuITwC"],"authorship_tag":"ABX9TyOQVltLH4qq41VbNTnNaaYO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第1章 〜バンディット問題〜\n","\n","ここでは、強化学習において最も基本となる「<font color=\"red\">**バンディット問題**</font>」について扱う。"],"metadata":{"id":"UqSNbVDr1rOT"}},{"cell_type":"markdown","source":["## 機械学習の区分\n","\n","機械学習で使われる手法は、解きたい問題の構造によって分けられる。  \n","代表的な区分は以下の3つである。\n","\n","- <font color=\"red\">**教師あり学習**</font>\n","- <font color=\"red\">**教師なし学習**</font>\n","- <font color=\"red\">**強化学習**</font>"],"metadata":{"id":"kAkkGEDL2B1W"}},{"cell_type":"markdown","source":["### 教師あり学習\n","\n","教師あり学習(Supervised Learning)では、入力と出力のペアデータが与えられる。  \n","入力するデータから、何らかのアルゴリズムのもとで出力したいデータを得られるように学習し、予測する。  \n","例えば、「手書き数字の画像」と「正解ラベル」が与えられたとすると、画像の特徴からペアになる正解ラベルと結びつくように学習させることで、未知の画像を使っても同様にラベルを予測することができる。\n","\n","教師あり学習の中にも、大きく分けて2種類のタスクがある。\n","\n","- <font color=\"red\">**分類**</font>\n","- <font color=\"red\">**回帰**</font>\n","\n","先述の画像からラベルを予測する問題は、「分類問題」に相当する。  \n","分類問題では、出力の値が離散値、またはテキストなどになる。  \n","株価・為替レートや1日の平均気温など、連続した値を取るデータを予測する問題は、「回帰問題」に相当する。  \n","回帰問題では、出力の値が連続値になる。\n","\n","教師あり学習の中で、入力として与えるデータのことを「<font color=\"red\">**説明変数**</font>」といい、出力として与えるデータのことを「<font color=\"red\">**目的変数**</font>」という。  \n","教師あり学習(主に分類問題)の場合、その多くは目的変数を人の手で用意しなければいけないため、データの規模が大きくなるにつれてコストがかかる。"],"metadata":{"id":"-BvBctgM3TGC"}},{"cell_type":"markdown","source":["### 教師なし学習\n","\n","教師なし学習(Unsupervised Learning)では、「教師」が存在しない。  \n","すなわち、予測してほしい正解データが手元に無く、ただのデータだけがある状態で何かしらの構造やパターンを見つけるものである。\n","\n","例えば<font color=\"red\">**クラスタリング**</font>では、ルールが定められていないテーブルデータから、指定した数のグループにそれぞれ近いと判断されたデータ同士が固まるように分けるといったことをする。  \n","また、<font color=\"red\">**主成分分析**</font>では、用意されたデータの変数が冗長であるとき(あまり関係性がなさそうな変数や、その変数を包含するような変数が他にあった場合)、必要な数だけ重要な変数を順に選択し、次元の削減を行う。\n","\n","教師なし学習は正解データが不要なため、ビッグデータが与えられても比較的容易に準備することができる。"],"metadata":{"id":"2AgSVxLuG_C0"}},{"cell_type":"markdown","source":["### 強化学習\n","\n","強化学習(Reinforcement Learning)では、「<font color=\"red\">**エージェント**</font>」と「<font color=\"red\">**環境**</font>」が相互にやり取りをすることによって学習を進める。\n","\n","![強化学習](https://www.skillupai.com/wp-content/uploads/2022/01/reinforcement-learning_01.png.webp)\n","\n","(出典: https://www.skillupai.com/blog/tech/reinforcement-learning/)\n","\n","エージェントとは、ロボットのような行動を起こすもののことである。  \n","このエージェントは何らかの環境に置かれており、その環境の「状態」を観測し、それに基づいて「行動」を起こす。  \n","その結果として環境の状態が変化し、エージェントは環境から「報酬」を受け取ると同時に新しい「状態」を観測する。  \n","強化学習の目的は、エージェントが得る報酬の総和を最大にする行動パターンを身につけることである。\n","\n","![DQN](https://pytorch.org/tutorials/_images/cartpole.gif)\n","\n","(出典: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n","\n","強化学習では、環境からのフィードバックとして報酬を受け取るが、これは教師あり学習で言う正解データとは性質が異なる。  \n","報酬は行動に対してのフィードバックであり、その報酬からは実際に行った行動が最適かどうかは判断できない。  \n","一方、教師あり学習では正解データがあり、これを強化学習の文脈で言えば、どんな行動を取っても最適な行動はこれだと提示してくれる教師がいる。  \n","つまり強化学習では、自分で試行錯誤してデータを集めて、その中から良い行動の作法を学習しなければいけないということである。\n","\n","この章では、強化学習の中でも最もシンプルな「<font color=\"red\">**バンディット問題**</font>」を取り扱う。"],"metadata":{"id":"jJ7zjtmOLQuW"}},{"cell_type":"markdown","source":["## バンディット問題とは\n","\n","バンディット問題(Bandit Problem)を扱う上で、1本のレバーが付いたスロットマシンが複数台並べてあることをイメージする。  \n","各スロットマシンは特性が異なり、当たり外れが違うとする。  \n","初めプレイヤーは、どのマシンが当たりやすいかという情報を持っていないとし、実際にプレイして試行錯誤しながらどのマシンが良いかを見極めていく必要がある。  \n","更に、予め決められた回数の中で得られるコインの枚数をできるだけ多くすることを目標とする。\n","\n","今回、スロットマシンは「環境(Environment)」であり、プレイヤーが「エージェント(Agent)」である。  \n","この2つの登場人物が相互に「やり取り」するというのが強化学習の枠組みである。  \n","プレイヤーが複数あるスロットマシンから1台選び、プレイすることが「行動(Action)」である。  \n","そして、その結果としてプレイヤーが受け取るコインが「報酬(Reward)」である。  \n","「状態(State)」は一般的に環境の内部に備わっているものとする。  \n","ここでは状態の変化を考える必要がない。"],"metadata":{"id":"3jqGspedTKXt"}},{"cell_type":"markdown","source":["### 良いスロットマシンを考える\n","\n","スロットマシンには「ランダム性」がある。  \n","ランダム性とは、スロットマシンをプレイして得られるコインの枚数(報酬)が毎回変わるということを言う。  \n","例えば以下の2種類のスロットマシン $a, b$ を考える。\n","\n","| もらえる<br>コインの枚数 | 0 | 1 | 5 | 10 |\n","| :--: | :--: | :--: | :--: | :--: |\n","| $a$ | 0.70 | 0.15 | 0.12 | 0.03 |\n","| $b$ | 0.50 | 0.40 | 0.09 | 0.01 |\n","\n","この表は<font color=\"red\">**確率分布表**</font>と呼ばれる。  \n","通常プレイヤーはこのような確率分布表が分かっていないが、ここでは敢えて分かっているとする。  \n","このとき、プレイヤーはどちらのスロットマシンを選ぶのが良いだろうか。\n","\n","確率分布表を使って、<font color=\"red\">**期待値**</font>(Expectation Value)を算出する。\n","\n","$$\n","a: 0 \\times 0.70 + 1 \\times 0.15 + 5 \\times 0.12 + 10 \\times 0.03 = 1.05 \\\\\n","b: 0 \\times 0.50 + 1 \\times 0.40 + 5 \\times 0.09 + 10 \\times 0.01 = 0.95\n","$$\n","\n","以上より、 $a$ を選んだ方が1回辺りの期待値は高いので良いと言える。"],"metadata":{"id":"u5yloMQweeEk"}},{"cell_type":"markdown","source":["### 数式を使った表記\n","\n","まず、報酬を $R$ と表すとする。  \n","更に、エージェントは連続して行動を起こすので、 $t$ 回目に得られる報酬を明示的に表す場合は $R_t$ と書くことにする。  \n","ここで、先ほどの例を使うのであれば、 $R \\in \\{0, 1, 5, 10\\}$ である。  \n","このように、取る値が確率的に決まる変数のことを<font color=\"red\">**確率変数**</font>(Random Variable)と言う。\n","\n","続いて、エージェントの行う行動を $A$ とする。  \n","エージェントの行動は $a$ または $b$ のスロットマシンでプレイすることなので、 $A \\in \\{a, b\\}$ と表せる。\n","\n","次に、期待値は $\\mathbb{E}$ で表す。  \n","例えば、報酬 $R$ の期待値は $\\mathbb{E}[R]$ のように書く。  \n","条件付き期待値は $\\mathbb{E}[R|A]$ と書く。  \n","更に、エージェントが例えば $a$ を選んだとすると、 $\\mathbb{E}[R|A = a]$ または単に $\\mathbb{E}[R|a]$ と記述する。\n","\n","報酬の期待値は<font color=\"red\">**行動価値**</font>(Action Value)と呼ばれ、強化学習の分野では $Q$ や $q$ という記号をあてる。  \n","ここでは\n","\n","$$\n","q(A) = \\mathbb{E}[R|A]\n","$$\n","\n","とする。  \n","一般的には、小文字を使うときは「真の行動価値」を表し、大文字を使うときは「推定した行動価値」を表す。"],"metadata":{"id":"Z3DVH1fRi1DI"}},{"cell_type":"markdown","source":["## バンディットアルゴリズム\n","\n","ここまでの話を整理すると次のようになる。\n","\n","- もし各スロットマシンの価値(報酬の期待値)がわかれば、プレイヤーは最も良いスロットマシンを選ぶことができる\n","- しかし各スロットマシンの価値はプレイヤーにはわからない\n","- よってプレイヤーには、各スロットマシンの価値を(できるだけ精度良く)推定することが求められる\n","\n","上記の点を満たし、得られるコインの総和を高める方策が求められる。"],"metadata":{"id":"oPiJSmKvnY78"}},{"cell_type":"markdown","source":["### 価値の推定\n","\n","| スロットマシン | 1回目 | 2回目 | 3回目 |\n","| :--: | :--: | :--: | :--: |\n","| $a$ | 0 | 1 | 5 |\n","| $b$ | 1 | 0 | 0 |\n","\n","この表は、各マシンで3回ずつプレイして得た報酬をまとめたものであるとする。  \n","1回目の結果だけを見て、 $b$ のマシンの方が良いと判断するのはあまり良くない。  \n","実際3回目までやると $a$ のマシンの方が良い結果が得られている。  \n","ここで、それぞれのマシンの価値の推定値 $Q(a), Q(b)$ は次のように計算できる。\n","\n","\\begin{eqnarray*}\n","Q(a) &=& \\frac{0 + 1 + 5}{3} = 2 \\\\\n","Q(b) &=& \\frac{1 + 0 + 0}{3} = 0.33 \\cdots\n","\\end{eqnarray*}\n","\n","これを一般化すると、次のようになる。  \n","試行回数を $T$ 回とすると、\n","\n","$$\n","Q_T(A) = \\mathbb{E}[R|A] = \\frac{1}{T} \\sum_{t=1}^T R_t\n","$$\n","\n","であり、この $T$ を $\\infty$ にとばすと、\n","\n","$$\n","\\lim_{T\\to \\infty} Q_T(A) = q(A)\n","$$\n","\n","となる。  \n","すなわち、試行回数を重ねるにつれて真の確率分布に近づく。"],"metadata":{"id":"jNO3829SYphW"}},{"cell_type":"markdown","source":["### 期待値を求める(*コーディング*)\n","\n","ここでは、 $T = 10$ として報酬を得るたびに推定値を求めるコードを作成する。"],"metadata":{"id":"Ttc031E7i-Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WuglWTP1GsE"},"outputs":[],"source":["# ライブラリのインポート\n","import numpy as np"]},{"cell_type":"code","source":["# 乱数シードの固定\n","np.random.seed(8192)\n","\n","# 報酬を記録するための空リストを用意\n","rewards = []\n","\n","# i 回目の報酬の推定結果を出力\n","for i in range(1, 11):\n","    reward = np.random.rand()\n","    rewards.append(reward)\n","    q = sum(rewards) / i\n","    print(q)"],"metadata":{"id":"lJ8SdudZj4n_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記の実装では正しく報酬の期待値を算出することができるが、改善点もある。  \n","試行回数 $T$ が増えるにつれて `rewards` の要素が増えてしまう。  \n","また、 $T$ 個分の和を求めるための計算量(`sum(rewards)`)も同様に増える。\n","\n","より良い実装をするために、数式を使って説明をする。  \n","まずは $T - 1$ 回目の時点での行動価値の推定値である $Q_{T-1}$ に注目する。\n","\n","$$\n","Q_{T-1} = \\frac{1}{T-1} \\sum_{t=1}^{T-1} R_t\n","$$\n","\n","ここで、両辺に $T-1$ をかけると\n","\n","$$\n","\\sum_{t=1}^{T-1} R_t = (T-1)Q_{T-1}\n","$$\n","\n","が得られる。  \n","$Q_T$ は、\n","\n","\\begin{eqnarray*}\n","Q_T &=& \\frac{1}{T} \\sum_{t=1}^{T} R_t \\\\\n","&=& \\frac{1}{T} \\left( \\left( \\sum_{t=1}^{T-1} R_t \\right) + R_T \\right) \\\\\n","&=& \\frac{1}{T} \\left( (T-1)Q_{T-1} + R_T \\right) \\\\\n","&=& \\left( 1 - \\frac{1}{T} \\right) Q_{T-1} + \\frac{1}{T}R_T\n","\\end{eqnarray*}\n","\n","と表せる。  \n","ポイントは、 $Q_T$ と $Q_{T-1}$ の関係が導かれたことにより、 $Q_T$ を求めるのに $Q_{T-1}$ と $R_T$ 、そして $T$ の3つの値があれば良いことがわかる。  \n","よって今までで得られた全ての報酬を毎回使わなくても使わなくても良いという事になる。\n","\n","\\begin{eqnarray*}\n","Q_T &=& \\left( 1 - \\frac{1}{T} \\right) Q_{T-1} + \\frac{1}{T}R_T \\\\\n","&=& Q_{T-1} + \\frac{1}{T}(R_T - Q_{T-1})\n","\\end{eqnarray*}\n","\n","$Q_{T-1}$ から $Q_T$ に更新するには、 $\\frac{1}{T}(R_T - Q_{T-1})$ を足してあげれば良いということが分かった。  \n","ここで、 $\\frac{1}{T}$ は更新する量を調整するので「学習率」としての役割を担う。\n","\n","以上を踏まえて再度実装し直してみる。"],"metadata":{"id":"Bjgjkok7na0k"}},{"cell_type":"code","source":["# 推定値の初期化\n","q = 0\n","\n","# 改善したアルゴリズムで　i 回目の報酬の推定結果を出力\n","for i in range(1, 11):\n","    reward = np.random.rand()\n","    q = q + (reward - q) / i # q += (reward - q) / i\n","    print(q)"],"metadata":{"id":"qrFnwBuTuFoW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### プレイヤーの戦略\n","\n","プレイヤーに求められるのは次の2点になる。\n","\n","- これまで実際にプレイした結果を利用して、最善と思われるスロットマシンをプレイすること(<font color=\"red\">**greedy な行動**</font>)\n","- スロットマシンの価値を精度良く推定するために、様々なスロットマシンを試すこと\n","\n","greedy な行動をすることで、これまでの経験に基づいて最善の行動を選択できるが、先の例では1回目の結果からは $b$ のスロットマシンが選ばれ続けることになる。  \n","この選択を「<font color=\"red\">**活用**</font>(Exploitation)」とも言う。  \n","一方で、より良い選択を見逃さないように「greedy でない行動」を試す必要があり、これを「<font color=\"red\">**探索**</font>(Exploration)」と言う。  \n","活用と探索はトレードオフの関係にあり、強化学習のアルゴリズムではこのバランスをいかに取るかが問題になる。  \n","このバランスを取る方法として最も基本的で応用の利くアルゴリズムに「<font color=\"red\">**ε-greedy 法**</font>(イプシロン-グリーディー法)」がある。"],"metadata":{"id":"rg3j0bE_2QF0"}},{"cell_type":"markdown","source":["## ε-greedy 法の実装\n","\n","話を単純にするために、スロットマシンが返すコインの枚数を0枚か1枚のどちらかに限られる場合を考えることにする。  \n","各スロットマシンには勝率が設定されているとし、例えば勝率が0.6であるマシンは60%の確率で1を返し、40%の確率で0を返すとする。  \n","勝率はそのままスロットマシンの価値になる。  \n","10台のスロットマシンを考え、プレイヤーはそれぞれのスロットマシンに設定された勝率を知らないものとする。  \n","そのため、実際にプレイした経験をもとに勝率の高いスロットマシンを探す必要がある。"],"metadata":{"id":"oScZDF-d8YGi"}},{"cell_type":"markdown","source":["### スロットマシンの実装(*コーディング*)\n","\n","スロットマシンを `Bandit` クラスとして実装する。  \n","このクラスの中に10台のマシンが存在するように実装する。"],"metadata":{"id":"hGrQkecLCPw9"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import numpy as np"],"metadata":{"id":"1ZnY2qhNCjhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bandit クラスの実装\n","class Bandit:\n","    def __init__(self, arms=10):\n","        self.rates = np.random.rand(arms)\n","\n","    def play(self, arm):\n","        rate = self.rates[arm]\n","        if rate > np.random.rand():\n","            return 1\n","        else:\n","            return 0"],"metadata":{"id":"JdfecaYqCn6e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`play()` メソッドでは、何番目のスロットマシンを使うかを表す `arm` を引数に指定する。  \n","`__init__()` メソッドで指定したそれぞれの勝率を閾値として、生成した乱数がそれを下回れば1を、上回ればを返す。"],"metadata":{"id":"QJvH--X2Hzev"}},{"cell_type":"code","source":["# インスタンス生成\n","bandit = Bandit()\n","\n","# 0番目のスロットマシンを3回プレイ\n","for i in range(3):\n","    print(bandit.play(0))"],"metadata":{"id":"-NvrtuHXIngE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### エージェントの実装(*コーディング*)\n","\n","まずは今作った `Bandit` クラスを使って0番目のスロットマシンを10回プレイしたときのその価値の推定をしてみる。"],"metadata":{"id":"W5EmKpNFJGJR"}},{"cell_type":"code","source":["# インスタンス生成\n","bandit = Bandit()\n","\n","# 推定値の初期化\n","q = 0\n","\n","# インクリメンタルな実装で推定結果を出力\n","for i in range(1, 11):\n","    reward = bandit.play(0)\n","    q += (reward - q) / i\n","    print(q)"],"metadata":{"id":"KWj4dNuzJqNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これを拡張し、10台のスロットマシンそれぞれの価値の推定値を求める。"],"metadata":{"id":"hO0q17fGccPL"}},{"cell_type":"code","source":["# インスタンス生成\n","bandit = Bandit()\n","\n","# 対応するスロットマシンの価値の推定値を格納する\n","qs = np.zeros(10)\n","\n","# 対応するスロットマシンをプレイした回数を格納する\n","ts = np.zeros(10)\n","\n","# インクリメンタルな実装で1000回プレイした推定結果を出力\n","for i in range(1000):\n","    action = np.random.randint(0, 10)\n","    reward = bandit.play(action)\n","\n","    ts[action] += 1\n","    qs[action] += (reward - qs[action]) / ts[action]\n","\n","print(qs)"],"metadata":{"id":"5PsvY6lbcqIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これで各スロットマシンの価値が推定できた。  \n","これを基に `Agent` クラスを実装する。  \n","この `Agent` クラスは、ε-greedy法によって行動を選択するアルゴリズムを実装している。"],"metadata":{"id":"TJ6Ny17cshwk"}},{"cell_type":"code","source":["# Agent クラスの実装\n","class Agent:\n","    def __init__(self, eps, action_size=10):\n","        self.eps = eps\n","        self.qs = np.zeros(action_size)\n","        self.ts = np.zeros(action_size)\n","\n","    def update(self, action, reward):\n","        self.ts[action] += 1\n","        self.qs[action] += (reward - self.qs[action]) / self.ts[action]\n","\n","    def get_action(self):\n","        if np.random.rand() < self.eps:\n","            return np.random.randint(0, len(self.qs))\n","        return np.argmax(self.qs)"],"metadata":{"id":"7K0rbfLttGCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["引数の `eps` にはエージェントがランダムな行動を行う確率を指定する。  \n","`update()` メソッドでは、スロットマシンの価値を推定し、 `get_action()` メソッドでは、ε-greedy法によって行動を選択する。"],"metadata":{"id":"IAauxWVtK1vR"}},{"cell_type":"markdown","source":["### 動かしてみる(*コーディング*)\n","\n","これまでに実装してきた `Bandit` クラスと `Agent` クラスを使って動かしてみる。"],"metadata":{"id":"4FUVSInWLn7y"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import matplotlib.pyplot as plt"],"metadata":{"id":"QE_h4dDtMbzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# グラフを描画する関数を定義\n","def plotter(\n","        data, xlabel: str, ylabel: str,\n","        figsize_x: int=18, figsize_y: int=12\n","    ):\n","    plt.figure(figsize=(figsize_x, figsize_y))\n","    plt.grid()\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.plot(data)\n","\n","    plt.show()"],"metadata":{"id":"E96coeyIUq45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","steps = 1000\n","eps = 0.1\n","\n","# インスタンスの生成と各種データの初期化\n","bandit = Bandit()\n","agent = Agent(eps)\n","total_reward = 0\n","total_rewards = []\n","rates = []"],"metadata":{"id":"fY042Ok9MrYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# インクリメンタルな実装で1000回プレイした推定結果を出力\n","for step in range(steps):\n","    action = agent.get_action()\n","    reward = bandit.play(action)\n","    agent.update(action, reward)\n","    total_reward += reward\n","\n","    total_rewards.append(total_reward)\n","    rates.append(total_reward / (step + 1))\n","\n","print(total_reward)"],"metadata":{"id":"fupAcjyCS5Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 報酬の総量をグラフに描画\n","plotter(total_rewards, xlabel=\"Steps\", ylabel=\"Total reward\")"],"metadata":{"id":"RUKWW6sOTwzj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 勝率をグラフに描画\n","plotter(rates, xlabel=\"Steps\", ylabel=\"Rates\")"],"metadata":{"id":"QFUP8e_5XMaQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 平均的な性質(*コーディング*)\n","\n","先ほどの実装では、乱数のシードを固定しなければ実行ごとに毎回結果が変わってしまう。  \n","試しに10回実行した結果をグラフに描画してみる。"],"metadata":{"id":"YCKqtkUbkyJ3"}},{"cell_type":"code","source":["# 10回分に拡張した描画関数\n","def plotter_ten(\n","        data, xlabel: str, ylabel: str,\n","        figsize_x: int=18, figsize_y: int=12\n","    ):\n","    plt.figure(figsize=(figsize_x, figsize_y))\n","    plt.grid()\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","\n","    for i in range(10):\n","        plt.plot(data[i])\n","\n","    plt.show()"],"metadata":{"id":"WajD46pXlSg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","steps = 1000\n","eps = 0.1\n","\n","# 10回分のデータを格納する\n","rate_data = []\n","\n","# 10回繰り返す\n","for i in range(10):\n","    bandit = Bandit()\n","    agent = Agent(eps)\n","    total_reward = 0\n","    total_rewards = []\n","    rates = []\n","\n","    for step in range(steps):\n","        action = agent.get_action()\n","        reward = bandit.play(action)\n","        agent.update(action, reward)\n","        total_reward += reward\n","\n","        total_rewards.append(total_reward)\n","        rates.append(total_reward / (step + 1))\n","\n","    rate_data.append(rates)\n","\n","# グラフの描画\n","plotter_ten(rate_data, xlabel=\"Steps\", ylabel=\"Rates\")"],"metadata":{"id":"kEcTsv2emIo9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["強化学習のアルゴリズムを比較するとき、ランダム性があるため1回の実験結果を報告することにあまり意味はない。  \n","同じ実験を繰り返し行い、その平均を見ることに意味がある。  \n","次は、200回繰り返した実験の平均を考えてみる。"],"metadata":{"id":"fmNpCs1WnPZM"}},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","runs = 200\n","steps = 1000\n","eps = 0.1\n","all_rates = np.zeros((runs, steps))\n","\n","# 200回の実験\n","for run in range(runs):\n","    bandit = Bandit()\n","    agent = Agent(eps)\n","    total_reward = 0\n","    rates = []\n","\n","    for step in range(steps):\n","        action = agent.get_action()\n","        reward = bandit.play(action)\n","        agent.update(action, reward)\n","        total_reward += reward\n","        rates.append(total_reward / (step + 1))\n","\n","    all_rates[run] = rates\n","\n","# 平均の算出\n","avg_rates = np.average(all_rates, axis=0)\n","\n","# 勝率をグラフに描画\n","plotter(avg_rates, xlabel=\"Steps\", ylabel=\"Rates\")"],"metadata":{"id":"FlFXJ__gnt20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 非定常問題\n","\n","ここまでは、定常問題として考えられる。  \n","定常問題とは、報酬の確率分布が定常である問題のことで、スロットマシンに設定した勝率が固定されていた。\n","\n","```python\n","class Bandit:\n","    def __init__(self, arms=10):\n","        self.rates = np.random.rand(arms)\n","\n","    def play(self, arm):\n","        rate = self.rates[arm]\n","        if rate > np.random.rand():\n","            return 1\n","        else:\n","            return 0\n","```\n","\n","このコードでは、 `self.rates` は初期化された後は変化しない。  \n","これをプレイするごとに少しずつ変化させる問題を考える。"],"metadata":{"id":"OSRgRfUuITwC"}},{"cell_type":"markdown","source":["### 非定常問題を解くにあたって\n","\n","標本平均は次の式で表せた。\n","\n","\\begin{eqnarray*}\n","Q_T &=& \\frac{R_1 + R_2 \\cdots + R_T}{T} \\\\\n","&=& \\frac{1}{T}R_1 + \\frac{1}{T}R_2 + \\cdots + \\frac{1}{T}R_T\n","\\end{eqnarray*}\n","\n","ここで、全ての報酬 $R_t$ に対して $\\frac{1}{T}$ が掛かっている。  \n","この $\\frac{1}{T}$ を各報酬に対する「重み」と見做す。  \n","全ての報酬は同じ重みなので、新しく得た報酬も遥か前に得た報酬も均等に扱ってしまっている。  \n","非定常問題では環境が時間と共に変化するため、過去のデータの重要性は時間と共に低くなるべきである。\n","\n","上記の $Q_T$ は次のようにも表せた。\n","\n","$$Q_T = Q_{T-1} + \\frac{1}{T}(R_T - Q_{T-1})$$\n","\n","ここの $\\frac{1}{T}$ を $\\alpha \\space (0 < \\alpha < 1)$ という固定値に変更すると、\n","\n","\\begin{eqnarray*}\n","Q_T &=& Q_{T-1} + \\alpha (R_T - Q_{T-1}) \\\\\n","&=& \\alpha R_T + Q_{T-1} - \\alpha Q_{T-1} \\\\\n","&=& \\alpha R_T + (1 - \\alpha)Q_{T-1}\n","\\end{eqnarray*}\n","\n","ここで、 $T-1$ の場合を考えると、\n","\n","$$Q_{T-1} = \\alpha R_{T-1} + (1 - \\alpha)Q_{T-2}$$\n","\n","であるので上記式に代入し、再帰的に展開すると\n","\n","\\begin{eqnarray*}\n","Q_T &=& \\alpha R_T + (1 - \\alpha)Q_{T-1} \\\\\n","&=& \\alpha R_T + (1 - \\alpha)(\\alpha R_{T-1} + (1 - \\alpha)Q_{T-2}) \\\\\n","&=& \\alpha R_T + \\alpha (1 - \\alpha)R_{T-1} + (1 - \\alpha)^2 Q_{T-2} \\\\\n","&=& \\alpha R_T + \\alpha (1 - \\alpha)R_{T-1} + \\alpha (1 - \\alpha)^2 R_{T-2} + (1 - \\alpha)^3 Q_{T-3} \\\\\n","&=& \\alpha R_T + \\alpha (1 - \\alpha)R_{T-1} + \\cdots + \\alpha (1 - \\alpha)^{T-1} R_{1} + (1 - \\alpha)^T Q_{0}\n","\\end{eqnarray*}\n","\n","となる。  \n","各報酬に対する重みが指数関数的に減少していることが確認できる。  \n","これらは<font color=\"red\">**指数移動平均**</font>(Exponential Moving Average)や<font color=\"red\">**指数加重移動平均**</font>(Exponential Weighted Moving Average)と言う。\n","\n","ここで注意すべき点は、 $Q_T$ を求める上で、 $Q_0$ が使われていることである。  \n","$Q_0$ は行動価値の初期値であり、自分で設定する値である。  \n","故にこの設定した値によっては学習結果にバイアスが生じてしまうことがある。"],"metadata":{"id":"TaENVWK_syMI"}},{"cell_type":"markdown","source":["### 非定常問題を解く(*コーディング*)\n","\n","まずは `Bandit` クラスの `self.rates` を変化させられる `NonStatBandit` クラスを実装する。"],"metadata":{"id":"2ngkyl5JwMZ7"}},{"cell_type":"code","source":["# NonStatBandit クラスの実装\n","class NonStatBandit:\n","    def __init__(self, arms=10):\n","        self.arms = arms\n","        self.rates = np.random.rand(arms)\n","\n","    def play(self, arm):\n","        rate = self.rates[arm]\n","        self.rates += 0.1 * np.random.randn(self.arms)\n","        if rate > np.random.rand():\n","            return 1\n","        else:\n","            return 0"],"metadata":{"id":"TtYVh4Otw_xX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["続いて、 `Agent` クラスに変更を加え、推定値を固定値 $\\alpha$ で更新する `AlphaAgent` クラスを実装する。"],"metadata":{"id":"6xCULCNlxxW-"}},{"cell_type":"code","source":["# AlphaAgent クラスの実装\n","class AlphaAgent:\n","    def __init__(self, eps, alpha, actions=10):\n","        self.eps = eps\n","        self.qs = np.zeros(actions)\n","        self.alpha = alpha\n","\n","    def update(self, action, reward):\n","        self.qs[action] += (reward - self.qs[action]) * self.alpha\n","\n","    def get_action(self):\n","        if np.random.rand() < self.eps:\n","            return np.random.randint(0, len(self.qs))\n","        return np.argmax(self.qs)"],"metadata":{"id":"9aQw0Qh4yEEK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これらを使って定常問題と同様に実験をしてみる。"],"metadata":{"id":"VhuYmfSEz8DT"}},{"cell_type":"code","source":["# ハイパーパラメータの設定\n","runs = 200\n","steps = 1000\n","eps = 0.1\n","alpha = 0.8\n","all_rates = np.zeros((runs, steps))\n","\n","# 200回の実験\n","for run in range(runs):\n","    bandit = NonStatBandit()\n","    agent = AlphaAgent(eps, alpha)\n","    total_reward = 0\n","    rates = []\n","\n","    for step in range(steps):\n","        action = agent.get_action()\n","        reward = bandit.play(action)\n","        agent.update(action, reward)\n","        total_reward += reward\n","        rates.append(total_reward / (step + 1))\n","\n","    all_rates[run] = rates\n","\n","# 平均の算出\n","avg_rates = np.average(all_rates, axis=0)\n","\n","# 勝率をグラフに描画\n","plotter(avg_rates, xlabel=\"Steps\", ylabel=\"Rates\")"],"metadata":{"id":"fmOHAp27z60Y"},"execution_count":null,"outputs":[]}]}