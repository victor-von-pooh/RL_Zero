{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMgkHx4PnUZ4PSsrPVgPVcy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第7章 〜ニューラルネットワークと Q 学習〜\n","\n","現実の問題は今までに扱ってきたグリッドワールドの問題のように単純ではなく、 Q 関数をコンパクトな関数で近似することが必要になる。  \n","そのための最も有力な手法がディープラーニングである。"],"metadata":{"id":"ZDKsz7ANk2KC"}},{"cell_type":"markdown","source":["## 線形回帰\n","\n","機械学習は「データ」を使って問題を解く。  \n","その中でも最も基本となる「線形回帰」を実装する。"],"metadata":{"id":"j8kA86aMlm61"}},{"cell_type":"markdown","source":["### トイ・データセット(*コーディング*)\n","\n","実験用に小さなデータセットを作る。  \n","そのような小さなデータセットを<font color=\"red\">**トイ・データセット**</font>(Toy Dataset)と呼ぶ。  \n","再現性を考慮して乱数のシードを固定してデータを生成する。"],"metadata":{"id":"P4ni7b6q18oB"}},{"cell_type":"code","source":["!pip install -U japanize-matplotlib"],"metadata":{"id":"XFYf0AwE3Wgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jb9eAV0SkxFy"},"outputs":[],"source":["# ライブラリのインポート\n","import japanize_matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","source":["# シードを固定して乱数を発生\n","seed = 8192\n","np.random.seed(seed)\n","\n","# トイ・データセットの作成\n","x = np.random.rand(100, 1)\n","y = 5 + 2 * x + np.random.rand(100, 1)"],"metadata":{"id":"uup0dwVz2urV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記のように、 `x` と `y` の2つの変数からなるデータセットを作る。  \n","ここで、点群は直線上にあり、 `y` にはノイズとして乱数が加算されている。"],"metadata":{"id":"TQwkEbIu5C1A"}},{"cell_type":"code","source":["# Matplotlib の初期設定\n","plt.figure(figsize=(10, 6), tight_layout=True)\n","plt.title(\"トイ・データセット\", size=15, color=\"red\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid()\n","\n","# トイ・データセットの点を散布図で描画\n","plt.scatter(x, y, label=\"トイ・データセットの点\", color=\"blue\")\n","\n","# 凡例を追加\n","plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", borderaxespad=0.)\n","\n","# 描画\n","plt.show()"],"metadata":{"id":"5mFtWua95Gh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["図を見ると `x` と `y` は「線形」の関係にあるが、そこにはノイズが含まれている。  \n","ここでの目標は `x` の値から `y` の値を予測するモデルを作ることである。"],"metadata":{"id":"adKBNfRx5a9F"}},{"cell_type":"markdown","source":["### 線形回帰の理論\n","\n","データが与えられたとき、それに適合するような関数を見つけることが目標になる。  \n","ここでは、 $y$ と $x$ の関係は線形であると仮定しているため、 $y = Wx + b$ という式で表せる。  \n","一方で、データが完全にこの式に適合する訳ではないため、データと式から導かれる予測値との差をできる限り減らすことが求められている。  \n","この差を「残差」といい、モデルの予測値とデータが「どれほど適合していないか」を表す指標にこの残差を用いて次のような式で定義する。\n","\n","$$L = \\frac{1}{N} \\sum_{i=1}^N (Wx_i + b - y_i)^2$$\n","\n","全部で $N$ 個の点があるとして、 $(x_i, y_i)$ の各点において2乗した誤差を求め、それらを足し合わせる。  \n","その平均を求めるために $N$ で割っている。  \n","これを<font color=\"red\">**平均2乗誤差**</font>(Mean Squared Error)という。  \n","我々の目標は、この損失関数が最小となる $W$ と $b$ を見つけることであり、すなわち関数の最適化問題である。"],"metadata":{"id":"tWgyAiui6Jce"}},{"cell_type":"markdown","source":["### 線形回帰の実装(*コーディング*)\n","\n","ここでは線形回帰を実装する。"],"metadata":{"id":"OcCRNOZNNVjQ"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import torch\n","from torch import nn\n","import torch.optim as optim"],"metadata":{"id":"AfEq-UpJN0kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PyTorch で作った線形回帰モデル\n","class LinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super(LinearRegressionModel, self).__init__()\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"metadata":{"id":"ayW0DddDOOFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# シードを固定して乱数を発生\n","seed = 8192\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","# トイ・データセットの作成\n","x = np.random.rand(100, 1)\n","y = 5 + 2 * x + np.random.rand(100, 1)\n","\n","# Numpy 配列を PyTorch のテンソルに変換\n","x_train = torch.tensor(x, dtype=torch.float32)\n","y_train = torch.tensor(y, dtype=torch.float32)\n","\n","# モデルのインスタンスを生成\n","model = LinearRegressionModel()\n","\n","# 損失関数(平均2乗誤差)と最適化アルゴリズム(SGD)の設定\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# 1000エポックで学習\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 100 == 0:\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","# 学習後のパラメータ(重みとバイアス)\n","[W, b] = model.parameters()\n","print(\"=====================\")\n","print(f\"学習後の重み: {W.item():.4f}\")\n","print(f\"学習後のバイアス: {b.item():.4f}\")"],"metadata":{"id":"IBiNskUdPweO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["損失関数の出力値が徐々に減っており、最終的には $W = 2, b = 5$ に近づいていることが確認できる。"],"metadata":{"id":"3NFtbLKFRerN"}},{"cell_type":"code","source":["# Matplotlib の初期設定\n","plt.figure(figsize=(10, 6), tight_layout=True)\n","plt.title(\"トイ・データセット\", size=15, color=\"red\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid()\n","\n","# トイ・データセットの点を散布図で描画\n","plt.scatter(x, y, label=\"トイ・データセットの点\", color=\"blue\")\n","\n","# 学習されたモデルのパラメータを取得\n","w_value = W.item()\n","b_value = b.item()\n","\n","# 学習された直線を描画\n","x_line = np.linspace(0, 1, 100)\n","y_line = w_value * x_line + b_value\n","plt.plot(x_line, y_line, label=\"学習された直線\", color=\"green\", linewidth=2)\n","\n","# 凡例を追加\n","plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", borderaxespad=0.)\n","\n","# 描画\n","plt.show()"],"metadata":{"id":"C7GjQzb_SYTH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上図の通り、データに適合したモデルを得ることができた。"],"metadata":{"id":"buFHRcpzVDTu"}},{"cell_type":"markdown","source":["## ニューラルネットワーク\n","\n","線形回帰が実装出来たので、同じように PyTorch を使ってニューラルネットワークを実装する。"],"metadata":{"id":"MKQINFFZcJDI"}},{"cell_type":"markdown","source":["### 非線形なデータセット(*コーディング*)\n","\n","前節では直線上に並ぶデータセットを使った。  \n","ここでは正弦関数を用いてデータを生成する。"],"metadata":{"id":"pColgiCactQ3"}},{"cell_type":"code","source":["# シードを固定して乱数を発生\n","seed = 8192\n","np.random.seed(seed)\n","\n","# トイ・データセットの作成\n","x = np.random.rand(100, 1)\n","y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n","\n","# Matplotlib の初期設定\n","plt.figure(figsize=(10, 6), tight_layout=True)\n","plt.title(\"非線形トイ・データセット\", size=15, color=\"red\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid()\n","\n","# トイ・データセットの点を散布図で描画\n","plt.scatter(x, y, label=\"トイ・データセットの点\", color=\"blue\")\n","\n","# 凡例を追加\n","plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", borderaxespad=0.)\n","\n","# 描画\n","plt.show()"],"metadata":{"id":"SQj9rSuqdAPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["図の通り、 `x` と `y` は線形の関係ではない。  \n","このような非線形なデータセットにはもちろん線形回帰は対応していない。  \n","そこでニューラルネットワークを使う。"],"metadata":{"id":"sUohiGx7fHCi"}},{"cell_type":"markdown","source":["### 線形変換と活性化関数\n","\n","線形回帰で行った計算は、損失関数を除くと「行列の積」と「足し算」だけであった。  \n","入力 `x` とパラメータ `W` との間で行列の積を求め、それに `b` を足し合わせた。  \n","この変換は、<font color=\"red\">**線形変換**</font>(Linear Transformation)や<font color=\"red\">**アフィン変換**</font>(Affine Transformation)と呼ばれる。  \n","線形変換は厳密には `b` の足し算を含まないが、ニューラルネットワークの分野ではここまでの演算を線形変換と呼ぶのが一般的である。  \n","また、線形変換はニューラルネットワークにおいては<font color=\"red\">**全結合層**</font>に対応する。  \n","パラメータの `W` は<font color=\"red\">**重み**</font>(Weight)、パラメータの `b` は<font color=\"red\">**バイアス**</font>(Bias)と呼ばれる。\n","\n","線形変換は、入力データに対して線形な変換を行うのに対し、ニューラルネットワークでは、線形変換の出力に対して非線形な変換を行う。  \n","その非線形な変換を行う関数を活性化関数と呼ぶ。  \n","代表的なものにはシグモイド関数や ReLU 関数などがある。\n","\n","![シグモイド関数](https://resources.zero2one.jp/2022/05/ai_exp_87-1-768x432.jpeg)\n","\n","(出典: https://zero2one.jp/ai-word/sigmoid-function/)\n","\n","![ReLU 関数](https://mathlandscape.com/wp-content/uploads/2022/05/relu-new-1536x884.png)\n","\n","(出典: https://mathlandscape.com/relu/)\n","\n","それぞれのグラフが示すように、これらの活性化関数は非線形の関数である。  \n","ニューラルネットワークでは、このような非線形な変換がテンソルの要素ごとに適用される。"],"metadata":{"id":"3_UPBzz5kFke"}},{"cell_type":"markdown","source":["### ニューラルネットワークの実装(*コーディング*)\n","\n","一般的なニューラルネットワークは「線形変換」と「活性化関数」を交互に使用する。  \n","これらを順に入力に対して適用することで、ニューラルネットワークの推論を可能にする。"],"metadata":{"id":"X1pWe7WJoSED"}},{"cell_type":"code","source":["# PyTorch で作ったニューラルネットワークモデル\n","class NeuralNetworkModel(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetworkModel, self).__init__()\n","        self.linear1 = nn.Linear(1, 10)\n","        self.sigmoid = nn.Sigmoid()\n","        self.linear2 = nn.Linear(10, 1)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.sigmoid(x)\n","        x = self.linear2(x)\n","        return x"],"metadata":{"id":"PrAQSqJkoxzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# シードを固定して乱数を発生\n","seed = 8192\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","# トイ・データセットの作成\n","x = np.random.rand(100, 1)\n","y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n","\n","# Numpy 配列を PyTorch のテンソルに変換\n","x_train = torch.tensor(x, dtype=torch.float32)\n","y_train = torch.tensor(y, dtype=torch.float32)\n","\n","# モデルのインスタンスを生成\n","model = NeuralNetworkModel()\n","\n","# 損失関数(平均2乗誤差)と最適化アルゴリズム(SGD)の設定\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.2)\n","\n","# 10000エポックで学習\n","num_epochs = 10000\n","for epoch in range(num_epochs):\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 1000 == 0:\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","# Matplotlib の初期設定\n","plt.figure(figsize=(10, 6), tight_layout=True)\n","plt.title(\"非線形トイ・データセット\", size=15, color=\"red\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid()\n","\n","# トイ・データセットの点を散布図で描画\n","plt.scatter(x, y, label=\"トイ・データセットの点\", color=\"blue\")\n","\n","# 学習された曲線を描画\n","x_eval = torch.tensor(\n","    [[data] for data in np.linspace(0, 1, 100)], dtype=torch.float32\n",")\n","model.eval()\n","with torch.no_grad():\n","    y_pred = model(x_eval).detach().numpy()\n","x_eval = x_eval.detach().numpy()\n","plt.plot(x_eval, y_pred, label=\"学習された曲線\", color=\"green\", linewidth=2)\n","\n","# 凡例を追加\n","plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", borderaxespad=0.)\n","\n","# 描画\n","plt.show()"],"metadata":{"id":"Qgp_3tovquED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["正弦関数の曲線が上手く表現できている。"],"metadata":{"id":"ViJSeYKwzWiC"}},{"cell_type":"markdown","source":["## Q 学習とニューラルネットワーク\n","\n","前章では TD 法、中でも Q 学習という強化学習において最も有名なアルゴリズムについて学んだ。  \n","ここでのテーマは、 Q 学習とニューラルネットワークを「融合」させることである。  \n","まずはニューラルネットワークの前処理について扱う。"],"metadata":{"id":"yohHL42e0iJr"}},{"cell_type":"markdown","source":["### ニューラルネットワークの前処理(*コーディング*)\n","\n","「3×4のグリッドワールド」の問題では、状態が(0, 0)や(0, 1)のように表された。  \n","この状態はエージェントの位置が $(y, x)$ のデータ形式で表現されており、12通りある「カテゴリデータ」と考えることができる。  \n","これらを one-hot ベクトルへ変換する。"],"metadata":{"id":"24EUwcAI2CMM"}},{"cell_type":"code","source":["# カテゴリデータを one-hot ベクトルに変換する関数を定義\n","def one_hot(state: set) -> list:\n","    HEIGHT, WIDTH = 3, 4\n","    vec = np.zeros(HEIGHT * WIDTH, dtype=np.float32)\n","    y, x = state\n","    idx = WIDTH * y + x\n","    vec[idx] = 1.0\n","    return vec[np.newaxis, :]"],"metadata":{"id":"YgB3LCFKhVMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 状態を one-hot ベクトルへ変換する\n","state = (2, 0)\n","x = one_hot(state)\n","print(f\"形式: {x.shape}\")\n","print(f\"変換後: {x}\")"],"metadata":{"id":"y74MG8XCjmSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Q 関数を表すニューラルネットワーク(*コーディング*)\n","\n","これまで Q 関数を次のようにテーブルとして実装してきた。"],"metadata":{"id":"9eNitbC3pIaa"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from collections import defaultdict"],"metadata":{"id":"n1gM8__UqzTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q 関数の辞書\n","Q = defaultdict(lambda: 0)\n","state = (2, 0)\n","action = 0\n","print(Q[state, action])"],"metadata":{"id":"cDRwzR4LsRKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["この Q 関数をニューラルネットワークへと変身させる。  \n","それにはまず、ニューラルネットワークの入出力をはっきりさせる必要がある。  \n","状態だけを入力として、行動の候補の数だけ Q 関数の値を出力するネットワークを考える。  \n","例えば、行動の候補が4つであれば4つの要素を持つベクトルを出力する。"],"metadata":{"id":"AbH9nHLGwn5u"}},{"cell_type":"code","source":["# Q 関数のニューラルネットワークモデル\n","class QNet(nn.Module):\n","    def __init__(self):\n","        super(QNet, self).__init__()\n","        self.linear1 = nn.Linear(12, 100)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(100, 4)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.linear2(x)\n","        return x"],"metadata":{"id":"i_iaqoN8wnA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# モデルのインスタンスを生成\n","qnet = QNet()\n","\n","# 状態を one-hot ベクトルへ変換する\n","state = (2, 0)\n","state = one_hot(state)\n","state = torch.tensor(state, dtype=torch.float32)\n","\n","# Q 関数のニューラルネットワークに値を入れる\n","qs = qnet(state)\n","print(qs.shape)"],"metadata":{"id":"rzZEhjno4kRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これで Q 関数をニューラルネットワークに置き換えることができた。"],"metadata":{"id":"cc2-BlD46eFP"}},{"cell_type":"markdown","source":["### ニューラルネットワークと Q 学習(*コーディング*)\n","\n","ここでは Q 学習のアルゴリズムを実装する。  \n","Q 学習では次の式によって Q 関数を更新する。\n","\n","$$Q'(S_t, A_t) = Q(S_t, A_t) + \\alpha \\left \\{ R_t + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right \\}$$\n","\n","この式によって $Q(S_t, A_t)$ の値は $R_t + \\gamma \\max_a Q(S_{t+1}, a)$ の方向へと更新される。  \n","ここでターゲットである $R_t + \\gamma \\max_a Q(S_{t+1}, a)$ を $T$ とすると、\n","\n","$$Q'(S_t, A_t) = Q(S_t, A_t) + \\alpha \\left \\{ T - Q(S_t, A_t) \\right \\}$$\n","\n","と表される。  \n","これは、入力が $S_t, A_t$ のとき出力が $T$ となるように Q 関数を更新すると解釈できる。  \n","これをニューラルネットワークの文脈に当てはめると、入力が $S_t, A_t$ で、出力が $T$ となるように学習させることと同じである。  \n","つまり $T$ はスカラ値の正解ラベルと見做せ、回帰問題と捉えることができる。"],"metadata":{"id":"kwJiaL_g6mv8"}},{"cell_type":"code","source":["# QLearningAgent クラスの実装\n","class QLearningAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.lr = 0.01\n","        self.eps = 0.1\n","        self.action_size = 4\n","\n","        self.qnet = QNet()\n","        self.optimizer = optim.SGD(self.qnet.parameters(), lr=self.lr)\n","        self.criterion = nn.MSELoss()\n","\n","    def get_action(self, state):\n","        if np.random.rand() < self.eps:\n","            return np.random.choice(self.action_size)\n","        else:\n","            qs = self.qnet(state)\n","            return torch.argmax(qs).item()\n","\n","    def update(self, state, action, reward, next_state, done):\n","        done = int(done)\n","        state = torch.tensor(state, dtype=torch.float32)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","        reward = torch.tensor([reward], dtype=torch.float32)\n","\n","        qs = self.qnet(state)\n","        q_value = qs[0, action]\n","\n","        with torch.no_grad():\n","            next_qs = self.qnet(next_state)\n","            next_q_value = next_qs.max()\n","            target = reward + (1 - done) * self.gamma * next_q_value\n","\n","        loss = self.criterion(q_value, target)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        return loss.item()"],"metadata":{"id":"TDhCWOfm-rzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GridWorld クラスの実装\n","class GridWorld:\n","    def __init__(self):\n","        self.action_space = [0, 1, 2, 3]\n","        self.action_meaning = {\n","            0: \"UP\",\n","            1: \"DOWN\",\n","            2: \"LEFT\",\n","            3: \"RIGHT\"\n","        }\n","        self.reward_map = np.array(\n","            [\n","                [0, 0, 0, 1.0],\n","                [0, None, 0, -1.0],\n","                [0, 0, 0, 0]\n","            ]\n","        )\n","        self.goal_state = (0, 3)\n","        self.wall_state = (1, 1)\n","        self.start_state = (2, 0)\n","        self.agent_state = self.start_state\n","\n","    @property\n","    def height(self):\n","        return len(self.reward_map)\n","\n","    @property\n","    def width(self):\n","        return len(self.reward_map[0])\n","\n","    @property\n","    def shape(self):\n","        return self.reward_map.shape\n","\n","    def actions(self):\n","        return self.action_space\n","\n","    def states(self):\n","        for h in range(self.height):\n","            for w in range(self.width):\n","                yield (h, w)\n","\n","    def next_state(self, state, action):\n","        action_move_map = [\n","            (-1, 0), (1, 0), (0, -1), (0, 1)\n","        ]\n","        move = action_move_map[action]\n","        next_state = (state[0] + move[0], state[1] + move[1])\n","        ny, nx = next_state\n","\n","        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:\n","            next_state = state\n","        elif next_state == self.wall_state:\n","            next_state = state\n","\n","        return next_state\n","\n","    def reward(self, state, action, next_state):\n","        return self.reward_map[next_state]\n","\n","    def step(self, action):\n","        state = self.agent_state\n","        next_state = self.next_state(state, action)\n","        reward = self.reward(state, action, next_state)\n","        done = (next_state == self.goal_state)\n","\n","        self.agent_state = next_state\n","\n","        return next_state, reward, done\n","\n","    def reset(self):\n","        self.agent_state = self.start_state\n","        return self.agent_state"],"metadata":{"id":"-Q2vZnJAUKO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ロスをプロットする関数を定義\n","def loss_show(history: list) -> None:\n","    x = [i for i in range(1, len(history) + 1)]\n","\n","    plt.figure(figsize=(8, 6), tight_layout=True)\n","    plt.title(\"ロスの遷移\", size=15, color=\"red\")\n","    plt.grid()\n","    plt.plot(x, history)"],"metadata":{"id":"_iRcbiTbZqNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = GridWorld()\n","agent = QLearningAgent()\n","\n","# 1000回のエピソードで実行\n","episodes = 1000\n","loss_history = []\n","for episode in range(episodes):\n","    state = env.reset()\n","    state = one_hot(state)\n","    state = torch.tensor(state, dtype=torch.float32)\n","    total_loss, cnt = 0, 0\n","    done = False\n","\n","    while not done:\n","        action = agent.get_action(state)\n","        next_state, reward, done = env.step(action)\n","        next_state = one_hot(next_state)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","\n","        loss = agent.update(\n","            state, action, reward, next_state, done\n","        )\n","        total_loss += loss\n","        cnt += 1\n","        state = next_state\n","\n","    average_loss = total_loss / cnt\n","    loss_history.append(average_loss)\n","\n","# ロスの遷移画像を描画\n","loss_show(loss_history)"],"metadata":{"id":"9qCBP9gDUVGI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ニューラルネットワークを使った強化学習では、損失をプロットしても安定した結果が得られないことが多々ある。  \n","変化の振れ幅は大きいが、大きな視点ではエピソードを重ねるごとにロスが小さくなっていることが確認できる。"],"metadata":{"id":"gC0opAWSbfib"}}]}