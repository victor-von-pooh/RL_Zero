{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP0lKtOkKHKl617ll7DLN08"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第3章 〜ベルマン方程式〜\n","\n","ここでは、確率的に振る舞う MDP において成り立つ最も重要な方程式である「<font color=\"red\">**ベルマン方程式**</font>」について扱う。"],"metadata":{"id":"Fmg2aHxL1g0Q"}},{"cell_type":"markdown","source":["## ベルマン方程式とは\n","\n","ベルマン方程式(Bellman Equation)の導出を行う上で、その下準備として確率と期待値について軽く触れる。"],"metadata":{"id":"pqBdDiny2Qnc"}},{"cell_type":"markdown","source":["### ベルマン方程式の導出\n","\n","次のような問題を考える。  \n","\n","1. サイコロを振る\n","2. 出た目が偶数であれば確率0.8で表が出るコインを、奇数であれば確率0.5で表が出るコインを投げる\n","3. 投げたコインが表の場合は最初に出たサイコロの目の数を、裏の場合は0を報酬として受け取る\n","\n","これの期待値は\n","\n","\\begin{eqnarray*}\n","&& \\frac{1}{6} \\cdot \\frac{1}{2} \\cdot 1 + \\frac{1}{6} \\cdot \\frac{1}{2} \\cdot 0 + \\frac{1}{6} \\cdot \\frac{4}{5} \\cdot 2 + \\frac{1}{6} \\cdot \\frac{1}{5} \\cdot 0 + \\frac{1}{6} \\cdot \\frac{1}{2} \\cdot 3 + \\frac{1}{6} \\cdot \\frac{1}{2} \\cdot 0 + \\\\\n","&& \\frac{1}{6} \\cdot \\frac{4}{5} \\cdot 4 + \\frac{1}{6} \\cdot \\frac{1}{5} \\cdot 0 + \\frac{1}{6} \\cdot \\frac{1}{2} \\cdot 5 + \\frac{1}{6} \\cdot \\frac{1}{2} \\cdot 0 + \\frac{1}{6} \\cdot \\frac{4}{5} \\cdot 6 + \\frac{1}{6} \\cdot \\frac{1}{5} \\cdot 0 \\\\\n","&=& 2.35\n","\\end{eqnarray*}\n","\n","となる。  \n","ここで、この計算を文字式で表すと次のようになる。  \n","サイコロの目を $x$ として、コインの結果(表か裏)を $y$ とする。  \n","具体的な条件付き確率 $p(y|x)$ は、例えば次のようになる。\n","\n","\\begin{eqnarray*}\n","p(y = \\text{表} | x = 4) &=& 0.8 \\\\\n","p(y = \\text{裏} | x = 3) &=& 0.5\n","\\end{eqnarray*}\n","\n","また、 $x$ と $y$ が同時に起こる確率(<font color=\"red\">**同時確率**</font>)は、\n","\n","$$p(x, y) = p(x)p(y|x)$$\n","\n","である。  \n","この問題については、 $x$ と $y$ の値によって貰える報酬が決まるので、報酬は関数 $r(x, y)$ として表すことができる。  \n","例えば、\n","\n","\\begin{eqnarray*}\n","r(x = 4, y = \\text{表}) &=& 4 \\\\\n","r(x = 3, y = \\text{裏}) &=& 0\n","\\end{eqnarray*}\n","\n","のようになる。  \n","期待値は、値とその値が起こる確率の積の総和なので、\n","\n","\\begin{eqnarray*}\n","\\mathbb{E}[r(x, y)] &=& \\sum_x \\sum_y p(x, y) r(x, y) \\\\\n","&=& \\sum_x \\sum_y p(x) p(y|x) r(x, y)\n","\\end{eqnarray*}\n","\n","となる。  \n","以降、 $\\sum_x \\sum_y$ という表記は $\\sum_{x, y}$ とまとめることにする。\n","\n","ここからは、いよいよベルマン方程式の導出に入る。  \n","第2章で定義した「収益」は次のようなものだった。\n","\n","$$G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots$$\n","\n","ここで、 $t+1$ の式を考えると\n","\n","\\begin{eqnarray*}\n","G_t &=& R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\\\\n","&=& R_t + \\gamma \\left( R_{t+1} + \\gamma R_{t+2} + \\cdots \\right) \\\\\n","&=& R_t + \\gamma G_{t+1}\n","\\end{eqnarray*}\n","\n","という $G_t$ と $G_{t+1}$ の関係式が導かれる。  \n","状態価値関数の中に代入すると、期待値の線形性より\n","\n","\\begin{eqnarray*}\n","v_{\\pi}(s) &=& \\mathbb{E}_\\pi[G_t | S_t = s] \\\\\n","&=& \\mathbb{E}_\\pi[R_t + \\gamma G_{t+1} | S_t = s] \\\\\n","&=& \\mathbb{E}_\\pi[R_t | S_t = s] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s]\n","\\end{eqnarray*}\n","\n","となる。  \n","今、状態が $s$ であり、エージェントは方策 $\\pi(a|s)$ に従って行動する。  \n","そして、状態遷移確率 $p(s'|s, a)$ に従って新しい状態 $s'$ に移行する。  \n","最後に、報酬が $r(s, a, s')$ という関数により決定するという状況である。  \n","よって、期待値 $\\mathbb{E}_\\pi[R_t | S_t = s]$ は次のような式で表される。\n","\n","$$\\mathbb{E}_\\pi[R_t | S_t = s] = \\sum_{a, s'} \\pi(a|s) p(s'|s, a) r(s, a, s')$$\n","\n","さて、続いて期待値 $\\mathbb{E}_\\pi[G_{t+1} | S_t = s]$ について考えたいが、これは時刻 $t$ のとき、1つ先の時刻 $t+1$ における収益の期待値を表している。  \n","まず、時刻 $t+1$ のときの状態価値関数は次のように表せる。\n","\n","$$v_{\\pi}(s') = \\mathbb{E}_\\pi[G_{t+1} | S_{t+1} = s']$$\n","\n","ここで、時刻 $t$ で状態 $s$ を取り、時刻 $t+1$ で状態 $s'$ に遷移することを考える。  \n","確率 $\\pi(a|s)$ で行動 $a$ を選択し、確率 $p(s'|s, a)$ で状態 $s'$ に遷移するので、期待値 $\\mathbb{E}_\\pi[G_{t+1} | S_t = s]$ を求めるためにはこの計算を全ての候補に対して行い、総和を求める。\n","\n","\\begin{eqnarray*}\n","\\mathbb{E}_\\pi[G_{t+1} | S_t = s] &=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\mathbb{E}_\\pi[G_{t+1} | S_{t+1} = s']\\\\\n","&=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) v_{\\pi}(s')\n","\\end{eqnarray*}\n","\n","よって、状態価値関数は\n","\n","\\begin{eqnarray*}\n","v_{\\pi}(s) &=& \\mathbb{E}_\\pi[R_t | S_t = s] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s] \\\\\n","&=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) r(s, a, s') + \\gamma \\sum_{a, s'} \\pi(a|s) p(s'|s, a) v_{\\pi}(s') \\\\\n","&=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}\n","\\end{eqnarray*}\n","\n","となり、これがベルマン方程式である。"],"metadata":{"id":"Pt-F6rB9X-r5"}},{"cell_type":"markdown","source":["### ベルマン方程式の例(*コーディング*)\n","\n","ここではまた2マスのグリッドワールドを考える。  \n","エージェントは、等確率で左右に動くとする。"],"metadata":{"id":"5vVQnPZ5YHaG"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"dbdVg2hZZxMW","executionInfo":{"status":"ok","timestamp":1725694429223,"user_tz":-540,"elapsed":527,"user":{"displayName":"Hiroki Akita","userId":"14124573534976314682"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# グリッドワールドを作成する関数を定義\n","# 0: 進める道, 1: 障害物, 2: スタート, 3: ゴール, 4: 加点ポイント, 5: 減点ポイント\n","def create_grid_world(\n","        rows: int, cols: int,\n","        start: set, goal: set,\n","        obstacles: list, items: dict, enemies: dict\n","    ):\n","    grid = np.zeros((rows, cols))\n","    points = np.zeros((rows, cols), dtype=int)\n","\n","    for obs in obstacles:\n","        grid[obs] = 1\n","\n","    grid[start] = 2\n","    if goal != ():\n","        grid[goal] = 3\n","\n","    for item, point in items.items():\n","        grid[item] = 4\n","        points[item] = point\n","\n","    for enemy, point in enemies.items():\n","        grid[enemy] = 5\n","        points[enemy] = point\n","\n","    return grid, points\n","\n","\n","# グリッドワールドを描画する関数を定義\n","def draw_grid_world(grid, points, figsize_x, figsize_y):\n","    rows, cols = grid.shape\n","    fig = plt.figure(figsize=(figsize_x, figsize_y), tight_layout=True)\n","    ax = fig.subplots()\n","\n","    ax.matshow(grid, vmin=-0.5, vmax=5)\n","\n","    ax.set_xticks(np.arange(-.5, cols, 1), minor=True)\n","    ax.set_yticks(np.arange(-.5, rows, 1), minor=True)\n","    ax.grid(which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=2)\n","\n","    for (i, j), val in np.ndenumerate(grid):\n","        if val == 1:\n","            ax.text(\n","                j, i, \"X\", ha=\"center\", va=\"center\",\n","                color=\"yellow\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 2:\n","            ax.text(\n","                j, i, \"S\", ha=\"center\", va=\"center\",\n","                color=\"red\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 3:\n","            ax.text(\n","                j, i, \"G\", ha=\"center\", va=\"center\",\n","                color=\"blue\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 4:\n","            ax.text(\n","                j, i, f\"+{points[i, j]}\", ha=\"center\", va=\"center\",\n","                color=\"white\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 5:\n","            ax.text(\n","                j, i, f\"{points[i, j]}\", ha=\"center\", va=\"center\",\n","                color=\"black\", fontsize=16, fontweight=\"bold\"\n","            )\n","\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","    plt.show()"],"metadata":{"id":"lM_FpIMGZoCv","executionInfo":{"status":"ok","timestamp":1725694429569,"user_tz":-540,"elapsed":15,"user":{"displayName":"Hiroki Akita","userId":"14124573534976314682"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# グリッドワールドのパラメータを設定\n","rows, cols = 1, 4\n","start = (0, 1)\n","goal = ()\n","obstacles = []\n","items = {(0, 2): +1}\n","enemies = {(0, 0): -1, (0, 3): -1}\n","\n","# グリッドワールドを作成して描画\n","grid, points = create_grid_world(\n","    rows, cols, start, goal, obstacles, items, enemies\n",")\n","draw_grid_world(grid, points, cols, rows)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"9Igwtm4HZnNU","executionInfo":{"status":"ok","timestamp":1725694429885,"user_tz":-540,"elapsed":330,"user":{"displayName":"Hiroki Akita","userId":"14124573534976314682"}},"outputId":"d064b95c-cfe8-493c-a3ab-b6dbb2289273"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 400x100 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["$v_\\pi(s = (0, 1))$ は、状態が(0, 1)でランダムな方策 $\\pi$ に従って行動したときに得られる収益の期待値である。  \n","ベルマン方程式を使ってこれを表してみる。\n","\n","\\begin{eqnarray*}\n","v_{\\pi}(s) &=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\} \\\\\n","&=& \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}\n","\\end{eqnarray*}\n","\n","状態は決定論的に遷移するので、 $p(s'|s, a)$ は $s'$ に遷移するときに1でそれ以外は0になる。  \n","よって上記の式はシンプルに\n","\n","$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}$$\n","\n","となる。\n","\n","それでは具体的に問題に当てはめてみる。  \n","状態(0, 1)から確率0.5で行動 Left を選択し、また状態(0, 1)に戻ってくる(このときの報酬は-1)。  \n","割引率 $\\gamma$ を0.9とすると行動 Left を選択したときの式は次のようになる。\n","\n","$$\\pi(\\text{Left}|(0, 1)) \\left\\{ r((0, 1), \\text{Left}, (0, 1)) + \\gamma v_{\\pi}((0, 1)) \\right\\} = 0.5\\left\\{ -1 + 0.9 v_{\\pi}((0, 1)) \\right\\}$$\n","\n","また、確率0.5で行動 Right を選択し、状態(0.2)に遷移する(報酬は1)。  \n","同じく割引率 $\\gamma$ を0.9とすると行動 Right を選択したときの式は次のようになる。\n","\n","$$\\pi(\\text{Right}|(0, 1)) \\left\\{ r((0, 1), \\text{Right}, (0, 2)) + \\gamma v_{\\pi}((0, 2)) \\right\\} = 0.5\\left\\{ 1 + 0.9 v_{\\pi}((0, 2)) \\right\\}$$\n","\n","以上の2パターンが考えられるので、ベルマン方程式は\n","\n","$$v_{\\pi}((0, 1)) = 0.5\\left\\{ -1 + 0.9 v_{\\pi}((0, 1)) \\right\\} + 0.5\\left\\{ 1 + 0.9 v_{\\pi}((0, 2)) \\right\\}$$\n","\n","であり、これを整理すると\n","\n","$$-0.55 v_{\\pi}((0, 1)) + 0.45 v_{\\pi}((0, 2)) = 0$$\n","\n","となる。  \n","同様に、状態(0, 2)からのベルマン方程式を考えると、\n","\n","$$v_{\\pi}((0, 2)) = 0.5\\left\\{ 0 + 0.9 v_{\\pi}((0, 1)) \\right\\} + 0.5\\left\\{ -1 + 0.9 v_{\\pi}((0, 2)) \\right\\}$$\n","\n","であり、整理すると\n","\n","$$0.45 v_{\\pi}((0, 1)) - 0.55 v_{\\pi}((0, 2)) = 0.5$$\n","\n","となる。  \n","よって連立方程式\n","\n","\\begin{equation}\n","\\left\\{ \\,\n","    \\begin{aligned}\n","    -0.55 v_{\\pi}((0, 1)) + 0.45 v_{\\pi}((0, 2)) &= 0 \\\\\n","    0.45 v_{\\pi}((0, 1)) - 0.55 v_{\\pi}((0, 2)) &= 0.5\n","    \\end{aligned}\n","\\right.\n","\\end{equation}\n","\n","を解くと、\n","\n","\\begin{equation}\n","\\left\\{ \\,\n","    \\begin{aligned}\n","    v_{\\pi}((0, 1)) &= -2.25 \\\\\n","    v_{\\pi}((0, 2)) &= -2.75\n","    \\end{aligned}\n","\\right.\n","\\end{equation}\n","\n","ということが分かる。  \n","これがランダムな方策における状態価値関数になる。"],"metadata":{"id":"filKXr3maQx1"}},{"cell_type":"markdown","source":["## 行動価値関数とベルマン方程式\n","\n","ここでは新たに<font color=\"red\">**行動価値関数**</font>(Action-Value Function)について説明する。  \n","これを慣例として <font color=\"red\">**Q 関数**</font>(Q Function)ということもある。"],"metadata":{"id":"5qQgufyV2ya9"}},{"cell_type":"markdown","source":["### Q 関数とは\n","\n","状態が $s$ で方策が $\\pi$ のとき、状態価値関数は\n","\n","$$v_{\\pi}(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$\n","\n","と定義された。  \n","ここに、行動 $a$ の条件を加えたものが Q 関数である。  \n","数式で表すと次のようなものである。\n","\n","$$q_{\\pi}(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$$\n","\n","これは、時刻 $t$ のときに状態 $s$ で行動 $a$ を取り、時刻 $t+1$ 以降では方策 $\\pi$ に従った行動を取り、そのときに得られる収益の期待値を表している。  \n","ここで、 $q_{\\pi}(s, a)$ の行動 $a$ は方策 $\\pi$ とは無関係なことに注意する。"],"metadata":{"id":"Q8W94JDQ3lEa"}},{"cell_type":"markdown","source":["### Q 関数に関するベルマン方程式\n","\n","状態価値関数に倣って Q 関数を展開すると、\n","\n","\\begin{eqnarray*}\n","q_{\\pi}(s, a) &=& \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\\n","&=& \\mathbb{E}_\\pi[R_t + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\\n","&=& \\mathbb{E}_\\pi[R_t | S_t = s, A_t = a] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s, A_t = a]\n","\\end{eqnarray*}\n","\n","であり、状態 $s$ と行動 $a$ が決まっていることを考慮すると、\n","\n","\\begin{eqnarray*}\n","q_{\\pi}(s, a) &=& \\mathbb{E}_\\pi[R_t | S_t = s, A_t = a] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s, A_t = a] \\\\\n","&=& \\sum_{s'} p(s'|s, a) r(s, a, s') + \\gamma \\sum_{s'} p(s'|s, a) v_{\\pi}(s') \\\\\n","&=& \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\} \\\\\n","&=& \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma \\sum_{a'} \\pi (a'|s') q_{\\pi}(s', a') \\right\\}\n","\\end{eqnarray*}\n","\n","としてベルマン方程式が表せる。"],"metadata":{"id":"TAaya2u-fF9D"}},{"cell_type":"markdown","source":["## ベルマン最適方程式\n","\n","ベルマン方程式は、ある方策 $\\pi$ に関して成り立つ方程式だが、最適方策ももちろんベルマン方程式を満たす。  \n","ここでは、最適方策に関して成り立つ<font color=\"red\">**ベルマン最適方程式**</font>(Bellman Optimality Equation)について説明する。"],"metadata":{"id":"QTBOGlB8mrmW"}},{"cell_type":"markdown","source":["### 状態価値関数におけるベルマン最適方程式\n","\n","ベルマン方程式は次のように表された。\n","\n","\\begin{eqnarray*}\n","v_{\\pi}(s) &=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\} \\\\\n","&=& \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}\n","\\end{eqnarray*}\n","\n","ベルマン方程式はどの方策を取っても成り立つので、最適方策を $\\pi_*(a|s)$ とすると次のベルマン方程式が成り立つ。\n","\n","$$v_*(s) = \\sum_{a} \\pi_*(a|s) \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}$$\n","\n","ここで考えたいのは、最適方策 $\\pi_*(a|s)$ によって選ばれる行動 $a$ についてである。  \n","まず行動が複数あり、その中の最適な行動を $a_*$ とすると、最適方策はこれを100%選択することになるので、確率的方策 $\\pi_*(a|s)$ は決定論的方策として考えることができる。   \n","すなわち、最適方策は $\\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}$ の値が最適の行動を選択し、その最大値がそのまま $v_*(s)$ になる。  \n","これを数式で表すと、\n","\n","$$v_*(s) = \\max_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}$$\n","\n","となる。"],"metadata":{"id":"74WMiXaynnMe"}},{"cell_type":"markdown","source":["### Q 関数におけるベルマン最適方程式\n","\n","Q 関数についても状態価値関数と同様にベルマン最適方程式を求められる。  \n","まず、 Q 関数におけるベルマン方程式は\n","\n","$$q_{\\pi}(s, a) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma \\sum_{a'} \\pi (a'|s') q_{\\pi}(s', a') \\right\\}$$\n","\n","で表された。  \n","こちらも最適方策を $\\pi_*(a|s)$ とすると次のベルマン方程式が成り立つ。\n","\n","$$q_*(s, a) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma \\sum_{a'} \\pi_* (a'|s') q_*(s', a') \\right\\}$$\n","\n","$\\pi_*$ は最適方策なので、先ほど同様に $\\max$ 演算子を用いると、\n","\n","$$q_*(s, a) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma \\max_{a'} q_*(s', a') \\right\\}$$\n","\n","として Q 関数におけるベルマン最適方程式が表せる。"],"metadata":{"id":"U3D5FwkRsdGF"}},{"cell_type":"markdown","source":["## ベルマン最適方程式の例\n","\n","先ほどの2マスのグリッドワールドの問題を再度考える。"],"metadata":{"id":"cvgYDJ3iTcKF"}},{"cell_type":"markdown","source":["### ベルマン最適方程式の適用\n","\n","まず、ベルマン最適方程式は次のように数式で表せた。\n","\n","$$v_*(s) = \\max_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}$$\n","\n","さらに、状態遷移が確率的でなく決定論的な場合は次のように簡略化できる。\n","\n","$$v_*(s) = \\max_a \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}$$\n","\n","これを2マスのグリッドワールドに適用する。  \n","ここで、割引率 $\\gamma$ を0.9とする。\n","\n","\\begin{equation}\n","v_*((0, 1)) = \\max \\left\\{ \\,\n","    \\begin{aligned}\n","    -1 + 0.9 v_*((0, 1))&, \\\\\n","    1 + 0.9 v_*((0, 2))&\n","    \\end{aligned}\n","\\right\\} \\\\\n","v_*((0, 2)) = \\max \\left\\{ \\,\n","    \\begin{aligned}\n","    0.9 v_*((0, 1))&, \\\\\n","    -1 + 0.9 v_*((0, 2))&\n","    \\end{aligned}\n","\\right\\}\n","\\end{equation}\n","\n","この連立方程式の解は\n","\n","\\begin{eqnarray*}\n","v_*((0, 1)) &=& 5.26 \\\\\n","v_*((0, 2)) &=& 4.73\n","\\end{eqnarray*}\n","\n","となる。  \n","これは $\\max$ 関数を使っているため、線形なソルバーでは解くことができない。  \n","一方、非線形なソルバーを使うことでこのような場合でも解くことができる。"],"metadata":{"id":"Sh5IQakpTtjS"}},{"cell_type":"markdown","source":["### 最適方策を得る\n","\n","2マスのグリッドワールドのような小さな問題であれば、ベルマン最適方程式は直接解くことができる。  \n","しかし、最終的に知りたいのは最適方策である。  \n","ここでは最適 Q 関数 $q_*(s, a)$ が分かっていると仮定する。  \n","このとき、状態 $s$ における最適な行動は、\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu_*(s) = \\argmax_a q_*(s, a)\n","$$\n","\n","によって得られる。  \n","また、\n","\n","$$q_{\\pi}(s, a) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}$$\n","\n","が成り立つので、上記の式に代入すると\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","\\mu_*(s) = \\argmax_a \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\}\n","$$\n","\n","と、最適状態価値関数 $v_*(s)$ を使って最適方策 $\\mu_*(s)$ を得ることができる。\n","\n","それでは、この式を使って2マスのグリッドワールドの問題の最適方策を求めてみる。  \n","まずは状態(0, 1)における最適な行動を求める。  \n","行動 Left を取ると\n","\n","\\begin{eqnarray*}\n","\\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\} &=& r(s, a, s') + \\gamma v_*(s') \\\\\n","&=& -1 + 0.9 \\cdot 5.26 \\\\\n","&=& 3.734\n","\\end{eqnarray*}\n","\n","であり、行動 Right を取ると\n","\n","\\begin{eqnarray*}\n","\\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_*(s') \\right\\} &=& r(s, a, s') + \\gamma v_*(s') \\\\\n","&=& 1 + 0.9 \\cdot 4.73 \\\\\n","&=& 5.257\n","\\end{eqnarray*}\n","\n","になる。  \n","よって状態(0, 1)における最適な行動は Right であると分かる。  \n","同様に状態(0, 2)における最適な行動を求めると Left になる。  \n","以上から、状態(0, 1)にいるときは右に、状態(0, 2)にいるときは左に進むことが最適方策となる。"],"metadata":{"id":"AnybDY1TXBrW"}}]}