{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyM/pHYafZOMp8hfjSGEbxZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第8章 〜DQN〜\n","\n","ここでは、 Q 学習とニューラルネットワークを使った手法である「<font color=\"red\">**DQN**</font>」について扱う。"],"metadata":{"id":"fZt1hRkTdqzY"}},{"cell_type":"markdown","source":["## OpenAI Gym\n","\n","OpenAI Gym はオープンソースライブラリであり、様々な強化学習のタスク(環境)が用意されている。  \n","ここでは、 OpenAI Gym の基本的な使い方について学ぶ。"],"metadata":{"id":"mZK4mSiIeM0T"}},{"cell_type":"markdown","source":["### OpenAI Gym の基礎知識(*コーディング*)\n","\n","OpenAI Gym には様々な環境が用意されているが、ここでは「カートポール」を見てみる。"],"metadata":{"id":"qDib4me-e-qk"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import gym"],"metadata":{"id":"M-ZnKuSlfUSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OpenAI Gym から環境を指定する\n","env = gym.make(\"CartPole-v1\")"],"metadata":{"id":"Q1vjsn02fa70"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これで「カートポール」という環境が生成された。  \n","カートポールは、カートを右もしくは左に動かしてボールのバランスを保つ。  \n","終了条件は、ポールのバランスが崩れるか、もしくはカートの位置がある範囲を超えて移動するかである。"],"metadata":{"id":"6laTNBnsfw1N"}},{"cell_type":"code","source":["# 初期状態\n","state = env.reset()\n","print(f\"初期状態: {state}\")\n","\n","# 行動の次元数\n","action_space = env.action_space\n","print(f\"行動の次元数: {action_space}\")"],"metadata":{"id":"ZeWCuLEwgMk4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["状態は頭から順に\n","\n","- カートの位置\n","- カートの速度\n","- ポールの角度\n","- ポールの角速度\n","\n","を表している。  \n","行動の次元数は、 `Discrete(2)` という独自のクラスのインスタンスになっている。  \n","これは2つの行動の候補があることを意味している。  \n","具体的には `0` が左向き、 `1` が右向きにカートを移動させる行動に対応する。\n","\n","それでは、実際に行動を起こして時間を1つ進めてみる。"],"metadata":{"id":"rbRCy6zbhNbP"}},{"cell_type":"code","source":["# 左向きの行動を1回起こす\n","action = 0\n","next_state, reward, done, info = env.step(action)\n","print(next_state)"],"metadata":{"id":"aAU0Mi-Mh6Rh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記のように、 `env.step(action)` によって行動が実行されている。  \n","その結果として次の4つの情報が得られる。\n","\n","- 次の状態(`next_state`)\n","- 報酬(`reward`)\n","- 終了かどうかのフラグ(`done`)\n","- 追加の情報(`info`)"],"metadata":{"id":"NZFcsZmpu0v4"}},{"cell_type":"markdown","source":["### ランダムなエージェント(*コーディング*)\n","\n","続いてコードを1つにまとめて動かしてみる。"],"metadata":{"id":"eUrIZUKKvVsF"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"uYHwVQS8vgks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# インスタンスの初期化\n","env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", new_step_api=True)\n","state = env.reset()\n","done = False\n","\n","# 1回分のエピソードを動かす\n","while not done:\n","    action = np.random.choice([0, 1])\n","    next_state, reward, done, truncated, info = env.step(action)\n","\n","    done = done or truncated\n","\n","    frame = env.render()\n","    clear_output(wait=True)\n","    plt.figure(figsize=(10, 6), tight_layout=True)\n","    plt.imshow(frame[0])\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","env.close()"],"metadata":{"id":"gq50j7DqvlCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DQN のコア技術\n","\n","Q 学習では、推定値を使って推定値を更新する。  \n","まだ正確ではない推定値を使って今ある推定値を更新するため、 Q 学習は不安定になりやすい性質がある。  \n","そこにニューラルネットワークのような表現力の高い関数近似法が加わると結果はさらに不安定になる。  \n","DQN は Q 学習とニューラルネットワークを組み合わせた手法であり、その特徴は、ニューラルネットワークの学習を安定させるために<font color=\"red\">**経験再生**</font>(Experience Replay)と<font color=\"red\">**ターゲットネットワーク**</font>(Target Network)という技術を使っていることである。"],"metadata":{"id":"OOW5irkL0gvS"}},{"cell_type":"markdown","source":["### 経験再生\n","\n","Q 学習では、エージェントが環境に対して行動を行うたびにデータが生成される。  \n","具体的には、ある時間 $t$ において得られた $E_t = (S_t, A_t, R_t, S_{t+1})$ を使って Q 関数を更新する。  \n","ここで $E_t$ を経験データと呼ぶとする。  \n","この経験データは時間 $t$ が進むに従って得られるが、経験データ間には強い相関がある。  \n","つまり Q 学習では相関の強いデータを使って学習を行なっているため、教師あり学習のようにミニバッチ化をしてデータに偏りがないように学習させるということが出来ないのである。  \n","これを克服するのが経験再生である。\n","\n","まずはエージェントが経験したデータ $E_t = (S_t, A_t, R_t, S_{t+1})$ を一度「バッファ」に保存する。  \n","そして、 Q 関数を更新する際はそのバッファから経験データをランダムに取り出す。  \n","これにより、経験データ間の相関が弱まり、偏りの少ないデータが得られる。  \n","さらに、経験データを繰り返し使うことができるため、データ効率が良くなる。  \n","ただし、経験再生は方策オフ型のアルゴリズムでしか使えないので、そこは注意が必要である。"],"metadata":{"id":"66eugHUH2ayM"}},{"cell_type":"markdown","source":["### 経験再生の実装(*コーディング*)\n","\n","経験再生のバッファは、現実的には無限にデータを格納することは出来ない。  \n","そこで、最大のサイズを例えば50000個などと予め設定しておく。  \n","その最大サイズを超えてデータを追加したい場合には古いデータから順に削除する。  \n","それでは、経験再生の仕組みを `ReplayBuffer` というクラスで実装する。"],"metadata":{"id":"sJ8j9cOE5PuF"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from collections import deque\n","import random"],"metadata":{"id":"8EywWtkd52id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 経験再生の仕組みである ReplayBuffer クラスの実装\n","class ReplayBuffer:\n","    def __init__(self, buffer_size, batch_size):\n","        self.buffer = deque(maxlen=buffer_size)\n","        self.batch_size = batch_size\n","\n","    def add(self, state, action, reward, next_state, done):\n","        data = (state, action, reward, next_state, done)\n","        self.buffer.append(data)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def get_batch(self):\n","        data = random.sample(self.buffer, self.batch_size)\n","\n","        state = np.stack([x[0] for x in data])\n","        action = np.stack([x[1] for x in data])\n","        reward = np.stack([x[2] for x in data])\n","        next_state = np.stack([x[3] for x in data])\n","        done = np.stack([x[4] for x in data]).astype(np.int32)\n","\n","        return state, action, reward, next_state, done"],"metadata":{"id":"CKRNsw9I59vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# インスタンスの初期化\n","env = gym.make(\"CartPole-v1\")\n","replay_buffer = ReplayBuffer(buffer_size=10000, batch_size=32)\n","\n","# 10回エピソードを繰り返す\n","for _ in range(10):\n","    state = env.reset()\n","    done = False\n","\n","    while not done:\n","        action = 0\n","        next_state, reward, done, info = env.step(action)\n","        replay_buffer.add(state, action, reward, next_state, done)\n","        state = next_state\n","\n","# ミニバッチを確認する\n","state, action, reward, next_state, done = replay_buffer.get_batch()\n","print(f\"state: {state.shape}\")\n","print(f\"action: {action.shape}\")\n","print(f\"reward: {reward.shape}\")\n","print(f\"next_state: {next_state.shape}\")\n","print(f\"done: {done.shape}\")"],"metadata":{"id":"wu6YP7Br77NW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これでバッチサイズのデータが `np.ndarray` のインスタンスとして取り出されていることがわかった。"],"metadata":{"id":"pXAMT1x790C5"}},{"cell_type":"markdown","source":["### ターゲットネットワーク\n","\n","Q 学習では、 $Q(S_t, A_t)$ の値が、 TD ターゲット $R_t + \\gamma \\max_a Q(S_{t+1}, a)$ となるように Q 関数を更新する。  \n","TD ターゲットの値は、 Q 関数が更新されるたびに変動するので、教師あり学習のようにラベルが学習途中で変わらないということがない。  \n","これはターゲットネットワークというテクニックにより TD ターゲットを固定することで克服できる。\n","\n","まずは Q 関数を表すオリジナルのネットワーク(`qnet` と呼ぶことにする)を用意する。  \n","それとは別にもう1つ同じ構造のネットワーク(`qnet_target` と呼ぶことにする)を用意する。  \n","`qnet` は通常の Q 学習によって更新を行い、 `qnet_target` は定期的に `qnet` の重みと同期するようにして、それ以外は重みパラメータを固定したままにする。  \n","あとは `qnet_target` を使って TD ターゲットの値を計算すれば、 TD ターゲットの変動が抑えられるのでニューラルネットワークの学習が安定することが期待される。"],"metadata":{"id":"1gpTJijs-ECB"}},{"cell_type":"markdown","source":["### ターゲットネットワークの実装(*コーディング*)\n","\n","`DQNAgent` というエージェントのクラスを実装する。  \n","まずはこの `DQNAgent` クラスに対応できるように `ReplayBuffer` クラスと `QNet` クラスを改めて用意する。"],"metadata":{"id":"lEKLDJLHoMm0"}},{"cell_type":"code","source":["!pip install -U japanize-matplotlib"],"metadata":{"id":"5PCtx6EiuZoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ライブラリのインポート\n","import japanize_matplotlib\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"metadata":{"id":"_1RRIdg_pfre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PyTorch に対応した ReplayBuffer クラスの実装\n","class ReplayBuffer:\n","    def __init__(self, buffer_size, batch_size):\n","        self.buffer = deque(maxlen=buffer_size)\n","        self.batch_size = batch_size\n","\n","    def add(self, state, action, reward, next_state, done):\n","        data = (state, action, reward, next_state, done)\n","        self.buffer.append(data)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def get_batch(self):\n","        data = random.sample(self.buffer, self.batch_size)\n","\n","        state = torch.tensor(np.stack([x[0] for x in data]))\n","        action = torch.tensor(np.array([x[1] for x in data]))\n","        reward = torch.tensor(np.array([x[2] for x in data]))\n","        next_state = torch.tensor(np.stack([x[3] for x in data]))\n","        done = torch.tensor(np.array([x[4] for x in data]).astype(np.int32))\n","\n","        return state, action, reward, next_state, done\n","\n","\n","# Q 関数のニューラルネットワークモデル\n","class QNet(nn.Module):\n","    def __init__(self, action_size):\n","        super(QNet, self).__init__()\n","        self.linear1 = nn.Linear(4, 128)\n","        self.linear2 = nn.Linear(128, 128)\n","        self.linear3 = nn.Linear(128, action_size)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.linear2(x)\n","        x = self.relu(x)\n","        x = self.linear3(x)\n","        return x\n","\n","\n","# DQNAgent クラスの実装\n","class DQNAgent:\n","    def __init__(self):\n","        self.gamma = 0.98\n","        self.lr = 0.0005\n","        self.eps = 0.1\n","        self.buffer_size = 10000\n","        self.batch_size = 32\n","        self.action_size = 2\n","\n","        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n","        self.qnet = QNet(self.action_size)\n","        self.qnet_target = QNet(self.action_size)\n","        self.optimizer = optim.Adam(self.qnet.parameters(), lr=self.lr)\n","        self.criterion = nn.MSELoss()\n","\n","    def get_action(self, state):\n","        if np.random.rand() < self.eps:\n","            return np.random.choice(self.action_size)\n","        else:\n","            qs = self.qnet(state)\n","            return torch.argmax(qs).item()\n","\n","    def update(self, state, action, reward, next_state, done):\n","        self.replay_buffer.add(state, action, reward, next_state, done)\n","        if len(self.replay_buffer) < self.batch_size:\n","            return\n","\n","        state, action, reward, next_state, done = self.replay_buffer.get_batch()\n","        qs = self.qnet(state)\n","        q = qs[np.arange(len(action)), action]\n","\n","        with torch.no_grad():\n","            next_qs = self.qnet_target(next_state)\n","            next_q = next_qs.max(1)[0]\n","            target = reward + (1 - done) * self.gamma * next_q\n","\n","        target = torch.tensor(target, dtype=torch.float32)\n","        loss = self.criterion(q, target)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def sync_qnet(self):\n","        self.qnet_target.load_state_dict(self.qnet.state_dict())"],"metadata":{"id":"SKXepEfUonFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 報酬をプロットする関数を定義\n","def reward_show(history: list) -> None:\n","    x = [i for i in range(1, len(history) + 1)]\n","\n","    plt.figure(figsize=(8, 6), tight_layout=True)\n","    plt.title(\"報酬の遷移\", size=15, color=\"red\")\n","    plt.grid()\n","    plt.plot(x, history)"],"metadata":{"id":"rPdny2TNuPdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = gym.make(\"CartPole-v1\")\n","agent = DQNAgent()\n","\n","# 300回のエピソードで実行\n","episodes = 300\n","sync_interval = 20\n","reward_history = []\n","for episode in range(episodes):\n","    state = env.reset()\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        state = torch.tensor(state, dtype=torch.float32)\n","        action = agent.get_action(state)\n","        next_state, reward, done, info = env.step(action)\n","\n","        agent.update(state, action, reward, next_state, done)\n","        state = next_state\n","        total_reward += reward\n","\n","    if episode % sync_interval == 0:\n","        agent.sync_qnet()\n","\n","    reward_history.append(total_reward)\n","    if episode % 10 == 0:\n","        print(f\"episode: {episode}\\t\\ttotal reward: {total_reward}\")\n","\n","# 報酬の遷移画像を描画\n","reward_show(reward_history)"],"metadata":{"id":"4O186rQutidB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["最初はすぐにバランスを崩すが、ある程度エピソードを繰り返すことで「コツ」を掴み始める。  \n","その後は振れ幅が大きいが、概ね良い方向に学習ができていることがわかる。"],"metadata":{"id":"jCKkq9e9-b1m"}},{"cell_type":"markdown","source":["## DQN と Atari\n","\n","DQN は \"[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\" において提案された手法であり、 Atari はコンピュータゲームを作成する会社の名前である。  \n","強化学習の分野では Atari が制作した昔のゲームソフトを指して \"Atari\" と呼んでいる。"],"metadata":{"id":"OCFZUofF5mJR"}},{"cell_type":"markdown","source":["### Atari のゲーム環境\n","\n","OpenAI Gym には Atari の様々なゲーム環境が用意されている。  \n","「Pong」というゲームは、敵とユーザーがボードを動かしてボールを打ち合うゲームである。  \n","ボールが相手の陣地を越えれば得点が入る。  \n","エージェントには、状態としてゲームの画像が与えられる。"],"metadata":{"id":"ReAwCLUVRKcw"}},{"cell_type":"markdown","source":["### 前処理\n","\n","これまで出てきた強化学習の理論は、 MDP を前提としていた。  \n","MDP では、最適な行動を決めるにあたって必要な情報が「現在の状態」に含まれる。  \n","しかし、上記の「Pong」の場合、この要件は満たさない。  \n","この問題は <font color=\"red\">**部分観測マルコフ決定過程**</font>(Partially Observable Markov Decision Process)という。  \n","「Pong」のようなテレビゲームの場合、 POMDP を MDP に変換するのは簡単であり、4フレームの連続する画像を重ね合わせ、それを1つの「状態」として扱う。  \n","また、 DQN の論文では、フレームを重ねる前に固定の前処理を行っている。\n","\n","- 画像の周囲をトリミング\n","- グレイスケールへの変換\n","- 画像のリサイズ\n","- 正規化"],"metadata":{"id":"zJr83kXNSMJY"}},{"cell_type":"markdown","source":["### CNN\n","\n","前節のカートポールでは、全結合層からなるニューラルネットワークを使ったが、 Atari のような画像データを扱う場合は <font color=\"red\">**畳み込みニューラルネットワーク**</font>(Convolutional Neural Network)が有効である。  \n","DQN で使われる CNN では、入力に近い層では畳み込み層を使用し、出力に近い層では全結合層を使い、最後の出力層はタスクに応じて行動の候補の数だけ出力する。  \n","活性化関数には ReLU 関数が使われている。"],"metadata":{"id":"FyAM3EvzUYG3"}},{"cell_type":"markdown","source":["## DQN の拡張\n","\n","DQN は深層強化学習において最も有名なアルゴリズムの1つであり、これを発展させた手法は数多く提案されている。"],"metadata":{"id":"VwQyBc-TVa0o"}},{"cell_type":"markdown","source":["### Double DQN\n","\n","DQN では「ターゲットネットワーク」というテクニックが使われた。  \n","これはメインとなるネットワークの他に、パラメータが異なるネットワーク(ターゲットネットワーク)を使う手法である。  \n","2つのネットワークのパラメータをそれぞれ $\\theta$ と $\\theta'$ で表すことにして、その2つのネットワークによって表現される Q 関数を $Q_\\theta(s, a), Q_{\\theta'}(s, a)$ で表すとする。  \n","このとき、 Q 関数の更新で用いるターゲットは\n","\n","$$R_t + \\gamma \\max_a Q_{\\theta'}(S_{t+1}, a)$$\n","\n","で表される。  \n","DQN では、 $Q_{\\theta}(s, a)$ の値を上式の値(TD ターゲット)に近づけるように学習する。  \n","ここで問題なのが $\\max_a Q_{\\theta'}(S_{t+1}, a)$ で、誤差が含まれる推定値 $Q_{\\theta'}$ に対して $\\max$ 演算子を使うと真の Q 関数を使って計算する場合に比べて過大に評価されてしまう。  \n","Double DQN は、次の式を TD ターゲットにしてこの問題を解決した。\n","\n","\\begin{eqnarray*}\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n","R_t + \\gamma Q_{\\theta'}(S_{t+1}, \\argmax_a Q_{\\theta}(S_{t+1}, a))\n","\\end{eqnarray*}\n","\n","ポイントとしては、 $Q_\\theta(s, a)$ を使って最大となる行動を選び、実際の値は $Q_{\\theta'}(s, a)$ から取得することである。  \n","このように2つの Q 関数を使い分けることで、過大評価が解消され、学習がより安定する。"],"metadata":{"id":"tBJO-ru2V8R4"}},{"cell_type":"markdown","source":["### 優先度付き経験再生\n","\n","DQN で使われる経験再生では、経験 $E_t = (S_t, A_t, R_t, S_{t+1})$ をバッファに保存し、学習時にはバッファから経験データをランダムに取り出して使う。  \n","これをさらに進化させたのが<font color=\"red\">**優先度付き経験再生**</font>(Prioritized Experience Replay)である。  \n","経験データの優先度を決める上で、自然に考えられるのは次の式である。\n","\n","$$\\delta_t = \\left| R_t + \\gamma \\max_a Q_{\\theta'}(S_{t+1}, a) - Q_\\theta(S_t, A_t) \\right|$$\n","\n","この値が大きければそれだけ修正すべきこと、すなわち学ぶべきことが大きいということである。  \n","優先度付き経験再生では、経験データをバッファに保存するときに $\\delta_t$ も計算する。  \n","そして、これを含めた $(S_t, A_t, R_t, S_{t+1}, \\delta_t)$ をバッファに追加する。  \n","バッファから経験データを取り出すときは、 $\\delta_t$ を使って各経験データが選ばれる確率を計算する。  \n","$N$ 個の経験データがバッファに含まれる場合、 $i$ 番目の経験データが選ばれる確率は\n","\n","$$p_i = \\frac{\\delta_i}{\\sum_{k=0}^N \\delta_k}$$\n","\n","で表される。  \n","優先度付き経験再生を使うことで、学ぶべきことが多いデータほど優先して使用されるため、学習がより早く進むことが期待される。"],"metadata":{"id":"c0nJLSqXbjS8"}},{"cell_type":"markdown","source":["### Dueling DQN\n","\n","Dueling DQN は、ニューラルネットワークの構造を工夫した手法であり、キーとなるのが<font color=\"red\">**アドバンテージ関数**</font>(Advantage function)である。  \n","アドバンテージ関数は、 Q 関数と価値関数の差分で、\n","\n","$$A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s)$$\n","\n","のように定義され、 $a$ という行動が方策 $\\pi$ に比べてどれだけ良いかを表す。  \n","$Q_\\pi(s, a)$ と $V_\\pi(s)$ の違いは、状態 $s$ において行動 $a$ を行うか、それとも方策 $\\pi$ に従って行動を選ぶかであるため、アドバンテージ関数は、「$a$ という行動」が「方策 $\\pi$ で選ばれる行動」に比べてどれだけ良いかを示す指標として解釈できる。  \n","また、上の式を変形すると、\n","\n","$$Q_\\pi(s, a) = A_\\pi(s, a) + V_\\pi(s)$$\n","\n","であるため、アドバンテージ関数をもとに Q 関数を求めることもできる。  \n","DQN の場合、ある状態 $s$ で実際に行った行動 $a$ に対して $Q(s, a)$ を学習する。  \n","もし、行動に関わらず結果が決まっている状態であっても、全ての行動を試さなければ DQN では $Q(s, a)$ は学習されない。  \n","一方で、 Dueling DQN は、価値関数 $V(s)$ を経由する($V(s)$ が学習される)ので、他の行動を試さなくても $Q(s, a)$ の近似性能が改善する。"],"metadata":{"id":"8Cc2yVDVed1W"}}]}