{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNqNOYB5ZhuFZmr5ieoFY5G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第6章 〜TD 法〜\n","\n","ここでは、環境のモデルを使わずに、さらには行動を1つ行うたびに価値関数を更新する「<font color=\"red\">**TD 法**</font>」について扱う。"],"metadata":{"id":"eJohmX2oUtrh"}},{"cell_type":"markdown","source":["## TD 法による方策評価\n","\n","TD 法は、これまでに学んだ「モンテカルロ法」と「動的計画法」を合わせたような手法である。  \n","モンテカルロ法のようにエピソードの終わりを待つのではなく、一定の時間が進むごとに方策の評価と改善を行う。"],"metadata":{"id":"v9WagxKgWNTh"}},{"cell_type":"markdown","source":["### TD 法の導出\n","\n","収益は次のように定義された。\n","\n","\\begin{eqnarray*}\n","G_t &=& R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\\\\n","&=& R_t + \\gamma G_{t+1}\n","\\end{eqnarray*}\n","\n","この収益を使って、価値関数は次のように期待値として定義された。\n","\n","\\begin{eqnarray*}\n","v_\\pi(s) &=& \\mathbb{E}_\\pi[G_t|S_t = s] \\\\\n","&=& \\mathbb{E}_\\pi[R_t + \\gamma G_{t+1}|S_t = s]\n","\\end{eqnarray*}\n","\n","モンテカルロ法では期待値を計算する代わりに、実際に得られた収益のサンプルデータを平均することで近似する。  \n","平均には、標本平均と指数移動平均の2つがある。  \n","指数移動平均を実現するには、新しい収益が得られるたびに固定値 $\\alpha$ で更新する。\n","\n","$$V'_\\pi(S_t) = V_\\pi(S_t) + \\alpha \\{G_t - V_\\pi(S_t)\\}$$\n","\n","動的計画法では、数式通りに期待値を計算する。\n","\n","\\begin{eqnarray*}\n","v_\\pi(s) &=& \\mathbb{E}_\\pi[R_t + \\gamma G_{t+1}|S_t = s] \\\\\n","&=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\}\n","\\end{eqnarray*}\n","\n","状態遷移確率 $p(s'|s, a)$ と報酬関数 $r(s, a, s')$ を使って期待値を計算する。  \n","これはベルマン方程式であり、 DP はこれに基づいて価値関数を逐次更新する。\n","\n","$$V'_\\pi(s) = \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma V_\\pi(s') \\right\\}$$\n","\n","DP では、今の価値関数の推定値を次の価値関数の推定値を使って更新する。  \n","この原理は「ブートストラップ」と呼ばれた。  \n","一方、モンテカルロ法は実際に得られた経験によって今の価値関数を更新する。  \n","この2つの手法を融合させたのが TD 法である。  \n","重要な点は以下の2つである。\n","\n","- DP のようにブートストラップにより価値関数を逐次更新できる\n","- モンテカルロ法のように環境に関する知識を必要とせずにサンプリングされたデータを使って価値関数を更新できる\n","\n","\\begin{eqnarray*}\n","v_\\pi(s) &=& \\sum_{a, s'} \\pi(a|s) p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma v_{\\pi}(s') \\right\\} \\\\\n","&=& \\mathbb{E}_\\pi[R_t + \\gamma v_\\pi(S_{t+1})|S_t = s]\n","\\end{eqnarray*}\n","\n","TD 法ではこれを用いて価値関数を更新するにあたり、 $R_t + \\gamma v_\\pi(S_{t+1})$ の部分をサンプルデータから近似する。\n","\n","$$V'_\\pi(S_t) = V_\\pi(S_t) + \\alpha \\{ R_t + \\gamma V_\\pi(S_{t+1}) - V_\\pi(S_t) \\}$$\n","\n","$V_\\pi$ は価値関数の推定値であり、目的地(ターゲット)が $R_t + \\gamma V_\\pi(S_{t+1})$ になる。  \n","この目的地は <font color=\"red\">**TD ターゲット**</font>と呼ばれる。"],"metadata":{"id":"ab0XpOJNjTh_"}},{"cell_type":"markdown","source":["### モンテカルロ法と TD 法の比較\n","\n","モンテカルロ法と TD 法の式を比較する。\n","\n","\\begin{eqnarray*}\n","\\text{【モンテカルロ法】} V'_\\pi(S_t) &=& V_\\pi(S_t) + \\alpha \\{G_t - V_\\pi(S_t)\\} \\\\\n","\\text{【TD 法】} V'_\\pi(S_t) &=& V_\\pi(S_t) + \\alpha \\{ R_t + \\gamma V_\\pi(S_{t+1}) - V_\\pi(S_t) \\}\n","\\end{eqnarray*}\n","\n","モンテカルロ法では $G_t$ をターゲットとし、その方向へと $V_\\pi$ を更新する。  \n","$G_t$ はゴールに辿り着いてから得られる収益のサンプルデータである。  \n","一方、 TD 法のターゲットは、1ステップ先の情報をもとに計算する。  \n","時間が1ステップ進むごとに価値関数を更新できるため、 TD 法の方が効率の良い学習が期待できる。  \n","また、モンテカルロ法では分散が大きくなってしまうのに対し、 TD 法は1ステップ先のデータに基づくので、その変動は小さくなる。"],"metadata":{"id":"0MvWr3Sm6_fH"}},{"cell_type":"markdown","source":["### TD 法の実装(*コーディング*)\n","\n","ランダムな方策を持つエージェントに対して TD 法を使って方策を評価する。"],"metadata":{"id":"BaIirWxG9qaK"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from collections import defaultdict\n","\n","import numpy as np"],"metadata":{"id":"gVlv7wim-OoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GridWorld クラスの実装\n","class GridWorld:\n","    def __init__(self):\n","        self.action_space = [0, 1, 2, 3]\n","        self.action_meaning = {\n","            0: \"UP\",\n","            1: \"DOWN\",\n","            2: \"LEFT\",\n","            3: \"RIGHT\"\n","        }\n","        self.reward_map = np.array(\n","            [\n","                [0, 0, 0, 1.0],\n","                [0, None, 0, -1.0],\n","                [0, 0, 0, 0]\n","            ]\n","        )\n","        self.goal_state = (0, 3)\n","        self.wall_state = (1, 1)\n","        self.start_state = (2, 0)\n","        self.agent_state = self.start_state\n","\n","    @property\n","    def height(self):\n","        return len(self.reward_map)\n","\n","    @property\n","    def width(self):\n","        return len(self.reward_map[0])\n","\n","    @property\n","    def shape(self):\n","        return self.reward_map.shape\n","\n","    def actions(self):\n","        return self.action_space\n","\n","    def states(self):\n","        for h in range(self.height):\n","            for w in range(self.width):\n","                yield (h, w)\n","\n","    def next_state(self, state, action):\n","        action_move_map = [\n","            (-1, 0), (1, 0), (0, -1), (0, 1)\n","        ]\n","        move = action_move_map[action]\n","        next_state = (state[0] + move[0], state[1] + move[1])\n","        ny, nx = next_state\n","\n","        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:\n","            next_state = state\n","        elif next_state == self.wall_state:\n","            next_state = state\n","\n","        return next_state\n","\n","    def reward(self, state, action, next_state):\n","        return self.reward_map[next_state]\n","\n","    def step(self, action):\n","        state = self.agent_state\n","        next_state = self.next_state(state, action)\n","        reward = self.reward(state, action, next_state)\n","        done = (next_state == self.goal_state)\n","\n","        self.agent_state = next_state\n","\n","        return next_state, reward, done\n","\n","    def reset(self):\n","        self.agent_state = self.start_state\n","        return self.agent_state"],"metadata":{"id":"kV422ZKjALuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaqjbOt8T3lG"},"outputs":[],"source":["# TdAgent クラスの実装\n","class TdAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.alpha = 0.01\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.V = defaultdict(lambda: 0)\n","\n","    def get_action(self, state):\n","        action_probs = self.pi[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def eval(self, state, reward, next_state, done):\n","        next_V = 0 if done else self.V[next_state]\n","        target = reward + self.gamma * next_V\n","\n","        self.V[state] += (target - self.V[state]) * self.alpha"]},{"cell_type":"markdown","source":["エージェントを実際に行動させて方策評価を行う。"],"metadata":{"id":"Bdga3_bN_16b"}},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = GridWorld()\n","agent = TdAgent()\n","\n","# 1000回のエピソードで実行\n","episodes = 1000\n","for episode in range(episodes):\n","    state = env.reset()\n","\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done = env.step(action)\n","\n","        agent.eval(state, reward, next_state, done)\n","        if done:\n","            break\n","        state = next_state\n","\n","print(agent.V)"],"metadata":{"id":"fvWorH9K__Yl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ランダムな方策を持つエージェントの価値関数を評価することが出来た。"],"metadata":{"id":"ry6p9cxABAYJ"}},{"cell_type":"markdown","source":["## SARSA\n","\n","前節では TD 法による方策評価を行った。  \n","方策評価が終われば、次は方策制御である。  \n","ここでは「方策オン型」の SARSA という手法に取り組む。"],"metadata":{"id":"fzBrcNarBXnU"}},{"cell_type":"markdown","source":["### 方策オン型の SARSA\n","\n","方策制御を行う場合には、 $V_\\pi(s)$ ではなく $Q_\\pi(s, a)$ を対象にしなければいけない。  \n","改善フェーズでは方策を greedy 化する必要があり、 $V_\\pi(s)$ の場合は環境のモデルが必要になる。  \n","一方、 $Q_\\pi(s, a)$ であれば\n","\n","$$\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits} \\\\\n","\\mu(s) = \\argmax_a Q_\\pi(s, a)\n","$$\n","\n","と計算でき、環境のモデルを必要としない。\n","\n","それでは、状態価値関数 $V_\\pi(S_t)$ から行動価値関数 $Q_\\pi(S_t, A_t)$ へと TD 法を変更する。\n","\n","$$Q'_\\pi(S_t, A_t) = Q_\\pi(S_t, A_t) + \\alpha \\{ R_t + \\gamma Q_\\pi(S_{t+1}, A_{t+1}) - Q_\\pi(S_t, A_t) \\}$$\n","\n","これが Q 関数を対象にした TD 法の更新式である。  \n","$Q_\\pi(S_t, A_t)$ の更新が終わればすぐに改善フェーズに進める。  \n","この例では、 $Q_\\pi(S_t, A_t)$ が更新されるので、状態 $S_t$ における方策が変更される可能性がある。\n","\n","\\begin{equation}\n","\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits} \\\\\n","\\pi'(a|S_t) = \\left\\{ \\,\n","    \\begin{aligned}\n","    \\argmax_a Q_\\pi(S_t, a)&\\quad \\text{(1 - ε の確率)} \\\\\n","    \\text{ランダムな行動}&\\quad \\text{(ε の確率)}\n","    \\end{aligned}\n","\\right.\n","\\end{equation}\n","\n","ε の確率でランダムな行動を選び、それ以外は greedy な行動を選ぶ。  \n","greedy な行動によって方策は改善され、ランダムな行動によって探索が行われる。  \n","この ε-greedy 法によって状態 $S_t$ における行動の選び方が更新される。"],"metadata":{"id":"LkvC8l_OBvGy"}},{"cell_type":"markdown","source":["### SARSA の実装(*コーディング*)\n","\n","`SarsaAgent` クラスを実装する。"],"metadata":{"id":"WKBTZ7TwKGKh"}},{"cell_type":"code","source":["# ライブラリのインポート\n","from collections import deque"],"metadata":{"id":"Ik4jL8HcKQWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# greedy_probs 関数を定義\n","def greedy_probs(\n","        Q: dict, state: tuple[int, int], eps: float=0, action_size: int=4\n","    ) -> dict:\n","    qs = [Q[(state, action)] for action in range(action_size)]\n","    max_action = np.argmax(qs)\n","\n","    base_prob = eps / action_size\n","    action_probs = {action: base_prob for action in range(action_size)}\n","    action_probs[max_action] += (1 - eps)\n","\n","    return action_probs"],"metadata":{"id":"aEoynuKfKgWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SarsaAgent クラスの実装\n","class SarsaAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.alpha = 0.8\n","        self.eps = 0.1\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.Q = defaultdict(lambda: 0)\n","        self.memory = deque(maxlen=2)\n","\n","    def get_action(self, state):\n","        action_probs = self.pi[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def reset(self):\n","        self.memory.clear()\n","\n","    def update(self, state, action, reward, done):\n","        self.memory.append((state, action, reward, done))\n","        if len(self.memory) < 2:\n","            return\n","\n","        state, action, reward, done = self.memory[0]\n","        next_state, next_action, _, _ = self.memory[1]\n","\n","        next_q = 0 if done else self.Q[next_state, next_action]\n","\n","        target = reward + self.gamma * next_q\n","        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n","\n","        self.pi[state] = greedy_probs(self.Q, state, self.eps)"],"metadata":{"id":"UBZ8pJMzKj7z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["この `SarsaAgent` クラスを動かしてみる。"],"metadata":{"id":"I-o0rZTNMrGI"}},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = GridWorld()\n","agent = SarsaAgent()\n","\n","# 10000回のエピソードで実行\n","episodes = 10000\n","for episode in range(episodes):\n","    state = env.reset()\n","    agent.reset()\n","\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done = env.step(action)\n","\n","        agent.update(state, action, reward, done)\n","\n","        if done:\n","            agent.update(next_state, None, None, None)\n","            break\n","\n","        state = next_state\n","\n","print(agent.Q)"],"metadata":{"id":"FFPey107MxS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["方策にはランダム性があるため、できるだけ負の報酬から遠ざかるような動きが見られる。"],"metadata":{"id":"H0buWgTLNsPk"}},{"cell_type":"markdown","source":["## 方策オフ型の SARSA\n","\n","まずは方策オフ型の SARSA を導出し、その後 Q 学習に進む。"],"metadata":{"id":"e0kHWackN2mj"}},{"cell_type":"markdown","source":["### 方策オフ型と重点サンプリング\n","\n","方策オフ型では、エージェントは挙動方策とターゲット方策の2つの方策を持つ。  \n","挙動方策で多様な行動を行ってサンプルデータを集め、それを使ってターゲット方策を greedy に更新する。  \n","$Q_\\pi(S_t, A_t)$ を更新する場合を考え、方策 $\\pi$ によって行動が選ばれることを明示すると SARSA の更新式は次のようになる。\n","\n","$$\n","\\text{sampling: } A_{t+1} \\sim \\pi \\\\\n","Q'_\\pi(S_t, A_t) = Q_\\pi(S_t, A_t) + \\alpha \\{ R_t + \\gamma Q_\\pi(S_{t+1}, A_{t+1}) - Q_\\pi(S_t, A_t) \\}\n","$$\n","\n","$Q_\\pi(S_t, A_t)$ を $R_t + \\gamma Q_\\pi(S_{t+1}, A_{t+1})$ の方向へと更新することを表している。  \n","この $R_t + \\gamma Q_\\pi(S_{t+1}, A_{t+1})$ は「TD ターゲット」と呼ばれる。  \n","行動 $A_{t+1}$ が方策 $b$ によってサンプリングされた場合を考える。  \n","重み $\\rho$ によって TD ターゲットを補正し、重点サンプリングを行う。\n","\n","$$\\rho = \\frac{\\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}$$\n","\n","よって、方策オフ型の SARSA の更新式は次のようになる。\n","\n","$$\n","\\text{sampling: } A_{t+1} \\sim b \\\\\n","Q'_\\pi(S_t, A_t) = Q_\\pi(S_t, A_t) + \\alpha \\{ \\rho \\left( R_t + \\gamma Q_\\pi(S_{t+1}, A_{t+1}) \\right) - Q_\\pi(S_t, A_t) \\}\n","$$"],"metadata":{"id":"ju0Wz7VCOPdH"}},{"cell_type":"markdown","source":["### 方策オフ型の SARSA の実装(*コーディング*)\n","\n","方策オフ型の SARSA を実装する。"],"metadata":{"id":"vfLeQclLU4xW"}},{"cell_type":"code","source":["# SarsaOffPolicyAgent クラスの実装\n","class SarsaOffPolicyAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.alpha = 0.8\n","        self.eps = 0.1\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.b = defaultdict(lambda: random_actions)\n","        self.Q = defaultdict(lambda: 0)\n","        self.memory = deque(maxlen=2)\n","\n","    def get_action(self, state):\n","        action_probs = self.b[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def reset(self):\n","        self.memory.clear()\n","\n","    def update(self, state, action, reward, done):\n","        self.memory.append((state, action, reward, done))\n","        if len(self.memory) < 2:\n","            return\n","\n","        state, action, reward, done = self.memory[0]\n","        next_state, next_action, _, _ = self.memory[1]\n","\n","        if done:\n","            next_q = 0\n","            rho = 1\n","        else:\n","            next_q = self.Q[next_state, next_action]\n","            rho = self.pi[next_state][next_action] / \\\n","                self.b[next_state][next_action]\n","\n","        target = rho * (reward + self.gamma * next_q)\n","        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n","\n","        self.pi[state] = greedy_probs(self.Q, state, 0)\n","        self.b[state] = greedy_probs(self.Q, state, self.eps)"],"metadata":{"id":"05gdV2yLVDY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ここで実装した `SarsaOffPolicyAgent` クラスを使ってグリッドワールドの問題を解かせる。"],"metadata":{"id":"NvRM7yJPbfjl"}},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = GridWorld()\n","agent = SarsaOffPolicyAgent()\n","\n","# 10000回のエピソードで実行\n","episodes = 10000\n","for episode in range(episodes):\n","    state = env.reset()\n","    agent.reset()\n","\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done = env.step(action)\n","\n","        agent.update(state, action, reward, done)\n","\n","        if done:\n","            agent.update(next_state, None, None, None)\n","            break\n","\n","        state = next_state\n","\n","print(agent.Q)"],"metadata":{"id":"0QGISmGdbBUp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q 学習\n","\n","重点サンプリングは結果が、特に2つの方策の確率分布が異なれば異なるほど不安定になりやすいという問題があるため、できることならば避けたい手法である。  \n","方策オフ型の SARSA ではこれが必要であり、更新式にあるターゲットも大きく変動するため Q 関数の更新が不安定になる。  \n","<font color=\"red\">**Q 学習**</font>(Q-learning)である。  \n","Q 学習は次の3つの特徴を持つ。\n","\n","- TD 法\n","- 方策オフ型\n","- 重点サンプリングを行わない"],"metadata":{"id":"vw87xk32bqxN"}},{"cell_type":"markdown","source":["### ベルマン方程式と SARSA\n","\n","まずは SARSA とベルマン方程式の関係性から確認する。  \n","方策 $\\pi$ における Q 関数を $q_\\pi(s, a)$ としたとき、ベルマン方程式は次の式で表される。\n","\n","$$q_\\pi(s, a) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma \\sum_{a'} \\pi (a'|s') q_{\\pi}(s', a') \\right\\}$$\n","\n","ベルマン方程式で重要なのは次の2点である。\n","\n","- 環境の状態遷移確率 $p(s'|s, a)$ によって「次の全ての状態遷移」を考慮していること\n","- エージェントの方策 $\\pi$ によって「次の全ての行動」を考慮していること\n","\n","SARSA はベルマン方程式の「サンプリング版」と見做すことができる。  \n","SARSA では次の状態 $S_{t+1}$ は $p(s'|s, a)$ に基づきサンプリングする。  \n","そして、次の行動 $A_{t+1}$ は方策 $\\pi(a|s)$ に基づきサンプリングする。  \n","このとき、 SARSA の TD ターゲットは $R_t + \\gamma Q_\\pi(S_{t+1}, A_{t+1})$ になる。  \n","このターゲットの方向に Q 関数を少しだけ更新する。"],"metadata":{"id":"X50iG_dx46OG"}},{"cell_type":"markdown","source":["### ベルマン最適方程式と Q 学習\n","\n","価値反復法は、最適方策を得るための「評価」と「改善」という2つのプロセスを1つにまとめた手法である。  \n","価値反復法の重要な点は、ベルマン最適方程式に基づくただ1つの更新式を繰り返すことで、最適方策が得られることである。  \n","ここではベルマン最適方程式による更新で、なおかつそれを「サンプリング版」にした手法について考える。  \n","初めに、 Q 関数のベルマン最適方程式を見てみる。\n","\n","$$q_*(s, a) = \\sum_{s'} p(s'|s, a) \\left\\{ r(s, a, s') + \\gamma \\max_{a'} q_*(s', a') \\right\\}$$\n","\n","これを「サンプリング版」に書き換える。  \n","Q 学習では、推定値 $Q(S_t, A_t)$ のターゲットは $R_t + \\gamma \\max_a Q(S_{t+1}, a)$ になる。  \n","このターゲットの方向へと Q 関数を更新する。\n","\n","$$Q'(S_t, A_t) = Q(S_t, A_t) + \\alpha \\left\\{ R_t + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right\\}$$\n","\n","この式に基づき Q 関数を繰り返し更新することで最適方策における Q 関数へと近づく。"],"metadata":{"id":"qbK4Qa-H8xvu"}},{"cell_type":"markdown","source":["### Q 学習の実装(*コーディング*)\n","\n","Q 学習を行う `QLearningAgent` クラスを実装する。"],"metadata":{"id":"5qPXeVboBhlB"}},{"cell_type":"code","source":["# QLearningAgent クラスの実装\n","class QLearningAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.alpha = 0.8\n","        self.eps = 0.1\n","        self.action_size = 4\n","\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","        self.b = defaultdict(lambda: random_actions)\n","        self.Q = defaultdict(lambda: 0)\n","\n","    def get_action(self, state):\n","        action_probs = self.b[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)\n","\n","    def update(self, state, action, reward, next_state, done):\n","        if done:\n","            next_q_max = 0\n","        else:\n","            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n","            next_q_max = max(next_qs)\n","\n","        target = reward + self.gamma * next_q_max\n","        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n","\n","        self.pi[state] = greedy_probs(self.Q, state, eps=0)\n","        self.b[state] = greedy_probs(self.Q, state, self.eps)"],"metadata":{"id":"aRBwMJ6tBtv0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これを動かしてみる。"],"metadata":{"id":"r3d5xjDhLL6b"}},{"cell_type":"code","source":["# 各クラスのインスタンスを生成\n","env = GridWorld()\n","agent = QLearningAgent()\n","\n","# 10000回のエピソードで実行\n","episodes = 10000\n","for episode in range(episodes):\n","    state = env.reset()\n","\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done = env.step(action)\n","\n","        agent.update(state, action, reward, next_state, done)\n","        if done:\n","            break\n","\n","        state = next_state\n","\n","print(agent.Q)"],"metadata":{"id":"Xhgi38fFLNzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["結果は毎回変わるが、多くの場合最適方策が得られることがわかる。"],"metadata":{"id":"ZljwCgszMGay"}},{"cell_type":"markdown","source":["## 分布モデルとサンプルモデル\n","\n","本章でこれまで行ってきた実装は分布モデルであり、サンプルモデルの方がよりシンプルに実装できることを示す。"],"metadata":{"id":"B8lMd2LWMNDs"}},{"cell_type":"markdown","source":["### 分布モデルとサンプルモデル(*コーディング*)\n","\n","分布モデルとは、確率分布を明示的に保持するモデルである。  \n","例えば、ランダムに行動するエージェントを分布モデルとして実装すると次のようになる。"],"metadata":{"id":"yJZfSd88bUdj"}},{"cell_type":"code","source":["# 分布モデルとして実装した RandomAgent クラス\n","class RandomAgent:\n","    def __init__(self):\n","        random_actions = {\n","            0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25\n","        }\n","        self.pi = defaultdict(lambda: random_actions)\n","\n","    def get_action(self, state):\n","        action_probs = self.pi[state]\n","        actions = list(action_probs.keys())\n","        probs = list(action_probs.values())\n","        return np.random.choice(actions, p=probs)"],"metadata":{"id":"1uCmlj8Ibxn9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["一方で、サンプルモデルはサンプリングできることだけが条件のモデルであり、確率分布を持つ必要がないため、分布モデルよりシンプルに実装できる。"],"metadata":{"id":"j5tFNavTcd0b"}},{"cell_type":"code","source":["# サンプルモデルとして実装した RandomAgent クラス\n","class RandomAgent:\n","    def get_action(self, state):\n","        return np.random.choice(4)"],"metadata":{"id":"2T12UI4vcwME"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ここでは確率分布を持たずに、単に4つの行動からランダムに1つを選びだすように実装する。"],"metadata":{"id":"ceNfNajfcvuK"}},{"cell_type":"markdown","source":["### サンプルモデル版の Q 学習\n","\n","前節で Q 学習のエージェントを分布モデルとして実装した。  \n","今回はサンプルモデルで実装する。"],"metadata":{"id":"MH-siL0mdIh6"}},{"cell_type":"code","source":["# サンプルモデルとして実装した QLearningAgent クラス\n","class QLearningAgent:\n","    def __init__(self):\n","        self.gamma = 0.9\n","        self.alpha = 0.8\n","        self.eps = 0.1\n","        self.action_size = 4\n","        self.Q = defaultdict(lambda: 0)\n","\n","    def get_action(self, state):\n","        if np.random.rand() < self.eps:\n","            return np.random.choice(self.action_size)\n","        else:\n","            qs = [self.Q[state, a] for a in range(self.action_size)]\n","            return np.argmax(qs)\n","\n","    def update(self, state, action, reward, next_state, done):\n","        if done:\n","            next_q_max = 0\n","        else:\n","            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n","            next_q_max = max(next_qs)\n","\n","        target = reward + self.gamma * next_q_max\n","        self.Q[state, action] += (target - self.Q[state, action]) * self.alpha"],"metadata":{"id":"zhqZTDeHivID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["見てわかる通り、このコードでは方策を確率分布として保持していない。  \n","確率分布を保持する必要がないのでよりシンプルな実装ができている。"],"metadata":{"id":"VMNEIXJDkLXA"}}]}