{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPhre7zXI8jJx81pQpO7j2J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ゼロから作る Deep Learning 4 強化学習編 勉強ノート 第2章 〜マルコフ決定過程〜\n","\n","ここでは、エージェントの行動によって状況が変わる問題の一部で定式化される「<font color=\"red\">**マルコフ決定過程**</font>」について扱う。"],"metadata":{"id":"eUxB71Wwc8Ho"}},{"cell_type":"markdown","source":["## マルコフ決定過程(MDP)とは\n","\n","マルコフ決定過程(Markov Decision Process)とは、「エージェントが、環境と相互作用しながら行動を決定する過程」である。"],"metadata":{"id":"-wS7IH0oejtG"}},{"cell_type":"markdown","source":["### 具体例(*コーディング*)\n","\n","グリッドで区切られた世界を「グリッドワールド」と呼ぶことにする。  \n","エージェントは最初スタート地点におり、そこからできる限り多くのポイントを獲得してゴールに向かうものとする。  \n","エージェントはグリッドワールド内を上下左右に1マスずつ動くことができるが、障害物を通り抜けることはできないとする。  \n","以下にグリッドワールドの一例を作成する。"],"metadata":{"id":"uxeF3aLBgZ7Y"}},{"cell_type":"code","source":["# ライブラリのインポート\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"W2pigpo3nfZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# グリッドワールドを作成する関数を定義\n","# 0: 進める道, 1: 障害物, 2: スタート, 3: ゴール, 4: 加点ポイント, 5: 減点ポイント\n","def create_grid_world(\n","        rows: int, cols: int,\n","        start: set, goal: set,\n","        obstacles: list, items: dict, enemies: dict\n","    ):\n","    grid = np.zeros((rows, cols))\n","    points = np.zeros((rows, cols), dtype=int)\n","\n","    for obs in obstacles:\n","        grid[obs] = 1\n","\n","    grid[start] = 2\n","    if goal != ():\n","        grid[goal] = 3\n","\n","    for item, point in items.items():\n","        grid[item] = 4\n","        points[item] = point\n","\n","    for enemy, point in enemies.items():\n","        grid[enemy] = 5\n","        points[enemy] = point\n","\n","    return grid, points\n","\n","\n","# グリッドワールドを描画する関数を定義\n","def draw_grid_world(grid, points, figsize_x, figsize_y):\n","    rows, cols = grid.shape\n","    fig = plt.figure(figsize=(figsize_x, figsize_y), tight_layout=True)\n","    ax = fig.subplots()\n","\n","    ax.matshow(grid, vmin=-0.5, vmax=5)\n","\n","    ax.set_xticks(np.arange(-.5, cols, 1), minor=True)\n","    ax.set_yticks(np.arange(-.5, rows, 1), minor=True)\n","    ax.grid(which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=2)\n","\n","    for (i, j), val in np.ndenumerate(grid):\n","        if val == 1:\n","            ax.text(\n","                j, i, \"X\", ha=\"center\", va=\"center\",\n","                color=\"yellow\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 2:\n","            ax.text(\n","                j, i, \"S\", ha=\"center\", va=\"center\",\n","                color=\"red\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 3:\n","            ax.text(\n","                j, i, \"G\", ha=\"center\", va=\"center\",\n","                color=\"blue\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 4:\n","            ax.text(\n","                j, i, f\"+{points[i, j]}\", ha=\"center\", va=\"center\",\n","                color=\"white\", fontsize=16, fontweight=\"bold\"\n","            )\n","        elif val == 5:\n","            ax.text(\n","                j, i, f\"{points[i, j]}\", ha=\"center\", va=\"center\",\n","                color=\"black\", fontsize=16, fontweight=\"bold\"\n","            )\n","\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","\n","    plt.show()"],"metadata":{"id":"mcphSXlEnqf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# グリッドワールドのパラメータを設定\n","rows, cols = 10, 10\n","start = (0, 0)\n","goal = (9, 9)\n","obstacles = [(1, 1), (1, 2), (2, 1), (3, 2), (7, 6), (7, 8)]\n","items = {(1, 3): +5, (2, 2): +3, (3, 4): +5, (5, 6): +2, (8, 6): +3}\n","enemies = {(3, 5): -4, (7, 7): -2, (9, 7): -3}\n","\n","# グリッドワールドを作成して描画\n","grid, points = create_grid_world(\n","    rows, cols, start, goal, obstacles, items, enemies\n",")\n","draw_grid_world(grid, points, cols, rows)"],"metadata":{"id":"NXBVmIaUp_kw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["スタートからゴールまで行く方法は無限にあるが、どこをどのように通るかで得られるポイントが変わってくる。  \n","すなわち、エージェントの行動によって、そのエージェントが置かれている状況が変わってくるということである。  \n","この状況のことを「<font color=\"red\">**状態**</font>(State)」と呼ぶ。  \n","MDP では「時間」の概念が必要になり、その単位を「<font color=\"red\">**タイムステップ**</font>」などと言う。"],"metadata":{"id":"3FgqjEDmxn2I"}},{"cell_type":"markdown","source":["### エージェントと環境の相互作用\n","\n","MDP ではエージェントと環境の間で相互にやり取りをする。  \n","重要なのは、エージェントが行動を起こすことで状態が遷移するということである。  \n","それに伴い、得られる報酬も変わってくる。\n","\n","![MDP](https://assets.st-note.com/production/uploads/images/15214525/rectangle_large_type_2_4bf7be1ec2e8476ebcba33684604ae5b.png?width=1200)\n","\n","(出典: https://note.com/npaka/n/n7fca1d4d5ce8)\n","\n","上図において、時刻 $t$ で報酬 $R_t$ を受け取って状態が $S_t$ となり、この状態に基づいてエージェントが行動 $A_t$ を行い、報酬 $R_{t+1}$ を受け取って状態が $S_{t+1}$ へと遷移する。  \n","すなわち、このエージェントと環境の相互作用は次のような遷移を生む。\n","\n","$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3 \\cdots$$\n","\n","強化学習の分野では、報酬 $R_t$ の時間のタイミングについて、\n","\n","- 状態 $S_t$ で行動 $A_t$ を行い、報酬 $R_t$ を受け取り、次の状態 $S_{t+1}$ に遷移する\n","- 状態 $S_t$ で行動 $A_t$ を行い、報酬 $R_{t+1}$ を受け取り、次の状態 $S_{t+1}$ に遷移する\n","\n","の2パターンの慣例が見られるが、以降プログラミングとの親和性を考慮して前者を採用する。"],"metadata":{"id":"hJw0CANizc8z"}},{"cell_type":"markdown","source":["## 環境とエージェントの定式化\n","\n","MDP は、エージェントと環境とのやり取りを数式で定式化する。\n","\n","- 状態遷移: 状態はどのように遷移するか\n","- 報酬: 報酬はどのように与えられるか\n","- 方策: エージェントはどのように行動を決定するか"],"metadata":{"id":"imd61caK28WO"}},{"cell_type":"markdown","source":["### 状態遷移\n","\n","上記のグリッドワールドのスタート地点においては、右または下にしか動くことができない。  \n","仮に右に動く確率を0.3、下に動く確率を0.7とすると次の状態 $s'$ に遷移するのは、今の状態 $s$ と行動 $a$ によって次のように表記できる。\n","\n","\\begin{eqnarray*}\n","p(s'|s, a=\\text{Right}) &=& 0.3 \\\\\n","p(s'|s, a=\\text{Down}) &=& 0.7\n","\\end{eqnarray*}\n","\n","この $p(\\cdot)$ は<font color=\"red\">**状態遷移確率**</font>(State Transition Probability)と呼ばれる。  \n","$p(s'|s, a)$ は現在の状態 $s$ および 行動 $a$ にのみ依存しているので、これまでにどのような状態にあって、どのような行動を行ってきたかという過去の情報は不要である。  \n","このような性質のことを<font color=\"red\">**マルコフ性**</font>(Markov Property)という。"],"metadata":{"id":"19YcV-Os3doh"}},{"cell_type":"markdown","source":["### 報酬関数\n","\n","エージェントが状態 $s$ にいて、行動 $a$ を行い、状態が $s'$ に遷移した時に得られる報酬の関数を $r(s, a, s')$ と定義する。  \n","これを<font color=\"red\">**報酬関数**</font>(Reward Function)という。  \n","例えば、エージェントが(0, 3)の位置にいたとすると、\n","\n","\\begin{eqnarray*}\n","r(s=(0, 3), a=\\text{Right}, s'=(0, 4)) &=& 0 \\\\\n","r(s=(0, 3), a=\\text{Left}, s'=(0, 2)) &=& 0 \\\\\n","r(s=(0, 3), a=\\text{Down}, s'=(1, 3)) &=& 5\n","\\end{eqnarray*}\n","\n","ということになる。  \n","今回の場合、移動先によって報酬が決定するため、単に報酬関数を $r(s')$ とすることもできる。"],"metadata":{"id":"X-ecDnU8F5W-"}},{"cell_type":"markdown","source":["### エージェントの方策\n","\n","エージェントがどのように行動するかを決めるものを<font color=\"red\">**方策**</font>(Policy)という。  \n","方策について重要なのは、環境がマルコフ性に基づいて遷移するため、エージェントは「現在の状態」にのみ基づいて行動が決定されるということである。  \n","方策は、一般的に確率的に決まる。  \n","先ほどの例で言えば、(0, 3)にいるとして、エージェントの行動が確率的に決まる方策 $\\pi(\\cdot)$ は、次のように表される。\n","\n","\\begin{eqnarray*}\n","\\pi(a=\\text{Right}|s=(0, 3)) &=& 0.6 \\\\\n","\\pi(a=\\text{Left}|s=(0, 3)) &=& 0.1 \\\\\n","\\pi(a=\\text{Down}|s=(0, 3)) &=& 0.3\n","\\end{eqnarray*}"],"metadata":{"id":"rQ2I285ZIRGb"}},{"cell_type":"markdown","source":["## MDP の目標\n","\n","今までのことをまとめると、\n","\n","- エージェントは方策 $\\pi(a|s)$によって行動する\n","- その行動 $a$ と状態遷移確率 $p(s'|s, a)$ によって次の状態に遷移する\n","- 報酬関数 $r(s, a, s')$ に従って報酬を得る\n","\n","となる。  \n","\n","MDP の目標は、この枠組みの中で<font color=\"red\">**最適方策**</font>(Optimal Policy)を見つけることである。  \n","これを定式化する上で、まずは MDP の問題が大きく\n","\n","- エピソードタスク\n","- 連続タスク\n","\n","の2つに分けられることを説明する。"],"metadata":{"id":"-iW7qA7eOK2p"}},{"cell_type":"markdown","source":["### エピソードタスクと連続タスク\n","\n","囲碁のように、「終わり」がある問題をエピソードタスクという。  \n","また、この中で始まりから終わりまでの一連の試行を「エピソード」と呼ぶ。\n","\n","一方で、「終わり」がない問題のことを連続タスクという。  \n","例えば、在庫管理を行う問題では、エージェントがどれだけ商品を仕入れるかを判断し、売れ行きや在庫の量に応じてベストな仕入れ量を判断する。  \n","このような問題では、「終わり」を設けずに永遠に続く問題として定義することができる。"],"metadata":{"id":"aC-uLo0pZ7OI"}},{"cell_type":"markdown","source":["### 収益\n","\n","時刻 $t$ において状態が $S_t$ である場合を仮定する。  \n","そしてエージェントが方策 $\\pi$ によって行動 $A_t$ を行い、報酬 $R_t$ を得て新しい状態 $S_{t+1}$ に遷移するとする。  \n","ここで、<font color=\"red\">**収益**</font>(Return) $G_t$ を次のように定義する。\n","\n","$$G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots$$\n","\n","時間が進むにつれて $\\gamma$ によって指数関数的に報酬が減衰する。  \n","この $\\gamma$ を<font color=\"red\">**割引率**</font>(Discount Rate)と呼び、0.0から1.0の間の実数を設定する。  \n","割引率を導入する理由としては、連続タスクを想定したときに、収益が無限大になることを防ぐためである。"],"metadata":{"id":"Jed8iyoAa6_W"}},{"cell_type":"markdown","source":["### 状態価値関数\n","\n","エージェントと環境は「確率的に」振る舞う可能性があるため、例えばあるエピソードでは収益が10.4で、別のエピソードでは8.7といったように、同じ状態からスタートしても得られる収益はエピソードごとに変動する。  \n","このような確率的な挙動に対応するには、「収益の期待値」を指標とする必要がある。  \n","収益の期待値は次の式で表される。\n","\n","$$v_\\pi(s) = \\mathbb{E}[G_t|S_t = s, \\pi] = \\mathbb{E}_\\pi[G_t|S_t = s]$$\n","\n","これを<font color=\"red\">**状態価値関数**</font>(State-Value Function)という。  \n","方策 $\\pi$ が変わればエージェントが得る報酬も変わり、その総和である収益も変わるため、方策 $\\pi$ は状態価値関数の条件として与えられる。"],"metadata":{"id":"f4Y4sRjOcrUw"}},{"cell_type":"markdown","source":["### 最適方策と最適状態価値関数\n","\n","強化学習において目標とすることは、最適方策を手に入れることである。  \n","ここに2つの方策 $\\pi, \\pi'$ があり、2つの方策において状態価値関数 $v_\\pi(s), v_{\\pi'}(s)$ がそれぞれ決まるとする。  \n","どちらの方策が優れているかを考えるためには、全ての状態において $v_\\pi(s) \\geq v_{\\pi'}(s)$ または $v_\\pi(s) \\leq v_{\\pi'}(s)$ を満たさなければならない。  \n","もし、ある状態においてはその不等号が逆転するといったことがあれば、その2つの方策に優劣をつけることはできないということである。\n","\n","では、この考え方を推し進め、最適方策を $\\pi_*$ と表すと、方策 $\\pi_*$ は他のどの方策と比較しても、全ての状態で $v_{\\pi_*}(s)$ の値が大きいと言うことができる。  \n","重要な事実として、 MDP では最適方策が少なくとも1つは存在し、その最適方策は「決定論的方策」である(証明略)。  \n","この最適方策における状態価値関数は<font color=\"red\">**最適状態価値関数**</font>(Optimal State-Value Function)と呼ばれ、 $v_*$ で表記することにする。"],"metadata":{"id":"f6aNpM8fwPIP"}},{"cell_type":"markdown","source":["## MDP の例\n","\n","MDP についての具体的な問題を見ていく。"],"metadata":{"id":"VivYWVuL0dyC"}},{"cell_type":"markdown","source":["### 問題設定(*コーディング*)\n","\n","2マスのグリッドワールドを考える。\n","\n","- エージェントは右か左かのどちらかに移動できる\n","- 状態遷移は決定論的とする(エージェントが右に進む行動を行うと、エージェントの次の状態は必ず右側に遷移する)\n","- エージェントが(0, 1)から(0, 2)に移動したときに $+1$ の報酬を得る\n","- エージェントが(0, 2)から(0, 1)に移動するときのみ報酬が再度出現する\n","- (0, 0)と(0, 3)に行くと $-1$ の報酬を得て、それぞれ(0, 1)と(0, 2)に戻る\n","- 連続タスクとして考える"],"metadata":{"id":"_lFRVZvhZMdZ"}},{"cell_type":"code","source":["# グリッドワールドのパラメータを設定\n","rows, cols = 1, 4\n","start = (0, 1)\n","goal = ()\n","obstacles = []\n","items = {(0, 2): +1}\n","enemies = {(0, 0): -1, (0, 3): -1}\n","\n","# グリッドワールドを作成して描画\n","grid, points = create_grid_world(\n","    rows, cols, start, goal, obstacles, items, enemies\n",")\n","draw_grid_world(grid, points, cols, rows)"],"metadata":{"id":"9nz-HVioae8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 最適方策を見つける(*コーディング*)\n","\n","この問題における方策は、次の4通りである。\n","\n","|  | (0, 1) | (0, 2) |\n","| :--: | :--: | :--: |\n","| $\\pi_1(s)$ | Right | Right |\n","| $\\pi_2(s)$ | Right | Left |\n","| $\\pi_3(s)$ | Left | Right |\n","| $\\pi_4(s)$ | Left | Left |\n","\n","それぞれの方策について、割引率を0.9として計算してみる。"],"metadata":{"id":"FSfDLbDvcYYT"}},{"cell_type":"markdown","source":["方策 $\\pi_1$ では状態価値関数はそれぞれ以下のようになる。\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_1}(s = (0, 1)) &=& 1 + 0.9 \\cdot (-1) + 0.9^2 \\cdot (-1) + \\cdots \\\\\n","&=& 1 - 0.9(1 + 0.9 + 0.9^2 + \\cdots) \\\\\n","&=& 1 - \\frac{0.9}{1-0.9} \\\\\n","&=& -8\n","\\end{eqnarray*}\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_1}(s = (0, 2)) &=& -1 + 0.9 \\cdot (-1) + 0.9^2 \\cdot (-1) + \\cdots \\\\\n","&=& -1 - 0.9(1 + 0.9 + 0.9^2 + \\cdots) \\\\\n","&=& -1 - \\frac{0.9}{1-0.9} \\\\\n","&=& -10\n","\\end{eqnarray*}\n","\n","---"],"metadata":{"id":"MVzqKlNrhYn_"}},{"cell_type":"markdown","source":["方策 $\\pi_2$ では状態価値関数はそれぞれ以下のようになる。\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_2}(s = (0, 1)) &=& 1 + 0.9 \\cdot 0 + 0.9^2 \\cdot 1 + \\cdots \\\\\n","&=& 1 + 0.9^2 + 0.9^4 + \\cdots \\\\\n","&=& \\frac{1}{1-0.9^2} \\\\\n","&=& 5.26316 ...\n","\\end{eqnarray*}\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_2}(s = (0, 2)) &=& 0 + 0.9 \\cdot 1 + 0.9^2 \\cdot 0 + \\cdots \\\\\n","&=& 0.9(1 + 0.9^2 + 0.9^4 + \\cdots) \\\\\n","&=& \\frac{0.9}{1-0.9^2} \\\\\n","&=& 4.73684 ...\n","\\end{eqnarray*}\n","\n","---"],"metadata":{"id":"ads_2_fvjgbL"}},{"cell_type":"markdown","source":["方策 $\\pi_3$ では状態価値関数はそれぞれ以下のようになる。\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_3}(s = (0, 1)) &=& -1 + 0.9 \\cdot (-1) + 0.9^2 \\cdot (-1) + \\cdots \\\\\n","&=& -1 - 0.9(1 + 0.9 + 0.9^2 + \\cdots) \\\\\n","&=& -1 - \\frac{0.9}{1-0.9} \\\\\n","&=& -10\n","\\end{eqnarray*}\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_3}(s = (0, 2)) &=& -1 + 0.9 \\cdot (-1) + 0.9^2 \\cdot (-1) + \\cdots \\\\\n","&=& -1 - 0.9(1 + 0.9 + 0.9^2 + \\cdots) \\\\\n","&=& -1 - \\frac{0.9}{1-0.9} \\\\\n","&=& -10\n","\\end{eqnarray*}\n","\n","---"],"metadata":{"id":"X366JcLLlIML"}},{"cell_type":"markdown","source":["方策 $\\pi_4$ では状態価値関数はそれぞれ以下のようになる。\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_4}(s = (0, 1)) &=& -1 + 0.9 \\cdot (-1) + 0.9^2 \\cdot (-1) + \\cdots \\\\\n","&=& -1 - 0.9(1 + 0.9 + 0.9^2 + \\cdots) \\\\\n","&=& -1 - \\frac{0.9}{1-0.9} \\\\\n","&=& -10\n","\\end{eqnarray*}\n","\n","---\n","\n","\\begin{eqnarray*}\n","v_{\\pi_4}(s = (0, 2)) &=& 0 + 0.9 \\cdot (-1) + 0.9^2 \\cdot (-1) + \\cdots \\\\\n","&=& -0.9(1 + 0.9 + 0.9^2 + \\cdots) \\\\\n","&=& - \\frac{0.9}{1-0.9} \\\\\n","&=& -9\n","\\end{eqnarray*}\n","\n","---"],"metadata":{"id":"dy5pYpJal9nW"}},{"cell_type":"code","source":["# 状態のラベルとそれぞれの方策の状態価値関数の値\n","x = [\"(0, 1)\", \"(0, 2)\"]\n","y_1 = [-8, -10]\n","y_2 = [5.26316, 4.73684]\n","y_3 = [-10, -10]\n","y_4 = [-10, -9]\n","\n","# 画像のサイズを指定\n","plt.figure(figsize=(10, 6))\n","\n","# 画像のラベル情報付与\n","plt.title(\"Each Policy Comparison\", size=15, color=\"red\")\n","plt.xlabel(\"State\")\n","plt.ylabel(\"Value\")\n","\n","# 罫線\n","plt.grid()\n","\n","# 4種類のグラフをプロット\n","plt.plot(x, y_1, label=\"Policy 1\")\n","plt.plot(x, y_2, label=\"Policy 2\")\n","plt.plot(x, y_3, label=\"Policy 3\")\n","plt.plot(x, y_4, label=\"Policy 4\")\n","\n","# 2つのグラフの凡例を付与\n","plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", borderaxespad=0.)\n","\n","# グラフの描画\n","plt.show()"],"metadata":{"id":"SuTHLjeKmTcB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["以上により、方策 $\\pi_2$ が全ての状態において他の方策より状態価値関数の値が大きいことがわかった。"],"metadata":{"id":"89K8r1HSpB1Q"}}]}